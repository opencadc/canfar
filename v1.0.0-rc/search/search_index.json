{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"CANFAR Science Platform  <p>Canadian Advanced Network for Astronomical Research<sup>1</sup></p> <p> A scalable, cloud-native workspace for astronomy research. <p>Spin up JupyterLab, submit batch jobs, and collaborate in shared project spaces\u2014no hardware to maintain, no software to install. The CANFAR Science Platform gives professors, postdocs, and students the tools they need with zero setup. </p> <p>Do astronomy, not ops.</p> <p>Discover what the CANFAR Science Platform can do for you, your team, and your research group.<sup>2</sup></p> <ul> <li> Interactive Sessions e.g. JupyterLab</li> <li> Batch Processing for large-scale analysis</li> <li> Shared Storage for collaborative datasets</li> <li> Software Containers with astronomy tools</li> <li> Collaboration Tools with group permissions</li> <li> Expert Support for research workflows</li> <li> Python API for access and automation</li> <li> CLI for terminal users</li> <li> Publications of DataCite DOIs</li> <li> and much more...</li> </ul> <ol> <li> <p>Built on IVOA standards, based on F.A.I.R. principles \u21a9</p> </li> <li> <p>Please include the following acknowledgment in your publications &amp; other research outputs, as it directly helps us secure continued funding and support for the CANFAR Science Platform.\u00a0\u21a9</p> </li> </ol>"},{"location":"bug-reports/","title":"Bug Reports","text":"<p>Thank you for taking the time to report a bug! Your feedback helps us improve Canfar for everyone. This guide will help you create effective bug reports that enable us to quickly identify and fix issues.</p>"},{"location":"bug-reports/#before-reporting-a-bug","title":"Before Reporting a Bug","text":"<p>Before creating a new bug report, please:</p> <ol> <li>Search existing issues: Check if the bug has already been reported in our GitHub Issues</li> <li>Update to the latest version: Ensure you're using the latest version of Canfar Client</li> <li>Check the documentation: Review our documentation to confirm the expected behavior</li> </ol>"},{"location":"bug-reports/#how-to-report-a-bug","title":"How to Report a Bug","text":""},{"location":"bug-reports/#gather-system-information","title":"Gather System Information","text":"<p>Before reporting a bug, collect detailed system information using the Canfar CLI:</p> <pre><code>canfar version --debug\n</code></pre> <p>This command provides comprehensive information about your environment, including:</p> <ul> <li>Client Information: Canfar version, git commit info, and installation method</li> <li>Python Environment: Python version, executable path, and implementation</li> <li>System Details: Operating system, version, architecture, and platform</li> <li>Dependencies: Versions of key packages that might affect functionality</li> </ul>"},{"location":"bug-reports/#create-a-detailed-bug-report","title":"Create a Detailed Bug Report","text":"<p>When creating your bug report, include the following sections:</p> <ol> <li>Bug Description:      Provide a clear and concise description of what the bug is,<ul> <li>What you were trying to do</li> <li>What actually happened</li> <li>What you expected to happen</li> </ul> </li> <li>Steps to Reproduce: List the exact steps to reproduce the behavior, including any relevant commands and options.</li> <li>Expected Behavior: Describe what you expected to happen instead</li> <li>System Information: Include the complete output from <code>canfar version --debug</code></li> <li>Error Messages and Logs: Include any error messages, stack traces, or relevant log output. Use code blocks to format them properly.</li> <li>Screenshots (if applicable): Include screenshots that might help explain the problem</li> <li>Additional Context: Add any other context about the problem, such as<ul> <li>When the issue started occurring</li> <li>Whether it happens consistently or intermittently</li> <li>Any workarounds you've found</li> <li>Related configuration or environment details</li> </ul> </li> </ol>"},{"location":"bug-reports/#what-makes-a-good-bug-report","title":"What Makes a Good Bug Report","text":""},{"location":"bug-reports/#good-bug-reports-include","title":"\u2705 Good Bug Reports Include:","text":"<ul> <li>Clear, descriptive title</li> <li>Complete system information from <code>canfar version --debug</code></li> <li>Detailed steps to reproduce</li> <li>Expected vs. actual behavior</li> <li>Error messages and stack traces</li> <li>Relevant context and environment details</li> </ul>"},{"location":"bug-reports/#avoid-these-common-issues","title":"\u274c Avoid These Common Issues:","text":"<ul> <li>Vague descriptions like \"it doesn't work\"</li> <li>Missing system information</li> <li>Incomplete reproduction steps</li> <li>Screenshots of text instead of copy-pasted text</li> <li>Mixing multiple unrelated issues in one report</li> </ul>"},{"location":"bug-reports/#security-issues","title":"Security Issues","text":"<p>If you discover a security vulnerability, please do not create a public issue. Instead, please refer to our Security Policy for instructions on how to report security issues responsibly.</p>"},{"location":"bug-reports/#getting-help","title":"Getting Help","text":"<p>If you're not sure whether something is a bug or need help with usage:</p> <ul> <li>Check our documentation</li> <li>Ask questions in GitHub Discussions</li> <li>Review existing GitHub Issues</li> </ul>"},{"location":"bug-reports/#after-reporting","title":"After Reporting","text":"<p>After you submit a bug report:</p> <ol> <li>Monitor the issue: Watch for responses from maintainers</li> <li>Provide additional information: Be ready to answer follow-up questions</li> <li>Test fixes: Help test proposed solutions when available</li> <li>Update the issue: Let us know if the problem is resolved</li> </ol> <p>Thank you for helping make Canfar better! \ud83d\ude80</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#080-2025-08-08","title":"0.8.0 (2025-08-08)","text":""},{"location":"changelog/#features","title":"Features","text":"<ul> <li>auth: add HTTPx authentication hooks for token refresh (489066d)</li> <li>auth: add OIDC authentication hooks for automatic token refresh (bc4596f)</li> <li>auth: added oidc device flow logic (1013cf5)</li> <li>auth: added x509 cert get logic (657a97d)</li> <li>auth: implement async and sync OIDC token refresh functions (b43b05f)</li> <li>auth: server info is now saved for each auth config (1788c1e)</li> <li>cli: added <code>skaha open</code> command to open sessions in a web browser. Made skaha info more readable (c932187)</li> <li>cli: added auth cmds: list, switch, rm &amp; purge (6ab0bd8)</li> <li>cli: added entrypoing cf (cf3fce8)</li> <li>cli: added feat to auto discover skaha servers (46eb565)</li> <li>cli: added list, ls to skaha ps as aliases (c88eb6a)</li> <li>cli: added support for using the CLI as canfar (5051af1)</li> <li>client: add expiry property and enhance SSL context handling for authentication (cb6261a)</li> <li>client: enhance SkahaClient documentation and improve authentication handling (1e6b770)</li> <li>client: significant improvements to the base skaha client to support context managers (a197553)</li> <li>cli: work-in-process (2e646ab)</li> <li>config: updated auth config to provide valid &amp; expired checks for all auth types and added tests (4dc6929)</li> <li>discover: added functionality to discover skaha servers from registries and also added tests (5857772)</li> <li>exceptions: added skaha exception classes (0651191)</li> <li>garble: added fernet and rot13 ciphers to encrypt/obsfucate sensitive info (7204879)</li> <li>helpers: added code to to split tasks for large scale processing (6ffe5cb)</li> <li>hooks: added cli tool typer hook for multiple command aliases (6c5bc15)</li> <li>logging: added comprehensive rich based logger for the project (a48c520)</li> <li>models: added http models for the client (0679e8f)</li> <li>models: all client models have been moved to skaha.models (a07c676)</li> <li>oidc: added tests and made the oidc auth flow async (0059e12)</li> <li>tests: add comprehensive tests for OIDC authentication and HTTPx hooks (be7fb50)</li> <li>types: added mode, which reflects the auth type of the client (5781e4a)</li> <li>types: updated to the auth data model (7f293c4)</li> </ul>"},{"location":"changelog/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>auth, config, http: refactor imports and clean up default handling in models (6095a09)</li> <li>auth, config, models: refactor authentication handling and improve configuration properties (aaa8c61)</li> <li>auth: added comprehensive login support (bd11fe4)</li> <li>auth: improve login flow and server selection logic (18b8d6d)</li> <li>auth: x509 expired now return True when there is no cert to be found (2ab7141)</li> <li>auth: x509 now has better exception handling (a4469b3)</li> <li>cli: added a new aliases section to cleanup the l&amp;f of the cli (6165575)</li> <li>cli: added new aliases for list and create, fixed added <code>Terminating</code> state for Status models (a7e946d), closes #103</li> <li>cli: config - added path (30f2617)</li> <li>cli: create options cpus changed to cpu (05081ed)</li> <li>client, overview: refactor base URL handling to use 'url' attribute instead of 'server' (b429820)</li> <li>client: the skaha client on performs x509 checks when creating the clients (b6c3b80)</li> <li>cli: fix for stats output print statement (856df95)</li> <li>cli: numerous fixes for general look and feel, updated usage and help to be more descriptive (ea59085)</li> <li>cli: ps,list,ls are now sorted by startTime (fdf34ed)</li> <li>cli: updated the discover logic to be cleaner (ebff177)</li> <li>docs: improve docstrings for clarity and consistency across authentication modules (48599d3)</li> <li>docs: improve formatting of docstring for gather function parameters (9807194)</li> <li>docs: remove slow tests details and clarify slow test marking criteria (18999b7)</li> <li>docs: updated test architecture docs (4c32ef8)</li> <li>github-actions: fix for disabling pypi release (078176a)</li> <li>github-actions: migrated worklows to be under opencadc/canfar (0f267f5)</li> <li>import: fix for ruff TypeChecking import error (6b5aa6f)</li> <li>init: added CERT_PATH as module global (bb4642d)</li> <li>lint: major improvements to standards (264b9b5)</li> <li>log: fixed a missed f-string (bf31015)</li> <li>logging: moved all modules to use the new logging facility (e05399f)</li> <li>models: added <code>Failed</code> as a status (ef2c34e)</li> <li>models: added desktop-app and contributed to allowed kinds (2b8514f)</li> <li>models: CreateSpec (f4cd04f)</li> <li>models: fixes model import errors for pydantic (fd264d4)</li> <li>models: typo (4c7dbe6)</li> <li>models: update server fields to allow None values and adjust default handling (84b9c5d)</li> <li>models: updated client auth and reg models (b1c3f2b)</li> <li>models: updated client config model for better backwards compatibility (e10f007)</li> <li>models: x509 now does a lazy, rather than an eager check for cert path (eb16f76)</li> <li>mypy: setting extra checks to true (0a29305)</li> <li>oidc: streamline OIDC imports and update token expiry handling (ff0b62d)</li> <li>overview: baseurl fix (4bcb8f4)</li> <li>pre-commit: added jwt to mypy (f787507)</li> <li>pre-commit: updates (fa1b1c4)</li> <li>registry: added swedish skaha server (cfad0ef)</li> <li>security: fixing logging of sensitive info (07ff1d0)</li> <li>security: private init (0fada02)</li> <li>security: removed secret info (dccb1c2)</li> <li>skaha-client: deprecated verify field (a70c4b4)</li> <li>style: lint (248565f)</li> <li>tests: consolidated registry tests (80ad530)</li> <li>tests: ensure newline at end of file in test_servers_mixed_status (b50349e)</li> <li>tests: fix for comparing float values (f391833)</li> <li>tests: fix for stdout (6c25d3e)</li> <li>tests: for bad filenames (f0f4831)</li> <li>tests: import (54ed5fc)</li> <li>tests: refactor authentication tests to use updated model attributes and improve structure (8cfa73f)</li> <li>tests: removed not needed tests (d5be89c)</li> <li>tests: stdout related errors (e262eb3)</li> <li>tests: tmp filename (1dd4477)</li> <li>tests: update mock path mkdir lambda to accept arbitrary arguments (2939796)</li> <li>tests: update URL validation tests to reject 'sftp' scheme (9117701)</li> <li>tests: updates (53a0a7e)</li> <li>utils: crypto fix for generating funny names :D (2eff101)</li> <li>wip: lint/style/type-hinting (ced042d)</li> <li>x509: fixes for handling unintialized expiry (76e7523)</li> <li>x509: updated x509 auth utils to be all colocated and updated tests (9e5e7c6)</li> </ul>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>auth: added auth and context docs (65d3c2e)</li> <li>bug-reports: added issue template and docs for reporting bugs (1e177ea)</li> <li>cli: added docs (9a7f20a)</li> <li>client: updates (3db735b)</li> <li>contributing: add note on alternative tooling for dependency management (f7683db)</li> <li>helpers: updated docs for distributed.chunk and distributed.stripe usage (a8c15a0)</li> <li>helpers: updated docs to be better for advanced examples (4eb7558)</li> <li>skaha: update (e40bad4)</li> <li>tests: updated info about slow tests (a4401a3)</li> <li>updates: all over, work-in-progress (8ea1cce)</li> </ul>"},{"location":"changelog/#070-2025-05-28","title":"0.7.0 (2025-05-28)","text":""},{"location":"changelog/#features_1","title":"Features","text":"<ul> <li>session: added session.events to show deployment events on the cluster, e.g. loading container image etc (58e9bca)</li> </ul>"},{"location":"changelog/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>security: improvements to assertion checks (2ea1108)</li> <li>security: X509 certs are checked if valid before first conn. to server (08327a9)</li> <li>session: events now returns None, when verbose=True (1b01dd6)</li> <li>session: improved docs, better testing logic to await async sleep (e54b9af)</li> </ul>"},{"location":"changelog/#documentation_1","title":"Documentation","text":"<ul> <li>events: added session.events docs (7540062)</li> <li>session: docstring (7189052)</li> </ul>"},{"location":"changelog/#061-2025-05-22","title":"0.6.1 (2025-05-22)","text":""},{"location":"changelog/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>cleanup: comments (8a1a078)</li> <li>gha: fix for gh-pages push (27aa8f8)</li> <li>Implement httpx error logging hooks and client integration (fee71c2)</li> <li>pre-commit: cleanup (abb0839)</li> <li>security: restricting ssl context to use TLSv1.2 at a minimum (09654d0)</li> </ul>"},{"location":"changelog/#060-2025-05-12","title":"0.6.0 (2025-05-12)","text":""},{"location":"changelog/#features_2","title":"Features","text":"<ul> <li>client: added loglevel to configure the python logging levels (431f46d)</li> <li>client: added token support to skaha client (6c7c748)</li> <li>client: updated skaha client to use httpx instead of requests (4c10de8)</li> <li>logs: added stdout for printing logs in terminal (0c34cad)</li> <li>sessions: added skaha AsyncSession (2693272)</li> <li>sessions: added support for firefly (87d16c1)</li> </ul>"},{"location":"changelog/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>api: updated context, images, overview with httpx (391e857)</li> <li>client: changed to the default and max values of parallel conns (ce552bf)</li> <li>client: deprecated client.verify since it no longer affects any logic (9eed6e9)</li> <li>client: fixed annotation issues for client, asyncClient (889a6a8)</li> <li>models: models no longer search for SKAHA_REGISTRY_[USERNAME|SECRET] from environ, this will be supported in future releases with a comprehensive environment variable support accross all configurable variables (a1702c9)</li> <li>models: updated checks for session kind (a8317f4)</li> <li>session: fixed sync log output when verbose is True (6ac63f9)</li> <li>session: solidified the skaha async session api, moved common query building logic to utils.build (3f11aee)</li> </ul>"},{"location":"changelog/#documentation_2","title":"Documentation","text":"<ul> <li>asyncSession: added docs (a8be7fc)</li> <li>client: updated class docstring (5531c6f)</li> <li>index: typo fixes (bc637c5)</li> <li>index: updates (a3c2e2e)</li> <li>session: updated docs for session and async sessions (7e49fef)</li> <li>updates: docs (c61fecd)</li> <li>updates: site config + token support (54d6ed9)</li> </ul>"},{"location":"changelog/#052-2025-03-03","title":"0.5.2 (2025-03-03)","text":""},{"location":"changelog/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>session: fixed kind translation for session.create, increased max replicas limit to 512 (9bfe357)</li> </ul>"},{"location":"changelog/#051-2025-02-26","title":"0.5.1 (2025-02-26)","text":""},{"location":"changelog/#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>models: createSpec model now outputs kind with alias type (3184d5c)</li> </ul>"},{"location":"changelog/#050-2024-11-22","title":"0.5.0 (2024-11-22)","text":""},{"location":"changelog/#features_3","title":"Features","text":"<ul> <li>context: updates to context.resources api (4d08876)</li> </ul>"},{"location":"changelog/#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>docker: fix for docker build due to uv path install changes (00fd5de)</li> </ul>"},{"location":"changelog/#documentation_3","title":"Documentation","text":"<ul> <li>style: updates (e886d77)</li> </ul>"},{"location":"changelog/#044-2024-11-22","title":"0.4.4 (2024-11-22)","text":""},{"location":"changelog/#bug-fixes_7","title":"Bug Fixes","text":"<ul> <li>session: fixed set env in session.create (00b67ac)</li> </ul>"},{"location":"changelog/#043-2024-11-08","title":"0.4.3 (2024-11-08)","text":""},{"location":"changelog/#bug-fixes_8","title":"Bug Fixes","text":"<ul> <li>models: create.spec model used in session.create now expects env to be None by default (c34b110)</li> </ul>"},{"location":"changelog/#042-2024-10-30","title":"0.4.2 (2024-10-30)","text":""},{"location":"changelog/#bug-fixes_9","title":"Bug Fixes","text":"<ul> <li>models: added logging (514fda2)</li> </ul>"},{"location":"changelog/#documentation_4","title":"Documentation","text":"<ul> <li>index: updated the landing page (e7dbac2)</li> </ul>"},{"location":"changelog/#040-2024-10-25","title":"0.4.0 (2024-10-25)","text":""},{"location":"changelog/#features_4","title":"Features","text":"<ul> <li>build: added edge container build and attestation (d07e008)</li> <li>codecov: added badge (373412d)</li> <li>conduct: added a code of conduct for skaha community (f37046e)</li> <li>contributions: added a guideline (271a6df)</li> <li>dockerfile: added base dockerfile for the project (28f7e51)</li> <li>docs: added conduct,contributing,license and security sections to docs (5cac3c0)</li> <li>github-actions: added pypi release action and updated client payload (b0b3593)</li> <li>license: project now uses the AGPLv3 license (706f6f8)</li> <li>module: added support for private container registries (3b47c5c)</li> <li>packaging: moved skaha from poetry backend to uv (3b7b89f)</li> <li>security: added a security policy for the project (1338e7f)</li> <li>security: ossf scorecard (719cdfc)</li> <li>session: added new feature to delete sessions with name prefix, kind and status (056254b), closes #37</li> <li>templates: added bug report and feature requests templates (8a8dd20)</li> <li>client: updated client to include skaha version in prep for v1 release (e6360c0)</li> <li>overview: added new overview module (4a6336f)</li> <li>docs: added build (9049b92)</li> <li>session: create session now embeds two env variables into the container, REPLICA_COUNT and REPLICA_ID (ecbf48a)</li> <li>session: added support for multiple session management (219b74c)</li> <li>session: skaha.sessions api deprecated (e184663)</li> <li>release-please: implemented (2ac9728)</li> </ul>"},{"location":"changelog/#bug-fixes_10","title":"Bug Fixes","text":"<ul> <li>attestation: added attestation for dockerhub container image (0ff4ba2)</li> <li>badge: update to codeql bagde url (c95b6e0)</li> <li>ci/cd: bugfixes (b4b153c)</li> <li>ci/cd: fix for docs build (98eea9b)</li> <li>ci/cd: fixes for action deprecations, and uv errors (6a5af8c)</li> <li>CI: change to pre-commit checks (6216b02)</li> <li>ci: ci indent fix (4e02f72)</li> <li>ci: fix to edge container build (59924bd)</li> <li>ci: improved secret cleanup (990c5a1)</li> <li>contribution: updated guidelines (bc5400e)</li> <li>dockerfile: fix to stage names (f46b081)</li> <li>docs/ci: small fixes (e92c9eb)</li> <li>docs: updated doc/status/badge links (6efed00)</li> <li>github-actions: added fixes for release deployments (dc1b03d)</li> <li>github-actions: possible fix for deployment action (41e1886)</li> <li>github-actions: release actions now checkout tag_name ref for code (ebffafe)</li> <li>readme: codeql bagde url (197a6eb)</li> <li>tests: debugging ci/cd and common errors (7d6b3a9)</li> <li>tests: fixed issue with session tests (d004fde)</li> <li>tests: fixed issues with codecov tokens (07f87d9)</li> <li>tests: fixed session tests to be more consistent and run ~60s (19f0a6e)</li> <li>tests: fixed threading issue caused when one of the futures timesout (ba55a38)</li> <li>tests: fixes for session tests (b3f3e48)</li> <li>typing: multiple type hint fixes throughout the project (a533481)</li> <li>utils: fixed logging x2 issue (7e218df)</li> <li>docs: updated docs to include changelog, added reference for calling gpus in session.create (e58f9be)</li> <li>deps: updates (5644e15)</li> <li>session: fix for spawning sessions with gpus (961f766)</li> <li>tests: fixed session tests, which now default spawn with name-{replica-id} format (7e48031)</li> <li>env: fixed multiple tests and added support for multiple env parameters (c0500bf)</li> <li>client: updated session header to have the correct content-type (3146e41)</li> <li>images: images api now always prunes (a436e21)</li> <li>pre-commit: fixed broken pre-commit config (baedb82)</li> <li>type-hints: fixed broken hints (9f4e9db)</li> <li>type-hints: fixed broken type hints (c1d1356)</li> <li>gha: fix to release action (cc7b61a)</li> </ul>"},{"location":"changelog/#documentation_5","title":"Documentation","text":"<ul> <li>github-actions: changed the workflow name (868e114)</li> <li>README: updated with CI status (175ffce)</li> <li>sessions: added docs for destroy_with fucntionality (afd0a11)</li> <li>skaha: updated all docs (04551c9)</li> <li>docs: updates with a new ability to edit docs via PR (aa2314d)</li> <li>readme: update (1b975b6)</li> <li>docs: build command issue (becbc60)</li> <li>docs: fixed build issue (98b0543)</li> <li>docs: created documentation for the project (e0f5483)</li> <li>API: changed where order of docs (569d34f)</li> </ul>"},{"location":"conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at shiny.brar@nrc-cnrc.gc.ca. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"contributing/","title":"Contributing to CANFAR","text":"<p>Thank you for considering contributing to the canfar project! We welcome contributions from everyone. Please follow the guidelines below to help us maintain a high-quality codebase.</p> <p>We follow the Contributor Convenant of Code of Conduct. If you wish to contribute to canfar, please make sure to familiarize yourself with it.</p> <p>Contributions are not limited to just code. You can help us by:</p> <ul> <li>Answering questions on the Discussions board</li> <li>Improving the Documentation</li> <li>Reporting bugs and suggesting features via GitHub Issues (see our Bug Reporting Guide for detailed instructions)</li> <li>Spreading the word about CANFAR</li> </ul>"},{"location":"contributing/#how-to-contribute-code","title":"How to Contribute Code","text":""},{"location":"contributing/#1-fork-the-repository","title":"1. Fork the Repository","text":"<p>Start by forking the repository on GitHub. This will create a copy of the project under your GitHub account.</p>"},{"location":"contributing/#2-clone-your-fork","title":"2. Clone Your Fork","text":"<p>Clone your forked repository to your local machine:</p> <pre><code>git clone https://github.com/your-username/canfar.git\ncd canfar\n</code></pre>"},{"location":"contributing/#3-set-up-your-development-environment","title":"3. Set Up Your Development Environment","text":"<ul> <li>CANFAR uses uv for for package, project and dependency management. To install uv, please refer to the astral-uv documentation.</li> <li>You need a valid CANFAR account and access to the CANFAR Science Platform. To request access, please request an account with the Canadian Astronomy Data Centre (CADC).</li> </ul> <p>To setup the development environment, simply run:</p> <pre><code>uv python install 3.13\nuv venv --python 3.13\nuv sync --all-extras --dev\n</code></pre> <p>These commands will install the Python version, create a virtual environment, and install all dependencies required for development.</p> <p>Alternative Tooling</p> <p>While this project uses uv for dependency and virtual environment management, you are welcome to use other tools like pip, conda, or virtualenv. The <code>pyproject.toml</code> file contains all the necessary information for these tools to create a compatible environment.</p> <p>Skaha uses pre-commit to manage the development workflow. To install the pre-commit hooks, simply run:</p> <pre><code>uv run pre-commit install --hook-type commit-msg\n</code></pre>"},{"location":"contributing/#4-make-your-changes","title":"4. Make Your Changes","text":"<p>Make your changes. Please make sure to add tests for your changes if applicable.</p>"},{"location":"contributing/#5-run-the-tests","title":"5. Run the Tests","text":"<p>To run tests for Skaha, you need to have a valid CANFAR account and access to the CANFAR Science Platform. To generate a certificate, please refer to the get started section.</p> <pre><code>uv run pytest\n</code></pre>"},{"location":"contributing/#running-tests-efficiently","title":"Running Tests Efficiently","text":"<p>Some tests in the Skaha test suite are marked as \"slow\" because they involve network operations, waiting for session states, or other time-consuming operations. These tests can take several minutes to complete.</p> <p>Run all tests (including slow ones): <pre><code>uv run pytest\n</code></pre></p> <p>Skip slow tests for faster development: <pre><code>uv run pytest -m \"not slow\"\n</code></pre></p> <p>Run only slow tests: <pre><code>uv run pytest -m \"slow\"\n</code></pre></p> <p>The slow tests are primarily integration tests that interact with the CANFAR Science Platform and include: - Session creation and management tests - Log retrieval tests - Authentication timeout tests - Session statistics tests</p> <p>For rapid development and testing, it's recommended to use <code>-m \"not slow\"</code> to skip these time-consuming tests during your development cycle, and run the full test suite before submitting your pull request.</p>"},{"location":"contributing/#6-commit-your-changes","title":"6. Commit Your Changes","text":"<p>Skaha uses the conventional commit messages standard to ensure the commit history human and machine readable. Skaha ships with a tool called <code>commitizen</code> that helps you craft commit messages in the correct format.</p> <pre><code>git add files/you/changed.py\nuv run cz commit\n</code></pre> <p>At this point, you will also see pre-commit hooks running to check your code for any issues and ensure that the code is linted and formatted correctly.</p>"},{"location":"contributing/#7-push-changes-to-your-fork","title":"7. Push Changes to Your Fork","text":"<p>Push your changes to your forked repository:</p> <pre><code>git push\n</code></pre>"},{"location":"contributing/#8-create-a-pull-request","title":"8. Create a Pull Request","text":"<p>Once your changes are pushed to your fork, you can create a pull request from your forked repository to the main Skaha repository. The maintainers will review your changes and merge them if everything is in order.</p>"},{"location":"contributing/#9-celebrate","title":"9. Celebrate","text":"<p>Congratulations! You've made it through the contribution process! Now it's time to celebrate your hard work. Here are a few fun ways to do so:</p> <ul> <li>Dance Party: Put on your favorite tunes and have a solo dance party in your living room. Bonus points for using a disco ball!</li> <li>Snack Attack: Treat yourself to your favorite snack. Whether it's pizza, ice cream, or a healthy smoothie, you deserve it!</li> <li>Virtual High-Five: Send a virtual high-five to your fellow contributors. You can even use a GIF for extra flair!</li> <li>Meme It Up: Create a meme about your contribution journey. Share it in the Discussions board for a good laugh!</li> <li>Celebrate with Code: Write a fun piece of code that does absolutely nothing but prints \"I did it!\" to the console. Because why not?</li> </ul> <p>Remember, every contribution counts, and you\u2019ve just made the Skaha project a little better. Now go forth and celebrate like the coding rockstar you are!</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>This unified FAQ covers the CANFAR Science Platform across three areas: Platform, Client, and CLI.</p>"},{"location":"faq/#platform","title":"Platform","text":""},{"location":"faq/#what-is-the-canfar-science-platform","title":"What is the CANFAR Science Platform?","text":"<p>The CANFAR Science Platform is a national cloud computing environment tailored for astronomy. It provides interactive notebooks and desktops, contributed applications (e.g., CARTA, Firefly), batch jobs, and direct access to CADC data holdings.</p>"},{"location":"faq/#who-can-use-it-and-what-does-it-cost","title":"Who can use it and what does it cost?","text":"<p>CANFAR is free for astronomical research. Canadian astronomers and their collaborators can use it subject to fair\u2011use and allocation limits. For larger needs, request additional resources via the Digital Research Alliance of Canada (DRAC) Resource Allocation Competition.</p>"},{"location":"faq/#how-do-i-get-access","title":"How do I get access?","text":"<ol> <li>To start, you must have a CADC account. If you don't have a CADC account, you can request one at: https://www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/en/auth/request.html</li> <li>To get CANFAR access, send a short e-mail to support@canfar.net with a short note about who you are, your research and what you plan to do with CANFAR. Include your CADC username. Turnaround is typically 1-2 business days. This can be done in parallel with requesting a CADC account.</li> <li>Alternatively, if you already have a CADC account and you are part of a research group that is already using CANFAR, you can ask your PI to add you to the appropriate project groups.</li> </ol>"},{"location":"faq/#what-session-types-are-available-and-when-should-i-use-them","title":"What session types are available and when should I use them?","text":"<ul> <li>Notebook: Jupyter Lab for interactive analysis and prototyping.</li> <li>Desktop: Full Linux desktop for GUI workflows and multi\u2011app sessions.</li> <li>Firefly: Interactive database access and visualization.</li> <li>CARTA: Specialized image/cube visualization.</li> <li>Headless: Non\u2011GUI batch processing and automation.</li> </ul>"},{"location":"faq/#how-long-can-sessions-run","title":"How long can sessions run?","text":"<ul> <li>Interactive sessions: up to 7 days of continuous runtime, with auto\u2011shutdown after prolonged inactivity; resumable if not deleted.</li> <li>Batch jobs: no strict time limit; queue priority depends on resource usage.</li> </ul>"},{"location":"faq/#can-i-run-gpuaccelerated-workloads","title":"Can I run GPU\u2011accelerated workloads?","text":"<p>Yes. Request GPUs in the session configuration (e.g., NVIDIA Tesla/RTX). Ensure your chosen container supports GPU computing.</p>"},{"location":"faq/#how-much-storage-do-i-get-and-where-should-i-put-data","title":"How much storage do I get and where should I put data?","text":"<ul> <li>Personal: <code>/arc/home/[username]/</code> (e.g., 10 GB typical).</li> <li>Project/group: <code>/arc/projects/[group]/</code> (hundreds of GB to TBs, varies by project). If you don't already have a project space, you can request one by e-mailing suport@canfar.net.</li> <li>Temporary: <code>/tmp/</code> inside sessions (cleared when the session ends). Suggested layout: raw \u2192 <code>/arc/projects/[group]/raw/</code>, working \u2192 <code>/arc/projects/[group]/data/</code>, results \u2192 <code>/arc/projects/[group]/results/</code>, scripts \u2192 <code>/arc/projects/[group]/scripts/</code>.</li> </ul>"},{"location":"faq/#how-do-i-transfer-large-datasets","title":"How do I transfer large datasets?","text":"<ul> <li>For files &lt;1 GB, the Science Portal file manager is convenient.</li> <li>For larger transfers, use <code>rsync</code>/<code>scp</code> or VOSpace for very large files (&gt;10 GB).</li> </ul> <p>Example: <pre><code>rsync -avz --progress source/ username@canfar.net:/arc/projects/mygroup/\ncadc-data put largefile.fits vos:myproject/data/\n</code></pre></p>"},{"location":"faq/#what-softwarecontainers-are-available","title":"What software/containers are available?","text":"<p>Containers include general astronomy stacks (AstroPy ecosystem), Jupyter, full Linux desktops, and specialized tools (CASA, CARTA, DS9, TOPCAT). You can also build and use custom containers. See the Container Guide at <code>platform/containers.md</code>.</p>"},{"location":"faq/#can-i-install-additional-software","title":"Can I install additional software?","text":"<ul> <li>Temporary (inside a running session): <code>pip install --user ...</code> or system packages if permitted.</li> <li>Permanent: build a custom container with your required stack (see <code>platform/containers.md</code>).</li> </ul>"},{"location":"faq/#collaboration-and-sharing","title":"Collaboration and sharing","text":"<ul> <li>Share sessions for real\u2011time collaboration (view or full access).</li> <li>Share data via project groups and <code>/arc/projects/[group]/</code> with appropriate permissions.</li> <li>Share code via Git and group storage; document workflows.</li> </ul>"},{"location":"faq/#troubleshooting-slow-or-failing-sessions","title":"Troubleshooting slow or failing sessions","text":"<ul> <li>Resource constraints: try fewer cores/less RAM, different time of day, or a different container.</li> <li>Container issues: verify name/version; try a maintained baseline image.</li> <li>Account/group issues: confirm group membership and active account status.</li> <li>Performance: process data in fast scratch (e.g., <code>/tmp/</code>), parallelize where appropriate, monitor with <code>htop</code>, <code>df -h</code>, <code>iotop</code>.</li> </ul>"},{"location":"faq/#where-are-my-files","title":"Where are my files?","text":"<ul> <li>Personal: <code>/arc/home/$(whoami)/</code></li> <li>Group: <code>/arc/projects/</code></li> <li>Temporary: session\u2011local <code>/tmp/</code> (deleted at end of session)</li> </ul>"},{"location":"faq/#getting-help-and-community","title":"Getting help and community","text":"<ul> <li>Documentation: start at <code>platform/home.md</code> and <code>platform/guides/index.md</code>.</li> <li>Help &amp; Support: <code>platform/help.md</code> (how to contact support and what to include).</li> <li>Community: Discord for Q&amp;A and announcements; workshops and office hours are announced there.</li> </ul>"},{"location":"faq/#client","title":"Client","text":""},{"location":"faq/#can-i-automate-session-management-with-the-python-client","title":"Can I automate session management with the Python client?","text":"<p>Yes. The Python client supports creating, monitoring, and cleaning up sessions programmatically.</p> <p>Example: <pre><code>import time\nfrom canfar import Session\n\nsession = Session()\nsid = session.create(name=\"automated\", kind=\"headless\", cmd=\"python\", args=[\"script.py\"])\n\nwhile session.info(sid)[0][\"status\"] != \"Completed\":\n    time.sleep(60)\n\nsession.logs([sid])\nsession.destroy([sid])\n</code></pre></p>"},{"location":"faq/#how-do-i-call-the-rest-api-directly","title":"How do I call the REST API directly?","text":"<p>You can use REST endpoints for jobs and sessions if you prefer low\u2011level control.</p> <p>Example: <pre><code>import requests\n\nresponse = requests.post(\n    \"https://ws-uv.canfar.net/skaha/v0/session\",\n    headers={\"Authorization\": f\"Bearer {token}\"},\n    data={\n        \"name\": \"automated-analysis\",\n        \"image\": \"images.canfar.net/skaha/astroml:latest\",\n        \"cores\": 4,\n        \"ram\": 16,\n        \"kind\": \"headless\",\n        \"cmd\": \"python /arc/projects/myproject/analyze.py\",\n    },\n)\nresponse.raise_for_status()\n</code></pre></p>"},{"location":"faq/#how-do-i-access-vospace-programmatically","title":"How do I access VOSpace programmatically?","text":"<p>Use CADC client libraries to interact with VOSpace objects.</p> <p>Example: <pre><code>from cadcdata import CadcDataClient\n\nclient = CadcDataClient()\nclient.put_file(\"local_file.fits\", \"vos:myproject/data/file.fits\")\n</code></pre></p>"},{"location":"faq/#authentication-options-for-programs","title":"Authentication options for programs","text":"<ul> <li>X.509 certificates (typical for many users).</li> <li>OIDC tokens via SRCNet for advanced and cross\u2011site workflows. See <code>cli/authentication-contexts.md</code> for options and flows.</li> </ul>"},{"location":"faq/#cli","title":"CLI","text":""},{"location":"faq/#how-do-i-authenticate","title":"How do I authenticate?","text":"<ul> <li>Certificates: <pre><code>cadc-get-cert -u &lt;username&gt;\n</code></pre></li> <li>OIDC (SRCNet\u2011aware): <pre><code>canfar auth login\n</code></pre></li> </ul> <p>Certificates typically last ~10 days; renew as needed.</p>"},{"location":"faq/#how-do-i-check-platform-status-and-quotas-from-the-cli","title":"How do I check platform status and quotas from the CLI?","text":"<pre><code>canfar stats\n</code></pre>"},{"location":"faq/#why-is-my-session-stuck-in-pending","title":"Why is my session stuck in \"Pending\"?","text":"<p>Possible reasons: insufficient resources, image issues, quota limits, or maintenance windows. Inspect events: <pre><code>canfar events &lt;session-id&gt;\n</code></pre></p>"},{"location":"faq/#i-cant-connect-to-my-session-url","title":"I can\u2019t connect to my session URL","text":"<ol> <li>Ensure the session is Running (<code>canfar ps</code>).</li> <li>Check for VPN/firewall interference.</li> <li>Try another browser or clear cache/private mode.</li> </ol>"},{"location":"faq/#can-i-run-multiple-sessions-at-once","title":"Can I run multiple sessions at once?","text":"<p>Yes. You can run multiple sessions concurrently subject to fair\u2011use and any configured limits per session type. Prefer batch/headless for automation.</p>"},{"location":"faq/#where-can-i-find-more-cli-help","title":"Where can I find more CLI help?","text":"<ul> <li>Quick start: <code>cli/quick-start.md</code></li> <li>Auth contexts: <code>cli/authentication-contexts.md</code></li> <li>Command reference: <code>cli/cli-help.md</code></li> </ul>"},{"location":"license/","title":"License","text":"<pre><code>                GNU AFFERO GENERAL PUBLIC LICENSE\n                   Version 3, 19 November 2007\n</code></pre> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/  Everyone is permitted to copy and distribute verbatim copies  of this license document, but changing it is not allowed.</p> <pre><code>                        Preamble\n</code></pre> <p>The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works.  By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.</p> <p>When we speak of free software, we are referring to freedom, not price.  Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.</p> <p>A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate.  Many developers of free software are heartened and encouraged by the resulting cooperation.  However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.</p> <p>The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community.  It requires the operator of a network server to provide the source code of the modified version running there to the users of that server.  Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.</p> <p>An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals.  This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p> <pre><code>                   TERMS AND CONDITIONS\n</code></pre> <ol> <li>Definitions.</li> </ol> <p>\"This License\" refers to version 3 of the GNU Affero General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License.  Each licensee is addressed as \"you\".  \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy.  The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy.  Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies.  Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License.  If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p> <ol> <li>Source Code.</li> </ol> <p>The \"source code\" for a work means the preferred form of the work for making modifications to it.  \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form.  A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities.  However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work.  For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p> <ol> <li>Basic Permissions.</li> </ol> <p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met.  This License explicitly affirms your unlimited permission to run the unmodified Program.  The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work.  This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force.  You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright.  Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below.  Sublicensing is not allowed; section 10 makes it unnecessary.</p> <ol> <li>Protecting Users' Legal Rights From Anti-Circumvention Law.</li> </ol> <p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p> <ol> <li>Conveying Verbatim Copies.</li> </ol> <p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p> <ol> <li>Conveying Modified Source Versions.</li> </ol> <p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <pre><code>a) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\n</code></pre> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit.  Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p> <ol> <li>Conveying Non-Source Forms.</li> </ol> <p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <pre><code>a) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\n</code></pre> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling.  In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage.  For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product.  A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source.  The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information.  But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed.  Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p> <ol> <li>Additional Terms.</li> </ol> <p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law.  If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it.  (Additional permissions may be written to require their own removal in certain cases when you modify the work.)  You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <pre><code>a) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\n</code></pre> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10.  If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term.  If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p> <ol> <li>Termination.</li> </ol> <p>You may not propagate or modify a covered work except as expressly provided under this License.  Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License.  If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p> <ol> <li>Acceptance Not Required for Having Copies.</li> </ol> <p>You are not required to accept this License in order to receive or run a copy of the Program.  Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance.  However, nothing other than this License grants you permission to propagate or modify any covered work.  These actions infringe copyright if you do not accept this License.  Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p> <ol> <li>Automatic Licensing of Downstream Recipients.</li> </ol> <p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License.  You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations.  If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License.  For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p> <ol> <li>Patents.</li> </ol> <p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based.  The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version.  For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement).  To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients.  \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License.  You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p> <ol> <li>No Surrender of Others' Freedom.</li> </ol> <p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License.  If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all.  For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p> <ol> <li>Remote Network Interaction; Use with the GNU General Public License.</li> </ol> <p>Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software.  This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.</p> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work.  The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.</p> <ol> <li>Revised Versions of this License.</li> </ol> <p>The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time.  Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number.  If the Program specifies that a certain numbered version of the GNU Affero General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation.  If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions.  However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p> <ol> <li>Disclaimer of Warranty.</li> </ol> <p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p> <ol> <li>Limitation of Liability.</li> </ol> <p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p> <ol> <li>Interpretation of Sections 15 and 16.</li> </ol> <p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <pre><code>                 END OF TERMS AND CONDITIONS\n\n        How to Apply These Terms to Your New Programs\n</code></pre> <p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program.  It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published\nby the Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source.  For example, if your program is a web application, its interface could display a \"Source\" link that leads users to an archive of the code.  There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.</p>"},{"location":"security/","title":"Security Policy","text":""},{"location":"security/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>If you discover a security vulnerability in this project, please report it by sending an email to shiny.brar@nrc-cnrc.gc.ca.</p> <p>We will respond as quickly as possible to address the issue.</p>"},{"location":"security/#security-updates","title":"Security Updates","text":"<p>We will make security updates available as soon as they are ready. Please ensure you are using the latest version of the project to benefit from these updates.</p>"},{"location":"security/#acknowledgments","title":"Acknowledgments","text":"<p>We appreciate the efforts of the community in helping us improve the security of this project.</p>"},{"location":"about/acknowledgement/","title":"Acknowledgement","text":"<p>If you have used CANFAR facilities for your research, please include the following acknowledgment in your publications, theses, and other research outputs.</p> <p>This directly helps us to secure continued funding and support for the CANFAR project.</p> <p>Citation</p> <p>The authors acknowledge the use of the Canadian Advanced Network for Astronomy Research (CANFAR) Science Platform operated by the Canadian Astronomy Data Center (CADC) and the Digital Research Alliance of Canada (DRAC), with support from the National Research Council of Canada (NRC), the Canadian Space Agency(CSA), CANARIE, and the Canadian Foundation for Innovation (CFI).</p>"},{"location":"about/governance/","title":"CANFAR Governance","text":"<p>The CANFAR project is operated by the Canadian Astronomy Data Centre (CADC) on behalf of the Canadian astronomy research community. Enhancement and allocation of the capacity of CANFAR (both operational and resource-related) are overseen by the Science Management Committee (SMC).  </p> <p>The SMC is composed of Canadian researchers leading projects that make significant use of the CANFAR platform. The chair of the SMC rotates among its members and is responsible for organizing SMC meetings as needed and reporting decisions to the operator, i.e. CADC.  </p> <p>In addition, one SMC member serves as the lead for the Digital Research Alliance Canada (DRAC) resource allocation request, the cyber-infrastructure resource that enables CANFAR. The SMC meets periodically, nominally at the annual Canadian Astronomical Society of Canada (CASCA) meeting and at other times via teleconference when issues arise, such as the operator (CADC) requesting direction on significant technology choices.  </p>"},{"location":"about/governance/#science-management-committee","title":"Science Management Committee","text":"Name Role / Affiliation Ray Carlberg University of Toronto Pauline Barmby University of Western Ontario (RAP Lead) Ren\u00e9 Doyon Universit\u00e9 de Montr\u00e9al James Di Francesco National Research Council Canada / University of Victoria S\u00e9bastien Fabbro National Research Council Laura Ferrarese National Research Council Canada / University of Victoria Bryan Gaensler University of Toronto Falk Herwig Professor, Dept. of Physics and Astronomy, University of Victoria Mike Hudson University of Waterloo JJ Kavelaars National Research Council Canada / University of Victoria (CADC Lead) Helen Kirk National Research Council Canada Alan McConnachie National Research Council Canada / University of Victoria Laura Parker McMaster University Chris Pritchet University of Victoria Erik Rosolowsky University of Alberta Marcin Sawicki St. Mary\u2019s University Douglas Scott University of British Columbia Alison Sills McMaster University Rob Thacker St. Mary\u2019s University Ludo von Waerbeke University of British Columbia James Wadsley McMaster University"},{"location":"about/home/","title":"Organization","text":"<p>The CANFAR consortium is represented by a network of university researchers that participate in determining directions and emphasis of the consortium and form the Science Management Committee.</p> <p>The day-to-day operation of the consortium is coordinated by the management team which consists of:</p> Name Role Pauline Barmby DRAC &amp; RPP Lead (UWO) Sharon Goliath Operations Lead (CADC) Brian Major Software Development Lead (CADC) Management Lead JJ Kavelaars (CADC/UVic) <p>The consortium has an advisory Science Management Committee that has both scientific and computational and data expertise representation.</p> <p>Agreements between members may formalize relations as needed.</p>"},{"location":"about/partners/","title":"Partners","text":"<p>The National Research Council of Canada (NRC) is Canada\u2019s premier science agency. They support the development of cutting-edge technologies and research that drive innovation and economic growth. </p> <p>The NRC is the home of the Canadian Astronomy Data Centre and the Canadian Advanced Network for Astronomy Research (CANFAR).</p> <ul> <li> <p></p> <p>CANARIE connects Canada to the world by providing ultra-high-speed network that connects Canada\u2019s researchers and educators to each other and to global data, technology, and colleagues through collaboration with the government, industry, and international partners.\\</p> </li> <li> <p></p> <p>Digital Research Alliance Canada drives the acceleration of research and innovation nationwide. By deploying advanced research computing (ARC) systems, storage, and software solutions\u2014delivered in partnership with regional organizations\u2014they provide essential infrastructure for Canadian researchers and their collaborators across academia and industry. Their resources form a cornerstone of CANFAR\u2019s capabilities.</p> </li> <li> <p></p> <p>The Canadian Space Agency supports Canada\u2019s leadership in space astronomy and the growth of the national space technology sector. Through its funding of the Canadian Astronomy Data Centre, they directly supports the operation and advancement of CANFAR.</p> </li> </ul>"},{"location":"about/terms/","title":"Terms of Reference","text":""},{"location":"about/terms/#purpose","title":"Purpose","text":"<p>The Canadian Advanced Network for Astronomy Research is a consortium that serves data-intensive storage, access, and processing needs of university groups and centers engaged in astronomy research.</p> <p>To this end, CANFAR develops and operates user-facing and integrated services, such as:</p> <ul> <li>Research Data Management</li> <li>User-managed storage and cloud processing</li> <li>Specialized visualization and analytics services</li> <li>Authentication and Authorization</li> <li>Support to researchers in adapting the system to their needs</li> </ul>"},{"location":"about/terms/#mission","title":"Mission","text":"<ol> <li> <p>To maintain and develop services in data and computationally intensive research that enable Canadian researchers and their international collaborators to generate the greatest possible scientific return on Canada\u2019s investment in telescopes and computational resources.</p> </li> <li> <p>To coordinate efforts and resources in multi-use applications and services across Canadian groups, by generating integrated grant proposals and funding requests that enable the combination and integration of the science application domain (predominantly located at the universities) and the technical expertise (e.g. at CADC). Such grant proposals and funding request would be directed to any of the Canadian agencies involved in Science, Innovation, Computing and Big Data, such as NSERC, CFI, Canarie or NRC.</p> </li> <li> <p>Interface on behalf of the astronomy community with Digital Research Alliance Canada to ensure that services meet the astronomy domain specific needs.</p> </li> <li> <p>Reduce barriers and accelerate adoption of new emerging technologies in Big Data and Computing in the astronomy community.</p> </li> <li> <p>Share capabilities developed in the astronomy community with other domains.</p> </li> </ol>"},{"location":"about/terms/#membership","title":"Membership","text":"<p>The consortium membership reflects the necessity of present and future science challenges to integrate those groups and institutions that specialize in the scientific aspects (typically at the universities and research centers and institutes) and the groups that have developed the technical and computer engineering expertise to respond to the needs of science programs.</p> <p>Consequently the following groups are eligible for membership in the CANFAR consortium:</p> <ul> <li>Individual university researchers and their groups</li> <li>University centers or institutions engaged in astronomy research</li> <li>Multi-institutional or multi-investigator collaborations, including their international partners</li> <li>Individuals and teams from the CADC, or teams from Digital Research Alliance Canada, WestGrid or other entities that provide computational and data resources and infrastructure</li> </ul>"},{"location":"cli/authentication-contexts/","title":"Authentication Guide","text":"<p>CANFAR Python Client and CLI are designed to connect to multiple Science Platform servers around the world. This guide covers everything you need to know about authentication, from basic setup to advanced scenarios.</p>"},{"location":"cli/authentication-contexts/#authentication-overview","title":"Authentication Overview","text":"<p>CANFAR clients use an Authentication Context system to manage connections to different Science Platform servers. This system supports multiple authentication methods and makes it easy to switch between servers.</p>"},{"location":"cli/authentication-contexts/#what-is-an-authentication-context","title":"What is an Authentication Context?","text":"<p>Think of an authentication context (context for short) as a saved profile that contains:</p> <ul> <li>Server information (URL, capabilities)</li> <li>Authentication credentials (X.509 certificate, OIDC tokens, etc.)</li> <li>User preferences for that specific server</li> </ul> <p>When you use Canfar, one context is always active, and all commands and API calls are directed to that server.</p>"},{"location":"cli/authentication-contexts/#authentication-methods","title":"Authentication Methods","text":"<p>Canfar supports several authentication methods:</p> <p>Authentication Methods</p> <ul> <li>X.509 Certificates - Most common, uses <code>.pem</code> certificate files</li> <li>OIDC Tokens - OpenID Connect for modern authentication flows</li> <li>Bearer Tokens - Direct token authentication for API access</li> </ul> <p>Automatic Configuration</p> <p>Canfar automatically configures the appropriate authentication method based on the server's capabilities and your configuration.</p>"},{"location":"cli/authentication-contexts/#cli-authentication-management","title":"CLI Authentication Management","text":"<p>The Canfar CLI provides comprehensive commands for managing your authentication contexts.</p>"},{"location":"cli/authentication-contexts/#initial-login-canfar-auth-login","title":"Initial Login (<code>canfar auth login</code>)","text":"<p>The <code>login</code> command is your starting point for connecting to any Science Platform server:</p> <pre><code>canfar auth login\n</code></pre> <p>What happens during login:</p> <ol> <li>Server Discovery - Automatically finds available Science Platform servers worldwide</li> <li>Server Selection - Interactive prompt to choose your target server</li> <li>Authentication Flow - Guides you through the server's authentication method</li> <li>Context Creation - Saves your credentials and server configuration</li> <li>Activation - Sets the new context as active for immediate use</li> </ol> <p>Login Options</p> <pre><code># Basic login with server discovery\ncanfar auth login\n\n# Include development/testing servers\ncanfar auth login --dev\n\n# Include non-responsive servers in discovery\ncanfar auth login --dead\n\n# Show detailed server information during selection\ncanfar auth login --details\n\n# Force re-authentication for existing context\ncanfar auth login --force\n</code></pre>"},{"location":"cli/authentication-contexts/#managing-multiple-contexts","title":"Managing Multiple Contexts","text":"<p>Once you have one or more authentication contexts, you can easily manage them:</p>"},{"location":"cli/authentication-contexts/#listing-contexts-canfar-auth-list","title":"Listing Contexts (<code>canfar auth list</code>)","text":"<p>View all your saved authentication contexts:</p> <pre><code>canfar auth list\n</code></pre> <p>Example Output: <pre><code>                  Available Authentication Contexts                  \n\n  Active   Name          Auth Mode   Server URL                      \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n    \u2705     CADC-CANFAR     x509      https://ws-uv.canfar.net/skaha  \n\n           SRCnet-Sweden   oidc      https://services.swesrc.chalmers.se/skaha\n</code></pre></p> <p>The active context (marked with \u2705) determines where your commands are sent.</p>"},{"location":"cli/authentication-contexts/#switching-contexts-canfar-auth-switch","title":"Switching Contexts (<code>canfar auth switch</code>)","text":"<p>Switch between your saved contexts safely:</p> <pre><code>canfar auth switch &lt;CONTEXT_NAME&gt;\n</code></pre> <p>Switching Examples</p> <pre><code># Switch to a different server\ncanfar auth switch SRCnet-Sweden\n\n# Switch back to CANFAR\ncanfar auth use CADC-CANFAR\n</code></pre> <p>All subsequent commands will use the newly active context.</p>"},{"location":"cli/authentication-contexts/#removing-contexts-canfar-auth-remove","title":"Removing Contexts (<code>canfar auth remove</code>)","text":"<p>Remove contexts you no longer need:</p> <pre><code>canfar auth remove &lt;CONTEXT_NAME&gt;\n</code></pre> <p>Safety Features</p> <ul> <li>You cannot remove the currently active context</li> <li>Switch to a different context first, then remove the unwanted one</li> <li>Removed contexts cannot be recovered (you'll need to login again)</li> </ul>"},{"location":"cli/authentication-contexts/#purging-all-contexts-canfar-auth-purge","title":"Purging All Contexts (<code>canfar auth purge</code>)","text":"<p>Remove all authentication contexts and credentials:</p> <pre><code>canfar auth purge\n</code></pre> <p>Complete Removal</p> <p>This command permanently deletes:</p> <ul> <li>All saved authentication contexts</li> <li>Your entire canfar configuration file (<code>~/.config/canfar/config.yaml</code>)</li> <li>You'll need to login again to use canfar</li> </ul> <p>Options: <pre><code># Skip confirmation prompt\ncanfar auth purge --yes\n\n# Interactive confirmation (default)\ncanfar auth purge\n</code></pre></p>"},{"location":"cli/authentication-contexts/#programmatic-authentication","title":"Programmatic Authentication","text":"<p>Once you have authentication contexts set up via the CLI, you can use them programmatically in your Python code.</p>"},{"location":"cli/authentication-contexts/#using-active-context","title":"Using Active Context","text":"<p>The simplest approach uses your currently active authentication context:</p> <pre><code>from canfar.session import Session\n\n# Uses the active authentication context automatically\nsession = Session()\n\n# Check which context is being used\nprint(f\"Active context: {session.config.active}\")\nprint(f\"Auth Context: {session.config.context}\")\n</code></pre>"},{"location":"cli/authentication-contexts/#authentication-priority","title":"Authentication Priority","text":"<p>When creating a session, canfar follows this priority order:</p> <ol> <li>User-provided token (highest priority)</li> <li>User-provided certificate</li> <li>Active authentication context</li> <li>Default certificate location (<code>~/.ssl/cadcproxy.pem</code>)</li> </ol> <pre><code>from pathlib import Path\nfrom pydantic import SecretStr\nfrom canfar.session import Session\n\n# Priority 1: Direct token (overrides everything)\nsession = Session(token=SecretStr(\"your-bearer-token\"))\n\n# Priority 2: Direct certificate (overrides context)\nsession = Session(certificate=Path(\"/path/to/cert.pem\"))\n\n# Priority 3: Uses active context (most common)\nsession = Session()\n</code></pre>"},{"location":"cli/authentication-contexts/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli/authentication-contexts/#common-authentication-issues","title":"Common Authentication Issues","text":"<p>Login Problems</p> <p>No servers found during discovery</p> <ul> <li>Check your internet connection</li> <li>Try <code>canfar auth login --dead</code> to include non-responsive servers</li> <li>Verify you're not behind a restrictive firewall</li> </ul> <p>Authentication failed</p> <ul> <li>Verify your username and password are correct</li> <li>Check if your account is active on the Science Platform</li> <li>Try logging into the web interface first</li> </ul> <p>Certificate expired</p> <ul> <li>X.509 certificates typically last 10 days</li> <li>Run <code>canfar auth login --force</code> to refresh</li> <li>Check expiry with your authentication status code above</li> </ul> <p>Context Management Issues</p> <p>No active context found</p> <ul> <li>Run <code>canfar auth list</code> to see available contexts</li> <li>Use <code>canfar auth switch &lt;context&gt;</code> to activate one</li> <li>If no contexts exist, run <code>canfar auth login</code></li> </ul> <p>Cannot remove active context</p> <ul> <li>Switch to a different context first: <code>canfar auth switch &lt;other&gt;</code></li> <li>Then remove the unwanted context: <code>canfar auth remove &lt;unwanted&gt;</code></li> </ul>"},{"location":"cli/authentication-contexts/#debug-mode","title":"Debug Mode","text":"<p>Enable detailed authentication logging:</p> <pre><code># CLI debug mode\ncanfar auth login --debug\n</code></pre>"},{"location":"cli/authentication-contexts/#getting-help","title":"Getting Help","text":"<p>Support Resources</p> <ul> <li>\ud83d\udcd6 CLI Reference - Complete command documentation</li> <li>\ud83d\udcac Community Discussions - Ask questions</li> <li>\ud83d\udc1b Report Issues - Bug reports and feature requests</li> </ul> <p>```</p>"},{"location":"cli/cli-help/","title":"CLI Reference","text":"<p>The Canfar CLI provides a comprehensive command-line interface for interacting with the Science Platform. This reference covers all available commands and their options.</p> <p>Getting Started</p> <p>The CLI can be accessed using the <code>canfar</code> command in your environment: <pre><code>canfar --help\n</code></pre></p>"},{"location":"cli/cli-help/#main-command","title":"Main Command","text":"<pre><code>canfar [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Description: Command Line Interface for Science Platform.</p>"},{"location":"cli/cli-help/#global-options","title":"Global Options","text":"Option Description <code>--install-completion</code> Install completion for the current shell <code>--show-completion</code> Show completion for the current shell, to copy it or customize the installation <code>--help</code> Show help message and exit <p>Shell Completion</p> <p>Enable shell completion for a better CLI experience by running: <pre><code>canfar --install-completion\n</code></pre></p>"},{"location":"cli/cli-help/#authentication-commands","title":"\ud83d\udd10 Authentication Commands","text":""},{"location":"cli/cli-help/#canfar-auth","title":"<code>canfar auth</code>","text":"<p>Authenticate with Science Platform.</p>"},{"location":"cli/cli-help/#canfar-auth-login","title":"<code>canfar auth login</code>","text":"<p>Login to Science Platform with automatic server discovery.</p> <pre><code>canfar auth login [OPTIONS]\n</code></pre> <p>Description: This command guides you through the authentication process, automatically discovering the upstream server and choosing the appropriate authentication method based on the server's configuration.</p>"},{"location":"cli/cli-help/#options","title":"Options","text":"Option Type Default Description <code>--force</code> Flag - Force re-authentication <code>--debug</code> Flag - Enable debug logging <code>--dead</code> Flag - Include dead servers in discovery <code>--dev</code> Flag - Include dev servers in discovery <code>--details</code> Flag - Include server details in discovery <code>--timeout</code>, <code>-t</code> INTEGER 2 Timeout for server response <code>--discovery-url</code> TEXT <code>https://ska-iam.stfc.ac.uk/.well-known/openid-configuration</code> OIDC Discovery URL <p>Basic Login</p> <pre><code>canfar auth login\n</code></pre> <p>Login with Debug Information</p> <pre><code>canfar auth login --debug --details\n</code></pre>"},{"location":"cli/cli-help/#canfar-auth-list-canfar-auth-ls","title":"<code>canfar auth list</code> / <code>canfar auth ls</code>","text":"<p>Show all available authentication contexts.</p> <pre><code>canfar auth list [OPTIONS]\n</code></pre> <p>Example</p> <pre><code>canfar auth list\n</code></pre>"},{"location":"cli/cli-help/#canfar-auth-switch-canfar-auth-use","title":"<code>canfar auth switch</code> / <code>canfar auth use</code>","text":"<p>Switch the active authentication context.</p> <pre><code>canfar auth switch CONTEXT\n</code></pre> <p>Arguments: - <code>CONTEXT</code> (required): The name of the context to activate</p> <p>Example</p> <pre><code>canfar auth switch production\n</code></pre>"},{"location":"cli/cli-help/#canfar-auth-remove-canfar-auth-rm","title":"<code>canfar auth remove</code> / <code>canfar auth rm</code>","text":"<p>Remove a specific authentication context.</p> <pre><code>canfar auth remove CONTEXT\n</code></pre> <p>Arguments: - <code>CONTEXT</code> (required): The name of the context to remove</p> <p>Permanent Action</p> <p>This action permanently removes the authentication context and cannot be undone.</p>"},{"location":"cli/cli-help/#canfar-auth-purge","title":"<code>canfar auth purge</code>","text":"<p>Remove all authentication contexts.</p> <pre><code>canfar auth purge [OPTIONS]\n</code></pre>"},{"location":"cli/cli-help/#options_1","title":"Options","text":"Option Description <code>--yes</code>, <code>-y</code> Skip confirmation prompt <p>Destructive Action</p> <p>This command removes ALL authentication contexts. Use with caution!</p>"},{"location":"cli/cli-help/#session-management-commands","title":"\ud83d\ude80 Session Management Commands","text":""},{"location":"cli/cli-help/#canfar-create","title":"<code>canfar create</code>","text":"<p>Create a new session on the Science Platform.</p> <pre><code>canfar create [OPTIONS] desktop|notebook|carta|headless|firefly|desktop-app|contributed IMAGE [-- CMD [ARGS]...]\n</code></pre> <p>Arguments: - <code>KIND</code> (required): Session type - one of: <code>desktop</code>, <code>notebook</code>, <code>carta</code>, <code>headless</code>, <code>firefly</code>, <code>desktop-app</code>, <code>contributed</code> - <code>IMAGE</code> (required): Container image to use - <code>CMD [ARGS]...</code> (optional): Runtime command and arguments</p>"},{"location":"cli/cli-help/#options_2","title":"Options","text":"Option Short Type Default Description <code>--name</code> <code>-n</code> TEXT Auto-generated Name of the session <code>--cpu</code> <code>-c</code> INTEGER 1 Number of CPU cores <code>--memory</code> <code>-m</code> INTEGER 2 Amount of RAM in GB <code>--gpu</code> <code>-g</code> INTEGER None Number of GPUs <code>--env</code> <code>-e</code> TEXT None Environment variables (e.g., <code>--env KEY=VALUE</code>) <code>--replicas</code> <code>-r</code> INTEGER 1 Number of replicas to create <code>--debug</code> - Flag - Enable debug logging <code>--dry-run</code> - Flag - Perform a dry run without creating the session <p>Create a Jupyter Notebook</p> <pre><code>canfar create --cpu 4 --memory 8 notebook skaha/astroml-notebook:latest\n</code></pre> <p>Create a Headless Session with Custom Command</p> <pre><code>canfar create headless skaha/terminal:1.1.2 -- env\n</code></pre>"},{"location":"cli/cli-help/#canfar-ps","title":"<code>canfar ps</code>","text":"<p>Show running sessions.</p> <pre><code>canfar ps [OPTIONS]\n</code></pre>"},{"location":"cli/cli-help/#options_3","title":"Options","text":"Option Short Type Description <code>--all</code> <code>-a</code> Flag Show all sessions (default shows just running) <code>--quiet</code> <code>-q</code> Flag Only show session IDs <code>--kind</code> <code>-k</code> Choice Filter by session kind: <code>desktop</code>, <code>notebook</code>, <code>carta</code>, <code>headless</code>, <code>firefly</code>, <code>desktop-app</code>, <code>contributed</code> <code>--status</code> <code>-s</code> Choice Filter by status: <code>Pending</code>, <code>Running</code>, <code>Terminating</code>, <code>Succeeded</code>, <code>Error</code>, <code>Failed</code> <code>--debug</code> - Flag Enable debug logging <p>List All Sessions</p> <pre><code>canfar ps --all\n</code></pre> <p>List Only Notebook Sessions</p> <pre><code>canfar ps --kind notebook\n</code></pre>"},{"location":"cli/cli-help/#canfar-events","title":"<code>canfar events</code>","text":"<p>Show session events for debugging and monitoring.</p> <pre><code>canfar events [OPTIONS] SESSION_IDS...\n</code></pre> <p>Arguments: - <code>SESSION_IDS...</code> (required): One or more session IDs</p>"},{"location":"cli/cli-help/#options_4","title":"Options","text":"Option Description <code>--debug</code> Enable debug logging <p>Example</p> <pre><code>canfar events abc123 def456\n</code></pre>"},{"location":"cli/cli-help/#canfar-info","title":"<code>canfar info</code>","text":"<p>Show detailed information about sessions.</p> <pre><code>canfar info [OPTIONS] SESSION_IDS...\n</code></pre> <p>Arguments: - <code>SESSION_IDS...</code> (required): One or more session IDs</p>"},{"location":"cli/cli-help/#options_5","title":"Options","text":"Option Description <code>--debug</code> Enable debug logging <p>Example</p> <pre><code>canfar info abc123\n</code></pre>"},{"location":"cli/cli-help/#canfar-open","title":"<code>canfar open</code>","text":"<p>Open sessions in a web browser.</p> <pre><code>canfar open [OPTIONS] SESSION_IDS...\n</code></pre> <p>Arguments: - <code>SESSION_IDS...</code> (required): One or more session IDs</p>"},{"location":"cli/cli-help/#options_6","title":"Options","text":"Option Description <code>--debug</code> Enable debug logging <p>Browser Integration</p> <p>This command automatically opens the session URLs in your default web browser.</p> <p>Example</p> <pre><code>canfar open abc123 def456\n</code></pre>"},{"location":"cli/cli-help/#canfar-logs","title":"<code>canfar logs</code>","text":"<p>Show session logs for troubleshooting.</p> <pre><code>canfar logs [OPTIONS] SESSION_IDS...\n</code></pre> <p>Arguments: - <code>SESSION_IDS...</code> (required): One or more session IDs</p>"},{"location":"cli/cli-help/#options_7","title":"Options","text":"Option Description <code>--debug</code> Enable debug logging <p>Example</p> <pre><code>canfar logs abc123\n</code></pre>"},{"location":"cli/cli-help/#canfar-delete","title":"<code>canfar delete</code>","text":"<p>Delete one or more sessions.</p> <pre><code>canfar delete [OPTIONS] SESSION_IDS...\n</code></pre> <p>Arguments: - <code>SESSION_IDS...</code> (required): One or more session IDs to delete</p>"},{"location":"cli/cli-help/#options_8","title":"Options","text":"Option Short Description <code>--force</code> <code>-f</code> Force deletion without confirmation <code>--debug</code> - Enable debug logging <p>Permanent Action</p> <p>Deleted sessions cannot be recovered. Use <code>--force</code> to skip confirmation prompts.</p> <p>Delete with Confirmation</p> <pre><code>canfar delete abc123\n</code></pre> <p>Force Delete Multiple Sessions</p> <pre><code>canfar delete abc123 def456 --force\n</code></pre>"},{"location":"cli/cli-help/#canfar-prune","title":"<code>canfar prune</code>","text":"<p>Prune sessions by criteria for bulk cleanup.</p> <pre><code>canfar prune [OPTIONS] NAME KIND STATUS\n</code></pre> <p>Arguments: - <code>NAME</code> (required): Prefix to match session names - <code>KIND</code> (optional): Session kind - default: <code>headless</code> (one of: <code>desktop</code>, <code>notebook</code>, <code>carta</code>, <code>headless</code>, <code>firefly</code>, <code>desktop-app</code>, <code>contributed</code>) - <code>STATUS</code> (optional): Session status - default: <code>Succeeded</code> (one of: <code>Pending</code>, <code>Running</code>, <code>Terminating</code>, <code>Succeeded</code>, <code>Error</code>, <code>Failed</code>)</p>"},{"location":"cli/cli-help/#options_9","title":"Options","text":"Option Short Description <code>--debug</code> - Enable debug logging <code>--help</code> <code>-h</code> Show help message and exit <p>Prune Completed Headless Sessions</p> <pre><code>canfar prune \"test-\" headless Running\n</code></pre> <p>Bulk Cleanup</p> <p>Use prune to clean up multiple sessions that match specific criteria, especially useful for automated workflows.</p>"},{"location":"cli/cli-help/#cluster-information-commands","title":"\ud83d\udcca Cluster Information Commands","text":""},{"location":"cli/cli-help/#canfar-stats","title":"<code>canfar stats</code>","text":"<p>Show cluster statistics and resource usage.</p> <pre><code>canfar stats [OPTIONS]\n</code></pre>"},{"location":"cli/cli-help/#options_10","title":"Options","text":"Option Description <code>--debug</code> Enable debug logging <p>Example</p> <pre><code>canfar stats\n</code></pre> <p>Resource Monitoring</p> <p>This command provides insights into cluster resource usage, helping you understand available capacity.</p>"},{"location":"cli/cli-help/#client-configuration-commands","title":"\u2699\ufe0f Client Configuration Commands","text":""},{"location":"cli/cli-help/#canfar-config","title":"<code>canfar config</code>","text":"<p>Manage client configuration settings.</p>"},{"location":"cli/cli-help/#canfar-config-show-canfar-config-list-canfar-config-ls","title":"<code>canfar config show</code> / <code>canfar config list</code> / <code>canfar config ls</code>","text":"<p>Display the current configuration.</p> <pre><code>canfar config show [OPTIONS]\n</code></pre> <p>Example</p> <pre><code>canfar config ls\n</code></pre>"},{"location":"cli/cli-help/#canfar-config-path","title":"<code>canfar config path</code>","text":"<p>Display the path to the configuration file.</p> <pre><code>canfar config path [OPTIONS]\n</code></pre> <p>Example</p> <pre><code>canfar config path\n</code></pre> <p>Configuration Location</p> <p>Use this command to find where your configuration file is stored for manual editing if needed.</p>"},{"location":"cli/cli-help/#canfar-version","title":"<code>canfar version</code>","text":"<p>View client version and system information.</p> <pre><code>canfar version [OPTIONS]\n</code></pre>"},{"location":"cli/cli-help/#options_11","title":"Options","text":"Option Default Description <code>--debug</code> / <code>--no-debug</code> <code>--no-debug</code> Show detailed information for bug reports <p>Basic Version Info</p> <pre><code>canfar version\n</code></pre> <p>Detailed Debug Information</p> <pre><code>canfar version --debug\n</code></pre>"},{"location":"cli/quick-start/","title":"5-Minute Quick Start","text":"<p>Goal</p> <p>By the end of this guide, you'll have a Jupyter Notebook Session on CANFAR with astronomy tools ready to use.</p> <p>Prerequisites</p> <ul> <li>A CADC Account (Canadian Astronomy Data Centre) - Sign up here</li> <li>You have atleast once logged into the CANFAR Science Platform and Harbor Container Registry.</li> <li>Python 3.10+</li> <li>Basic familiarity with Python and Jupyter notebooks</li> </ul>"},{"location":"cli/quick-start/#installation","title":"Installation","text":"&gt; pip install canfar --upgradeInstalled"},{"location":"cli/quick-start/#authentication","title":"Authentication","text":"Login to CANFAR Science Platform<pre><code>canfar auth login\n</code></pre> canfar auth loginStarting Science Platform LoginFetched CADC in 0.12sFetched SRCnet in 1.15sDiscovery completed in 3.32s (5/18 active)Select a Canfar Server: (Use arrow keys)   \ud83d\udfe2 Canada  SRCnet   \ud83d\udfe2 UK-CAM  SRCnet   \ud83d\udfe2 Swiss   SRCnet   \ud83d\udfe2 Spain   SRCnet \u00bb \ud83d\udfe2 CANFAR  CADCSelected a Canfar Server: \ud83d\udfe2 CANFAR  CADCX509 Certificate AuthenticationUsername: usernameusername@ws.cadc-ccda.hia-iha.nrc-cnrc.gc.caPassword: ***********\u2713 Saving configurationLogin completed successfully! <p>Login Pathways</p> CADC Users with Existing <code>~/.ssl/cadcproxy.pem</code>SRCnet Users <p>If you\u2019re using the CADC CANFAR Science Platform already have a valid certificate at <code>~/.ssl/cadcproxy.pem</code>, the CLI will log you in automatically</p> <pre><code>Starting Science Platform Login\n\u2713 Credentials valid\n\u2713 Authenticated with CADC-CANFAR @ https://ws-uv.canfar.net/skaha\nUse --force to re-authenticate.\n</code></pre> <p>If you are a SRCnet user, you will be required to go through the OpenID Connect login process in your web browser.</p> <pre><code>Starting Science Platform Login\nFetched CADC in 0.13s\nFetched SRCnet in 1.03s\nDiscovery completed in 3.20s (13/19 active)\n? Select a Canfar Server: \ud83d\udfe2 Canada  SRCnet\nDiscovering capabilities for https://src.canfar.net/skaha\nOIDC Authentication for https://src.canfar.net/skaha\nStarting OIDC Device Authentication\n\u2713 OIDC Configuration discovered successfully\n\u2713 OIDC device registered successfully\n\u2713 Follow the link below to authorize:\n</code></pre> Force Re-Login<pre><code>canfar auth login --force\n</code></pre> <p>What just happened?</p> <ul> <li><code>canfar</code> discovered all available Science Platform servers around the world</li> <li>You selected the <code>CADC CANFAR Server</code></li> <li>You logged into the Science Platform using your CADC credentials</li> <li>The Science Platform generated a certificate for you valid for 30 days</li> <li>The certificate is stored in <code>~/.ssl/cadcproxy.pem</code></li> </ul>"},{"location":"cli/quick-start/#launch-your-first-notebook","title":"Launch Your First Notebook","text":"<p>Lets launch a Jupyter notebook with astronomy tools pre-installed, </p> # Launch a notebook sessioncanfar create notebook skaha/astroml-notebook:latestSuccessfully created session 'finish-inmate' (ID: d1tsqexh) <p>What just happened?</p> <ul> <li>We connected to CANFAR using your certificate</li> <li>The CLI defaulted the container image to <code>images.canfar.net/skaha/astroml-notebook:latest</code></li> <li>A Jupyter notebook was launched with the container image</li> <li>A random name was generated for your session, <code>scare-monster</code> in this case</li> <li>The Science Platform allocated resources for your notebook and started it.</li> </ul>"},{"location":"cli/quick-start/#peek-under-the-hood","title":"Peek Under the Hood","text":"# Timeline of events taken to launch the notebook sessioncanfar events $(canfar ps -q) <p>What just happened?</p> <ul> <li>We connected to CANFAR using your certificate</li> <li>We queried the Science Platform for all running sessions via <code>canfar ps -q</code></li> <li>We fetched the events (actions performed by the Science Platform to start your session) for your session</li> <li>The events show the progress of your session being created</li> </ul>"},{"location":"cli/quick-start/#check-status","title":"Check Status","text":"canfar ps                                                CANFAR SessionsSESSION ID  NAME          KIND         STATUS    IMAGE                           CREATED\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500d1tsqexh    finish-inmate notebook     Running   skaha/astroml-notebook:latest   7 minutes <p>What just happened?</p> <ul> <li>We connected to CANFAR using your certificate</li> <li>The status of your session was checked</li> <li>The session is in <code>Running</code> state, ready to use</li> </ul>"},{"location":"cli/quick-start/#get-session-information","title":"Get Session Information","text":"canfar info $(canfar ps -q)  Session ID    d1tsqexh  Name          finish-inmate  Status        Running  Type          notebook  Image         images.canfar.net/skaha/astroml-notebook:latest  User ID       brars  Start Time    13 minutes ago  Expiry Time   3 days and 23.77 hours  Connect URL   https://connect.to/notebook/here  UID           123456789  GID           123456789  Groups        [12345, 67890]  App ID        &lt;none&gt;  CPU Usage     0% of 1 core(s)  RAM Usage     0% of 2G GB  GPU Usage     Not Requested <p>What just happened?</p> <ul> <li>We connected to CANFAR using your certificate</li> <li>The information for your session was fetched</li> <li>When we created a your session, we never specified a name, cpu or memory, so the default values were used</li> <li>The default values are 1 core, 2GB of RAM, and 4 days of lifetime</li> </ul>"},{"location":"cli/quick-start/#access-your-notebook","title":"Access Your Notebook","text":"<p>Check the status and get the URL to access your notebook:</p> canfar open $(canfar ps -q)Opening session tcgle3m3 in a new tab. <p>What just happened?</p> <ul> <li>We connected to CANFAR using your certificate</li> <li><code>canfar ps -q</code> returns only the session ID of your session</li> <li>Your browser opened the notebook in a new tab</li> </ul> <p>Pro Tip</p> <p>The notebook usually takes 60-120 seconds to start. You can also check status from the command line:</p>"},{"location":"cli/quick-start/#start-analyzing","title":"Start Analyzing!","text":"<p>Once your notebook is running, click the URL to open it in your browser. You'll have access to:</p> <ul> <li>Jupyter Lab with a full Python environment</li> <li>Pre-installed astronomy libraries: AstroPy, Matplotlib, SciPy, PyTorch, etc.</li> <li>Storages<ul> <li>Persistent: Your work is automatically saved at <code>/arc/home/username/</code></li> <li>Project: Large datasets shared within your project at <code>/arc/projects/name</code></li> <li>Ephemeral: For temporary data staging, use <code>/scratch/</code></li> </ul> </li> </ul> <p>Try This First</p> <p>In JupyterLab, open a new Notebook and run the following code to verify your environment:</p> <pre><code>import astropy\nfrom astropy.io import fits\nimport matplotlib\nimport numpy as np\n\nprint(f\"AstroPy version: {astropy.__version__}\")\nprint(f\"Matplotlib version: {matplotlib.__version__}\")\nprint(f\"Numpy version: {np.__version__}\")\nprint(f\"GPU available: {torch.cuda.is_available()}\")\nprint(\"Ready for astronomy!\")\n</code></pre>"},{"location":"cli/quick-start/#clean-up","title":"Clean Up","text":"<p>When you're done, clean up your session to free up resources for others:</p> canfar delete $(canfar ps -q)Confirm deletion of 1 session(s)? [y/n] (n): ySuccessfully deleted {'tcgle3m3': True} session(s)."},{"location":"cli/quick-start/#congratulations","title":"Congratulations!","text":"<p>You now have a fully-equipped astronomy computing environment running in the cloud. No software installation, no environment conflicts, no waiting for local resources.</p>"},{"location":"cli/quick-start/#troubleshooting","title":"Troubleshooting","text":"<p>Common Issues</p> <ul> <li>Notebook won't start?<ul> <li>Check available resources: <code>canfar stats</code></li> <li>Try a smaller configuration (fewer cores/RAM)</li> <li>Check session status: <code>canfar ps</code></li> </ul> </li> <li>Can't access notebook URL?</li> <li>Wait 1-2 minutes for full startup</li> <li>Check if you're on a VPN that might block the connection</li> <li>Verify the session is in \"Running\" status</li> </ul>"},{"location":"cli/quick-start/#need-help","title":"Need Help?","text":"<ul> <li>\ud83d\udcac Community Support</li> <li>\ud83d\udc1b Report Issues</li> </ul> <p>Success Story</p> <p>\"I went from never using clouds to analyzing my furry data in under 10 minutes. The setup was so smooth!\" - Tabby Cat, University of Purr</p>"},{"location":"client/advanced-examples/","title":"Advanced Examples","text":"<p>Complex use cases and power-user examples for CANFAR Science Platform.</p>"},{"location":"client/advanced-examples/#quick-start","title":"Quick Start","text":"<p>Info</p> <p>Canfar automatically sets these environment variables in each container:</p> <ul> <li><code>REPLICA_ID</code>: Current container ID (1, 2, 3, ...)</li> <li><code>REPLICA_COUNT</code>: Total number of containers</li> </ul>"},{"location":"client/advanced-examples/#massively-parallel-processing","title":"Massively Parallel Processing","text":"<p>Let's assume you have a large dataset of 1000 FITS files that you want to process in parallel. You have a Python script that can process a single FITS file, and you want to run this script in parallel on 100 different canfar sessions, with each container processing a subset of the files. This is a common pattern for distributed computing on canfar, and can be achieved with a few lines of code.</p> Batch Processing Script<pre><code>from canfar.helpers import distributed\nfrom glob import glob\nfrom your.code import analysis\n\n# Find all FITS files to process\ndatafiles = glob(\"/path/to/data/files/*.fits\")\n\n# Each replica processes its assigned chunk of files\n# The chunk function automatically handles 1-based REPLICA_ID values\nfor datafile in distributed.chunk(datafiles):\n    analysis(datafile)\n</code></pre>"},{"location":"client/advanced-examples/#large-scale-parallel-processing","title":"Large Scale Parallel Processing","text":"API CLI <pre><code>from canfar.session import AsyncSession\n\nasync with AsyncSession() as session:\n    sessions = await session.create(\n        name=\"fits-processing\",\n        image=\"images.canfar.net/your/analysis-container:latest\",\n        kind=\"headless\",\n        cores=8,\n        ram=32,\n        cmd=\"python\",\n        args=[\"/path/to/batch_processing.py\"],\n        replicas=100,\n    )\n    return sessions\n</code></pre> <pre><code>canfar create -c 8 -m 32 -r 100 -n fits-processing headless images.canfar.net/your/analysis-container:latest -- python /path/to/batch_processing.py\n</code></pre>"},{"location":"client/advanced-examples/#distributed-processing-strategies","title":"Distributed Processing Strategies","text":"<p>The <code>canfar.helpers.distributed</code> module provides two main strategies for distributing data across replicas:</p>"},{"location":"client/advanced-examples/#chunking-distributedchunk","title":"Chunking (<code>distributed.chunk</code>)","text":"<p>The <code>chunk</code> function divides your data into contiguous blocks, with each replica processing a consecutive chunk. The function uses 1-based replica IDs (matching canfar's <code>REPLICA_ID</code> environment variable):</p> Chunking Example<pre><code>from canfar.helpers import distributed\n\n# With 1000 files and 100 replicas:\n# - Replica 1 processes files 0-9\n# - Replica 2 processes files 10-19  \n# - Replica 3 processes files 20-29\n# - And so on...\n\ndatafiles = glob(\"/path/to/data/*.fits\")\nfor datafile in distributed.chunk(datafiles):\n    process_datafile(datafile)\n</code></pre>"},{"location":"client/advanced-examples/#striping-distributedstripe","title":"Striping (<code>distributed.stripe</code>)","text":"<p>The <code>stripe</code> function distributes data in a round-robin fashion, which is useful when file sizes vary significantly:</p> Striping Example<pre><code>from canfar.helpers import distributed\n\n# With 1000 files and 100 replicas:\n# - Replica 1 processes files 0, 100, 200, 300, ...\n# - Replica 2 processes files 1, 101, 201, 301, ...\n# - Replica 3 processes files 2, 102, 202, 302, ...\n# - And so on...\n\ndatafiles = glob(\"/path/to/data/*.fits\")\nfor datafile in distributed.stripe(datafiles):\n    process_datafile(datafile)\n</code></pre>"},{"location":"client/advanced-examples/#when-to-use-each-strategy","title":"When to Use Each Strategy","text":"<ul> <li>Use <code>chunk</code> when files are similar in size and you want each replica to process a contiguous block of data</li> <li>Use <code>stripe</code> when file sizes vary significantly, as it distributes the workload more evenly across replicas</li> </ul>"},{"location":"client/advanced-examples/#real-world-example-processing-astronomical-data","title":"Real-World Example: Processing Astronomical Data","text":"<pre><code>import os\nimport json\nfrom pathlib import Path\nfrom canfar.helpers.distributed import chunk\n\ndef process_observations():\n    \"\"\"Process FITS files across multiple containers.\"\"\"\n\n    # Get all observation files\n    fits_files = list(Path(\"/data/observations\").glob(\"*.fits\"))\n    my_files = list(chunk(fits_files))\n\n    if not my_files:\n        print(\"No files assigned to this container\")\n        return\n\n    replica_id = os.environ.get('REPLICA_ID')\n    print(f\"Container {replica_id} processing {len(my_files)} files\")\n\n    # Process each file\n    results = []\n    for fits_file in my_files:\n        # Your analysis code here\n        result = {\"file\": fits_file.name, \"stars_detected\": analyze_fits(fits_file)}\n        results.append(result)\n\n    # Save results with container ID\n    output_file = f\"/results/container_{replica_id}_results.json\"\n    with open(output_file, 'w') as f:\n        json.dump(results, f, indent=2)\n\n    print(f\"Saved {len(results)} results to {output_file}\")\n\ndef analyze_fits(fits_path):\n    \"\"\"Your FITS analysis logic here.\"\"\"\n    return 42  # Placeholder\n</code></pre>"},{"location":"client/advanced-examples/#best-practices","title":"Best Practices","text":"<p>Choose the right function: - Use <code>chunk()</code> when you need contiguous data blocks - Use <code>stripe()</code> for round-robin distribution</p> <p>Handle empty containers: <pre><code>my_data = list(chunk(data))\nif not my_data:\n    print(\"No data for this container\")\n    return\n</code></pre></p> <p>Save results with container ID: <pre><code>import os\nreplica_id = os.environ.get('REPLICA_ID')\noutput_file = f\"/results/container_{replica_id}_results.json\"\n</code></pre></p> <p>Combine results from all containers: <pre><code>from pathlib import Path\nimport json\n\ndef combine_results():\n    \"\"\"Merge results from all containers.\"\"\"\n    all_results = []\n    for result_file in Path(\"/results\").glob(\"container_*_results.json\"):\n        with open(result_file) as f:\n            all_results.extend(json.load(f))\n\n    with open(\"/results/final_results.json\", 'w') as f:\n        json.dump(all_results, f, indent=2)\n</code></pre></p>"},{"location":"client/advanced-examples/#common-issues","title":"Common Issues","text":"<p>Some containers get no data This happens when you have more containers than data items. Handle it gracefully: <pre><code>my_data = list(chunk(data))\nif not my_data:\n    print(\"No data assigned to this container\")\n    return\n</code></pre></p> <p>Debugging distribution <pre><code>import os\nreplica_id = os.environ.get('REPLICA_ID')\nreplica_count = os.environ.get('REPLICA_COUNT')\nprint(f\"Container {replica_id} of {replica_count} processing {len(my_data)} items\")\n</code></pre></p>"},{"location":"client/async_session/","title":"Asynchronous Sessions","text":"<p>Overview</p> <p>CANFAR supports asynchronous sessions using the <code>AsyncSession</code> class while maintaining 1-to-1 compatibility with the <code>Session</code> class.</p> <p>               Bases: <code>HTTPClient</code></p> <p>Asynchronous CANFAR Session Management Client.</p> <p>This class provides methods to manage sessions in the system, including fetching session details, creating new sessions, retrieving logs, and destroying existing sessions.</p> <p>Parameters:</p> Name Type Description Default <code>HTTPClient</code> <code>HTTPClient</code> <p>Base HTTP client for making API requests.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession(\n        server=\"https://something.example.com\",\n        version=\"v1\",\n        token=\"token\",\n        timeout=30,\n        concurrency=100,\n        loglevel=40,\n    )\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>class AsyncSession(HTTPClient):\n    \"\"\"Asynchronous CANFAR Session Management Client.\n\n    This class provides methods to manage sessions in the system,\n    including fetching session details, creating new sessions,\n    retrieving logs, and destroying existing sessions.\n\n    Args:\n        HTTPClient (canfar.client.HTTPClient): Base HTTP client for making API requests.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession(\n                server=\"https://something.example.com\",\n                version=\"v1\",\n                token=\"token\",\n                timeout=30,\n                concurrency=100,\n                loglevel=40,\n            )\n    \"\"\"\n\n    async def fetch(\n        self,\n        kind: Kind | None = None,\n        status: Status | None = None,\n        view: View | None = None,\n    ) -&gt; list[dict[str, str]]:\n        \"\"\"List open sessions for the user.\n\n        Args:\n            kind (Kind | None, optional): Session kind. Defaults to None.\n            status (Status | None, optional): Session status. Defaults to None.\n            view (View | None, optional): Session view level. Defaults to None.\n\n        Notes:\n            By default, only the calling user's sessions are listed. If views is\n            set to 'all', all user sessions are listed (with limited information).\n\n        Returns:\n            list: Sessions information.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; await session.fetch(kind=\"notebook\")\n            [{'id': 'vl91sfzz',\n            'userid': 'brars',\n            'runAsUID': '166169204',\n            'runAsGID': '166169204',\n            'supplementalGroups': [34241,\n            34337,\n            35124,\n            36227,\n            1902365706,\n            1454823273,\n            1025424273],\n            'appid': '&lt;none&gt;',\n            'image': 'image-server/repo/image:version',\n            'type': 'notebook',\n            'status': 'Running',\n            'name': 'notebook1',\n            'startTime': '2025-03-05T21:48:29Z',\n            'expiryTime': '2025-03-09T21:48:29Z',\n            'connectURL': 'https://canfar.net/session/notebook/some/url',\n            'requestedRAM': '8G',\n            'requestedCPUCores': '2',\n            'requestedGPUCores': '0',\n            'ramInUse': '&lt;none&gt;',\n            'gpuRAMInUse': '&lt;none&gt;',\n            'cpuCoresInUse': '&lt;none&gt;',\n            'gpuUtilization': '&lt;none&gt;'}]\n        \"\"\"\n        parameters: dict[str, Any] = build.fetch_parameters(kind, status, view)\n        response: Response = await self.asynclient.get(url=\"session\", params=parameters)\n        data: list[dict[str, str]] = response.json()\n        return data\n\n    async def stats(self) -&gt; dict[str, Any]:\n        \"\"\"Get statistics for the canfar cluster.\n\n        Returns:\n            Dict[str, Any]: Cluster statistics.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; await session.stats()\n            {'instances': {\n             'session': 88, 'desktopApp': 30, 'headless': 0, 'total': 118},\n             'cores': {'requestedCPUCores': 377,\n             'coresAvailable': 960,\n             'maxCores': {'cores': 32, 'withRam': '147Gi'}},\n             'ram': {'maxRAM': {'ram': '226Gi', 'withCores': 32}}}\n        \"\"\"\n        parameters = {\"view\": \"stats\"}\n        response: Response = await self.asynclient.get(\"session\", params=parameters)\n        data: dict[str, Any] = response.json()\n        return data\n\n    async def info(self, ids: list[str] | str) -&gt; list[dict[str, Any]]:\n        \"\"\"Get information about session[s].\n\n        Args:\n            ids (Union[List[str], str]): Session ID[s].\n\n        Returns:\n            Dict[str, Any]: Session information.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; await session.info(session_id=\"hjko98yghj\")\n            &gt;&gt;&gt; await session.info(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        # Convert id to list if it is a string\n        if isinstance(ids, str):\n            ids = [ids]\n        parameters: dict[str, str] = {\"view\": \"event\"}\n        results: list[dict[str, Any]] = []\n        tasks: list[Any] = []\n        semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n\n        async def bounded(value: str) -&gt; dict[str, Any]:\n            async with semaphore:\n                response = await self.asynclient.get(\n                    url=f\"session/{value}\",\n                    params=parameters,\n                )\n                data: dict[str, Any] = response.json()\n                return data\n\n        tasks = [bounded(value) for value in ids]\n        responses = await asyncio.gather(*tasks, return_exceptions=True)\n        for reply in responses:\n            if isinstance(reply, Exception):\n                log.error(reply)\n            elif isinstance(reply, dict):\n                results.append(reply)\n        return results\n\n    async def logs(\n        self,\n        ids: list[str] | str,\n        verbose: bool = False,\n    ) -&gt; dict[str, str] | None:\n        \"\"\"Get logs from a session[s].\n\n        Args:\n            ids (Union[List[str], str]): Session ID[s].\n            verbose (bool, optional): Print logs to stdout. Defaults to False.\n\n        Returns:\n            Dict[str, str]: Logs in text/plain format.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; await session.logs(id=\"hjko98yghj\")\n            &gt;&gt;&gt; await session.logs(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        if isinstance(ids, str):\n            ids = [ids]\n        parameters: dict[str, str] = {\"view\": \"logs\"}\n        results: dict[str, str] = {}\n\n        semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n        tasks: list[Any] = []\n\n        async def bounded(value: str) -&gt; tuple[str, str]:\n            async with semaphore:\n                response = await self.asynclient.get(\n                    url=f\"session/{value}\",\n                    params=parameters,\n                )\n                return value, response.text\n\n        tasks = [bounded(value) for value in ids]\n        responses = await asyncio.gather(*tasks, return_exceptions=True)\n        for reply in responses:\n            if isinstance(reply, Exception):\n                log.error(reply)\n            elif isinstance(reply, tuple):\n                results[reply[0]] = reply[1]\n\n        # Print logs to stdout if verbose is set to True\n        if verbose:\n            for key, value in results.items():\n                log.info(\"Session ID: %s\\n\", key)\n                log.info(value)\n            return None\n        return results\n\n    async def create(\n        self,\n        name: str,\n        image: str,\n        cores: int = 2,\n        ram: int = 4,\n        kind: Kind = \"headless\",\n        gpu: int | None = None,\n        cmd: str | None = None,\n        args: str | None = None,\n        env: dict[str, Any] | None = None,\n        replicas: int = 1,\n    ) -&gt; list[str]:\n        \"\"\"Launch a canfar session.\n\n        Args:\n            name (str): A unique name for the session.\n            image (str): Container image to use for the session.\n            cores (int, optional): Number of cores. Defaults to 2.\n            ram (int, optional): Amount of RAM (GB). Defaults to 4.\n            kind (str, optional): Type of canfar session. Defaults to \"headless\".\n            gpu (Optional[int], optional): Number of GPUs. Defaults to None.\n            cmd (Optional[str], optional): Command to run. Defaults to None.\n            args (Optional[str], optional): Arguments to the command. Defaults to None.\n            env (Optional[Dict[str, Any]], optional): Environment variables to inject.\n                Defaults to None.\n            replicas (int, optional): Number of sessions to launch. Defaults to 1.\n\n        Notes:\n            The name of the session suffixed with the replica number. eg. test-1, test-2\n            Each container will have the following environment variables injected:\n                * REPLICA_ID - The replica number\n                * REPLICA_COUNT - The total number of replicas\n\n        Returns:\n            List[str]: A list of session IDs for the launched sessions.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; session.create(\n                    name=\"test\",\n                    image='images.canfar.net/skaha/terminal:1.1.1',\n                    cores=2,\n                    ram=8,\n                    gpu=1,\n                    kind=\"headless\",\n                    cmd=\"env\",\n                    env={\"TEST\": \"test\"},\n                    replicas=2,\n                )\n            &gt;&gt;&gt; [\"hjko98yghj\", \"ikvp1jtp\"]\n        \"\"\"\n        payloads: list[list[tuple[str, Any]]] = build.create_parameters(\n            name,\n            image,\n            cores,\n            ram,\n            kind,\n            gpu,\n            cmd,\n            args,\n            env,\n            replicas,\n        )\n        results: list[str] = []\n        tasks: list[Any] = []\n        semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n\n        async def bounded(parameters: list[tuple[str, Any]]) -&gt; Any:\n            async with semaphore:\n                log.debug(\"HTTP Request Parameters: %s\", parameters)\n                response = await self.asynclient.post(url=\"session\", params=parameters)\n                return response.text.rstrip(\"\\r\\n\")\n\n        tasks = [bounded(payload) for payload in payloads]\n        msg = f\"Creating {replicas} {kind} session[s].\"\n        log.debug(msg)\n        responses = await asyncio.gather(*tasks, return_exceptions=True)\n        for reply in responses:\n            if isinstance(reply, Exception):\n                log.error(reply)\n            elif isinstance(reply, str):\n                results.append(reply)\n        return results\n\n    async def events(\n        self,\n        ids: str | list[str],\n        verbose: bool = False,\n    ) -&gt; list[dict[str, str]] | None:\n        \"\"\"Get deployment events for a session[s].\n\n        Args:\n            ids (Union[str, List[str]]): Session ID[s].\n            verbose (bool, optional): Print events to stdout. Defaults to False.\n\n        Returns:\n            Optional[List[Dict[str, str]]]: A list of events for the session[s].\n\n        Notes:\n            When verbose is True, the events will be printed to stdout only.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; await session.events(id=\"hjko98yghj\")\n            &gt;&gt;&gt; await session.events(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        if isinstance(ids, str):\n            ids = [ids]\n        results: list[dict[str, str]] = []\n        parameters: dict[str, str] = {\"view\": \"events\"}\n        tasks: list[Any] = []\n        semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n\n        async def bounded(value: str) -&gt; dict[str, str]:\n            async with semaphore:\n                response = await self.asynclient.get(\n                    url=f\"session/{value}\",\n                    params=parameters,\n                )\n                return {value: response.text}\n\n        tasks = [bounded(value) for value in ids]\n        responses = await asyncio.gather(*tasks, return_exceptions=True)\n        for reply in responses:\n            if isinstance(reply, Exception):\n                log.error(reply)\n            elif isinstance(reply, dict):\n                results.append(dict(reply))\n\n        if verbose and results:\n            for result in results:\n                for key, value in result.items():\n                    log.info(\"Session ID: %s\", key)\n                    log.info(value)\n        return results if not verbose else None\n\n    async def destroy(self, ids: str | list[str]) -&gt; dict[str, bool]:\n        \"\"\"Destroy session[s].\n\n        Args:\n            ids (Union[str, List[str]]): Session ID[s].\n\n        Returns:\n            Dict[str, bool]: A dictionary of session IDs\n            and a bool indicating if the session was destroyed.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; await session.destroy(id=\"hjko98yghj\")\n            &gt;&gt;&gt; await session.destroy(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        if isinstance(ids, str):\n            ids = [ids]\n        results: dict[str, bool] = {}\n        semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n        tasks: list[Any] = []\n\n        async def bounded(value: str) -&gt; tuple[str, bool]:\n            async with semaphore:\n                try:\n                    await self.asynclient.delete(url=f\"session/{value}\")\n                except HTTPError as err:\n                    msg = f\"Failed to destroy session {value}: {err}\"\n                    log.exception(msg)\n                    return value, False\n                else:\n                    return value, True\n\n        tasks = [bounded(value) for value in ids]\n        responses = await asyncio.gather(*tasks, return_exceptions=True)\n        for reply in responses:\n            if isinstance(reply, tuple):\n                results[reply[0]] = reply[1]\n        return results\n\n    async def destroy_with(\n        self,\n        prefix: str,\n        kind: Kind = \"headless\",\n        status: Status = \"Succeeded\",\n    ) -&gt; dict[str, bool]:\n        \"\"\"Destroy session[s] matching search criteria.\n\n        Args:\n            prefix (str): Prefix to match in the session name.\n            kind (Kind): Type of session. Defaults to \"headless\".\n            status (Status): Status of the session. Defaults to \"Succeeded\".\n\n\n        Returns:\n            Dict[str, bool]: A dictionary of session IDs\n            and a bool indicating if the session was destroyed.\n\n        Notes:\n            The prefix is case-sensitive.\n            This method is useful for destroying multiple sessions at once.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; await session.destroy_with(prefix=\"test\")\n            &gt;&gt;&gt; await session.destroy_with(prefix=\"test\", kind=\"desktop\")\n            &gt;&gt;&gt; await session.destroy_with(prefix=\"car\", kind=\"carta\", status=\"Running\")\n\n        \"\"\"\n        ids: list[str] = [\n            session[\"id\"]\n            for session in await self.fetch(kind=kind, status=status)\n            if session[\"name\"].startswith(prefix)\n        ]\n        return await self.destroy(ids)\n\n    async def connect(self, ids: list[str] | str) -&gt; None:\n        \"\"\"Connect to a session[s] in a web browser.\n\n        Args:\n            ids (Union[List[str], str]): Session ID[s].\n\n        Examples:\n            &gt;&gt;&gt; from canfar.sessions import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; await session.connect(id=\"hjko98yghj\")\n            &gt;&gt;&gt; await session.connect(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        if isinstance(ids, str):\n            ids = [ids]\n        info = await self.info(ids)\n        for session in info:\n            connect_url = session.get(\"connectURL\")\n            if connect_url:\n                open_new_tab(connect_url)\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.connect","title":"<code>connect(ids)</code>  <code>async</code>","text":"<p>Connect to a session[s] in a web browser.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>Union[List[str], str]</code> <p>Session ID[s].</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.sessions import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; await session.connect(id=\"hjko98yghj\")\n&gt;&gt;&gt; await session.connect(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def connect(self, ids: list[str] | str) -&gt; None:\n    \"\"\"Connect to a session[s] in a web browser.\n\n    Args:\n        ids (Union[List[str], str]): Session ID[s].\n\n    Examples:\n        &gt;&gt;&gt; from canfar.sessions import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; await session.connect(id=\"hjko98yghj\")\n        &gt;&gt;&gt; await session.connect(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    if isinstance(ids, str):\n        ids = [ids]\n    info = await self.info(ids)\n    for session in info:\n        connect_url = session.get(\"connectURL\")\n        if connect_url:\n            open_new_tab(connect_url)\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.create","title":"<code>create(name, image, cores=2, ram=4, kind='headless', gpu=None, cmd=None, args=None, env=None, replicas=1)</code>  <code>async</code>","text":"<p>Launch a canfar session.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A unique name for the session.</p> required <code>image</code> <code>str</code> <p>Container image to use for the session.</p> required <code>cores</code> <code>int</code> <p>Number of cores. Defaults to 2.</p> <code>2</code> <code>ram</code> <code>int</code> <p>Amount of RAM (GB). Defaults to 4.</p> <code>4</code> <code>kind</code> <code>str</code> <p>Type of canfar session. Defaults to \"headless\".</p> <code>'headless'</code> <code>gpu</code> <code>Optional[int]</code> <p>Number of GPUs. Defaults to None.</p> <code>None</code> <code>cmd</code> <code>Optional[str]</code> <p>Command to run. Defaults to None.</p> <code>None</code> <code>args</code> <code>Optional[str]</code> <p>Arguments to the command. Defaults to None.</p> <code>None</code> <code>env</code> <code>Optional[Dict[str, Any]]</code> <p>Environment variables to inject. Defaults to None.</p> <code>None</code> <code>replicas</code> <code>int</code> <p>Number of sessions to launch. Defaults to 1.</p> <code>1</code> Notes <p>The name of the session suffixed with the replica number. eg. test-1, test-2 Each container will have the following environment variables injected:     * REPLICA_ID - The replica number     * REPLICA_COUNT - The total number of replicas</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: A list of session IDs for the launched sessions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; session.create(\n        name=\"test\",\n        image='images.canfar.net/skaha/terminal:1.1.1',\n        cores=2,\n        ram=8,\n        gpu=1,\n        kind=\"headless\",\n        cmd=\"env\",\n        env={\"TEST\": \"test\"},\n        replicas=2,\n    )\n&gt;&gt;&gt; [\"hjko98yghj\", \"ikvp1jtp\"]\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def create(\n    self,\n    name: str,\n    image: str,\n    cores: int = 2,\n    ram: int = 4,\n    kind: Kind = \"headless\",\n    gpu: int | None = None,\n    cmd: str | None = None,\n    args: str | None = None,\n    env: dict[str, Any] | None = None,\n    replicas: int = 1,\n) -&gt; list[str]:\n    \"\"\"Launch a canfar session.\n\n    Args:\n        name (str): A unique name for the session.\n        image (str): Container image to use for the session.\n        cores (int, optional): Number of cores. Defaults to 2.\n        ram (int, optional): Amount of RAM (GB). Defaults to 4.\n        kind (str, optional): Type of canfar session. Defaults to \"headless\".\n        gpu (Optional[int], optional): Number of GPUs. Defaults to None.\n        cmd (Optional[str], optional): Command to run. Defaults to None.\n        args (Optional[str], optional): Arguments to the command. Defaults to None.\n        env (Optional[Dict[str, Any]], optional): Environment variables to inject.\n            Defaults to None.\n        replicas (int, optional): Number of sessions to launch. Defaults to 1.\n\n    Notes:\n        The name of the session suffixed with the replica number. eg. test-1, test-2\n        Each container will have the following environment variables injected:\n            * REPLICA_ID - The replica number\n            * REPLICA_COUNT - The total number of replicas\n\n    Returns:\n        List[str]: A list of session IDs for the launched sessions.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; session.create(\n                name=\"test\",\n                image='images.canfar.net/skaha/terminal:1.1.1',\n                cores=2,\n                ram=8,\n                gpu=1,\n                kind=\"headless\",\n                cmd=\"env\",\n                env={\"TEST\": \"test\"},\n                replicas=2,\n            )\n        &gt;&gt;&gt; [\"hjko98yghj\", \"ikvp1jtp\"]\n    \"\"\"\n    payloads: list[list[tuple[str, Any]]] = build.create_parameters(\n        name,\n        image,\n        cores,\n        ram,\n        kind,\n        gpu,\n        cmd,\n        args,\n        env,\n        replicas,\n    )\n    results: list[str] = []\n    tasks: list[Any] = []\n    semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n\n    async def bounded(parameters: list[tuple[str, Any]]) -&gt; Any:\n        async with semaphore:\n            log.debug(\"HTTP Request Parameters: %s\", parameters)\n            response = await self.asynclient.post(url=\"session\", params=parameters)\n            return response.text.rstrip(\"\\r\\n\")\n\n    tasks = [bounded(payload) for payload in payloads]\n    msg = f\"Creating {replicas} {kind} session[s].\"\n    log.debug(msg)\n    responses = await asyncio.gather(*tasks, return_exceptions=True)\n    for reply in responses:\n        if isinstance(reply, Exception):\n            log.error(reply)\n        elif isinstance(reply, str):\n            results.append(reply)\n    return results\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.destroy","title":"<code>destroy(ids)</code>  <code>async</code>","text":"<p>Destroy session[s].</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>Union[str, List[str]]</code> <p>Session ID[s].</p> required <p>Returns:</p> Type Description <code>dict[str, bool]</code> <p>Dict[str, bool]: A dictionary of session IDs</p> <code>dict[str, bool]</code> <p>and a bool indicating if the session was destroyed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; await session.destroy(id=\"hjko98yghj\")\n&gt;&gt;&gt; await session.destroy(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def destroy(self, ids: str | list[str]) -&gt; dict[str, bool]:\n    \"\"\"Destroy session[s].\n\n    Args:\n        ids (Union[str, List[str]]): Session ID[s].\n\n    Returns:\n        Dict[str, bool]: A dictionary of session IDs\n        and a bool indicating if the session was destroyed.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; await session.destroy(id=\"hjko98yghj\")\n        &gt;&gt;&gt; await session.destroy(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    if isinstance(ids, str):\n        ids = [ids]\n    results: dict[str, bool] = {}\n    semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n    tasks: list[Any] = []\n\n    async def bounded(value: str) -&gt; tuple[str, bool]:\n        async with semaphore:\n            try:\n                await self.asynclient.delete(url=f\"session/{value}\")\n            except HTTPError as err:\n                msg = f\"Failed to destroy session {value}: {err}\"\n                log.exception(msg)\n                return value, False\n            else:\n                return value, True\n\n    tasks = [bounded(value) for value in ids]\n    responses = await asyncio.gather(*tasks, return_exceptions=True)\n    for reply in responses:\n        if isinstance(reply, tuple):\n            results[reply[0]] = reply[1]\n    return results\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.destroy_with","title":"<code>destroy_with(prefix, kind='headless', status='Succeeded')</code>  <code>async</code>","text":"<p>Destroy session[s] matching search criteria.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Prefix to match in the session name.</p> required <code>kind</code> <code>Kind</code> <p>Type of session. Defaults to \"headless\".</p> <code>'headless'</code> <code>status</code> <code>Status</code> <p>Status of the session. Defaults to \"Succeeded\".</p> <code>'Succeeded'</code> <p>Returns:</p> Type Description <code>dict[str, bool]</code> <p>Dict[str, bool]: A dictionary of session IDs</p> <code>dict[str, bool]</code> <p>and a bool indicating if the session was destroyed.</p> Notes <p>The prefix is case-sensitive. This method is useful for destroying multiple sessions at once.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; await session.destroy_with(prefix=\"test\")\n&gt;&gt;&gt; await session.destroy_with(prefix=\"test\", kind=\"desktop\")\n&gt;&gt;&gt; await session.destroy_with(prefix=\"car\", kind=\"carta\", status=\"Running\")\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def destroy_with(\n    self,\n    prefix: str,\n    kind: Kind = \"headless\",\n    status: Status = \"Succeeded\",\n) -&gt; dict[str, bool]:\n    \"\"\"Destroy session[s] matching search criteria.\n\n    Args:\n        prefix (str): Prefix to match in the session name.\n        kind (Kind): Type of session. Defaults to \"headless\".\n        status (Status): Status of the session. Defaults to \"Succeeded\".\n\n\n    Returns:\n        Dict[str, bool]: A dictionary of session IDs\n        and a bool indicating if the session was destroyed.\n\n    Notes:\n        The prefix is case-sensitive.\n        This method is useful for destroying multiple sessions at once.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; await session.destroy_with(prefix=\"test\")\n        &gt;&gt;&gt; await session.destroy_with(prefix=\"test\", kind=\"desktop\")\n        &gt;&gt;&gt; await session.destroy_with(prefix=\"car\", kind=\"carta\", status=\"Running\")\n\n    \"\"\"\n    ids: list[str] = [\n        session[\"id\"]\n        for session in await self.fetch(kind=kind, status=status)\n        if session[\"name\"].startswith(prefix)\n    ]\n    return await self.destroy(ids)\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.events","title":"<code>events(ids, verbose=False)</code>  <code>async</code>","text":"<p>Get deployment events for a session[s].</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>Union[str, List[str]]</code> <p>Session ID[s].</p> required <code>verbose</code> <code>bool</code> <p>Print events to stdout. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[dict[str, str]] | None</code> <p>Optional[List[Dict[str, str]]]: A list of events for the session[s].</p> Notes <p>When verbose is True, the events will be printed to stdout only.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; await session.events(id=\"hjko98yghj\")\n&gt;&gt;&gt; await session.events(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def events(\n    self,\n    ids: str | list[str],\n    verbose: bool = False,\n) -&gt; list[dict[str, str]] | None:\n    \"\"\"Get deployment events for a session[s].\n\n    Args:\n        ids (Union[str, List[str]]): Session ID[s].\n        verbose (bool, optional): Print events to stdout. Defaults to False.\n\n    Returns:\n        Optional[List[Dict[str, str]]]: A list of events for the session[s].\n\n    Notes:\n        When verbose is True, the events will be printed to stdout only.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; await session.events(id=\"hjko98yghj\")\n        &gt;&gt;&gt; await session.events(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    if isinstance(ids, str):\n        ids = [ids]\n    results: list[dict[str, str]] = []\n    parameters: dict[str, str] = {\"view\": \"events\"}\n    tasks: list[Any] = []\n    semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n\n    async def bounded(value: str) -&gt; dict[str, str]:\n        async with semaphore:\n            response = await self.asynclient.get(\n                url=f\"session/{value}\",\n                params=parameters,\n            )\n            return {value: response.text}\n\n    tasks = [bounded(value) for value in ids]\n    responses = await asyncio.gather(*tasks, return_exceptions=True)\n    for reply in responses:\n        if isinstance(reply, Exception):\n            log.error(reply)\n        elif isinstance(reply, dict):\n            results.append(dict(reply))\n\n    if verbose and results:\n        for result in results:\n            for key, value in result.items():\n                log.info(\"Session ID: %s\", key)\n                log.info(value)\n    return results if not verbose else None\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.fetch","title":"<code>fetch(kind=None, status=None, view=None)</code>  <code>async</code>","text":"<p>List open sessions for the user.</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>Kind | None</code> <p>Session kind. Defaults to None.</p> <code>None</code> <code>status</code> <code>Status | None</code> <p>Session status. Defaults to None.</p> <code>None</code> <code>view</code> <code>View | None</code> <p>Session view level. Defaults to None.</p> <code>None</code> Notes <p>By default, only the calling user's sessions are listed. If views is set to 'all', all user sessions are listed (with limited information).</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list[dict[str, str]]</code> <p>Sessions information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; await session.fetch(kind=\"notebook\")\n[{'id': 'vl91sfzz',\n'userid': 'brars',\n'runAsUID': '166169204',\n'runAsGID': '166169204',\n'supplementalGroups': [34241,\n34337,\n35124,\n36227,\n1902365706,\n1454823273,\n1025424273],\n'appid': '&lt;none&gt;',\n'image': 'image-server/repo/image:version',\n'type': 'notebook',\n'status': 'Running',\n'name': 'notebook1',\n'startTime': '2025-03-05T21:48:29Z',\n'expiryTime': '2025-03-09T21:48:29Z',\n'connectURL': 'https://canfar.net/session/notebook/some/url',\n'requestedRAM': '8G',\n'requestedCPUCores': '2',\n'requestedGPUCores': '0',\n'ramInUse': '&lt;none&gt;',\n'gpuRAMInUse': '&lt;none&gt;',\n'cpuCoresInUse': '&lt;none&gt;',\n'gpuUtilization': '&lt;none&gt;'}]\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def fetch(\n    self,\n    kind: Kind | None = None,\n    status: Status | None = None,\n    view: View | None = None,\n) -&gt; list[dict[str, str]]:\n    \"\"\"List open sessions for the user.\n\n    Args:\n        kind (Kind | None, optional): Session kind. Defaults to None.\n        status (Status | None, optional): Session status. Defaults to None.\n        view (View | None, optional): Session view level. Defaults to None.\n\n    Notes:\n        By default, only the calling user's sessions are listed. If views is\n        set to 'all', all user sessions are listed (with limited information).\n\n    Returns:\n        list: Sessions information.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; await session.fetch(kind=\"notebook\")\n        [{'id': 'vl91sfzz',\n        'userid': 'brars',\n        'runAsUID': '166169204',\n        'runAsGID': '166169204',\n        'supplementalGroups': [34241,\n        34337,\n        35124,\n        36227,\n        1902365706,\n        1454823273,\n        1025424273],\n        'appid': '&lt;none&gt;',\n        'image': 'image-server/repo/image:version',\n        'type': 'notebook',\n        'status': 'Running',\n        'name': 'notebook1',\n        'startTime': '2025-03-05T21:48:29Z',\n        'expiryTime': '2025-03-09T21:48:29Z',\n        'connectURL': 'https://canfar.net/session/notebook/some/url',\n        'requestedRAM': '8G',\n        'requestedCPUCores': '2',\n        'requestedGPUCores': '0',\n        'ramInUse': '&lt;none&gt;',\n        'gpuRAMInUse': '&lt;none&gt;',\n        'cpuCoresInUse': '&lt;none&gt;',\n        'gpuUtilization': '&lt;none&gt;'}]\n    \"\"\"\n    parameters: dict[str, Any] = build.fetch_parameters(kind, status, view)\n    response: Response = await self.asynclient.get(url=\"session\", params=parameters)\n    data: list[dict[str, str]] = response.json()\n    return data\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.info","title":"<code>info(ids)</code>  <code>async</code>","text":"<p>Get information about session[s].</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>Union[List[str], str]</code> <p>Session ID[s].</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>Dict[str, Any]: Session information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; await session.info(session_id=\"hjko98yghj\")\n&gt;&gt;&gt; await session.info(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def info(self, ids: list[str] | str) -&gt; list[dict[str, Any]]:\n    \"\"\"Get information about session[s].\n\n    Args:\n        ids (Union[List[str], str]): Session ID[s].\n\n    Returns:\n        Dict[str, Any]: Session information.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; await session.info(session_id=\"hjko98yghj\")\n        &gt;&gt;&gt; await session.info(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    # Convert id to list if it is a string\n    if isinstance(ids, str):\n        ids = [ids]\n    parameters: dict[str, str] = {\"view\": \"event\"}\n    results: list[dict[str, Any]] = []\n    tasks: list[Any] = []\n    semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n\n    async def bounded(value: str) -&gt; dict[str, Any]:\n        async with semaphore:\n            response = await self.asynclient.get(\n                url=f\"session/{value}\",\n                params=parameters,\n            )\n            data: dict[str, Any] = response.json()\n            return data\n\n    tasks = [bounded(value) for value in ids]\n    responses = await asyncio.gather(*tasks, return_exceptions=True)\n    for reply in responses:\n        if isinstance(reply, Exception):\n            log.error(reply)\n        elif isinstance(reply, dict):\n            results.append(reply)\n    return results\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.logs","title":"<code>logs(ids, verbose=False)</code>  <code>async</code>","text":"<p>Get logs from a session[s].</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>Union[List[str], str]</code> <p>Session ID[s].</p> required <code>verbose</code> <code>bool</code> <p>Print logs to stdout. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, str] | None</code> <p>Dict[str, str]: Logs in text/plain format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; await session.logs(id=\"hjko98yghj\")\n&gt;&gt;&gt; await session.logs(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def logs(\n    self,\n    ids: list[str] | str,\n    verbose: bool = False,\n) -&gt; dict[str, str] | None:\n    \"\"\"Get logs from a session[s].\n\n    Args:\n        ids (Union[List[str], str]): Session ID[s].\n        verbose (bool, optional): Print logs to stdout. Defaults to False.\n\n    Returns:\n        Dict[str, str]: Logs in text/plain format.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; await session.logs(id=\"hjko98yghj\")\n        &gt;&gt;&gt; await session.logs(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    if isinstance(ids, str):\n        ids = [ids]\n    parameters: dict[str, str] = {\"view\": \"logs\"}\n    results: dict[str, str] = {}\n\n    semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n    tasks: list[Any] = []\n\n    async def bounded(value: str) -&gt; tuple[str, str]:\n        async with semaphore:\n            response = await self.asynclient.get(\n                url=f\"session/{value}\",\n                params=parameters,\n            )\n            return value, response.text\n\n    tasks = [bounded(value) for value in ids]\n    responses = await asyncio.gather(*tasks, return_exceptions=True)\n    for reply in responses:\n        if isinstance(reply, Exception):\n            log.error(reply)\n        elif isinstance(reply, tuple):\n            results[reply[0]] = reply[1]\n\n    # Print logs to stdout if verbose is set to True\n    if verbose:\n        for key, value in results.items():\n            log.info(\"Session ID: %s\\n\", key)\n            log.info(value)\n        return None\n    return results\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.stats","title":"<code>stats()</code>  <code>async</code>","text":"<p>Get statistics for the canfar cluster.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: Cluster statistics.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; await session.stats()\n{'instances': {\n 'session': 88, 'desktopApp': 30, 'headless': 0, 'total': 118},\n 'cores': {'requestedCPUCores': 377,\n 'coresAvailable': 960,\n 'maxCores': {'cores': 32, 'withRam': '147Gi'}},\n 'ram': {'maxRAM': {'ram': '226Gi', 'withCores': 32}}}\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get statistics for the canfar cluster.\n\n    Returns:\n        Dict[str, Any]: Cluster statistics.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; await session.stats()\n        {'instances': {\n         'session': 88, 'desktopApp': 30, 'headless': 0, 'total': 118},\n         'cores': {'requestedCPUCores': 377,\n         'coresAvailable': 960,\n         'maxCores': {'cores': 32, 'withRam': '147Gi'}},\n         'ram': {'maxRAM': {'ram': '226Gi', 'withCores': 32}}}\n    \"\"\"\n    parameters = {\"view\": \"stats\"}\n    response: Response = await self.asynclient.get(\"session\", params=parameters)\n    data: dict[str, Any] = response.json()\n    return data\n</code></pre>"},{"location":"client/client/","title":"HTTPClient","text":"<p>The <code>canfar.client</code> module provides a comprehensive HTTP client for interacting with CANFAR Science Platform services. Built on the powerful <code>httpx</code> library, it offers both synchronous and asynchronous interfaces with advanced authentication capabilities.</p>"},{"location":"client/client/#features","title":"Features","text":"<p>Key Capabilities</p> <ul> <li>Multiple Authentication Methods: X.509 certificates, OIDC tokens, and bearer tokens</li> <li>Automatic SSL Configuration: Seamless certificate-based authentication</li> <li>Async/Sync Support: Both synchronous and asynchronous HTTP clients</li> <li>Connection Pooling: Optimized for concurrent requests</li> <li>Debug Logging: Comprehensive logging for troubleshooting</li> <li>Context Managers: Proper resource management</li> </ul> <p>This is a low-level client that is used by all other API clients in Canfar. It is not intended to be used directly by users, but rather as a building block for other clients and contributors.</p>"},{"location":"client/client/#authentication-modes","title":"Authentication Modes","text":"<p>The client supports multiple authentication modes that can be configured through the authentication system:</p>"},{"location":"client/client/#debug-logging","title":"Debug Logging","text":"<pre><code>import logging\nfrom canfar.client import HTTPClient\n\n# Enable debug logging to see client creation details\nclient = HTTPClient(loglevel=logging.DEBUG)\n\n# This will log:\n# - Authentication mode selection\n# - SSL context creation\n# - Header generation\n# - Client configuration\n</code></pre>"},{"location":"client/client/#configuration","title":"Configuration","text":"<p>The client inherits from the <code>Configuration</code> class and supports all configuration options:</p> <pre><code>from canfar.client import HTTPClient\n\nclient = HTTPClient(\n    timeout=60,           # Request timeout in seconds\n    concurrency=64,       # Max concurrent connections\n    loglevel=20,         # Logging level (INFO)\n)\n</code></pre>"},{"location":"client/client/#authentication-expiry","title":"Authentication Expiry","text":"<p>The client provides an <code>expiry</code> property that returns the expiry time for the current authentication method:</p> <pre><code>import time\n\nclient = HTTPClient()\n\nif client.expiry:\n    time_left = client.expiry - time.time()\n    print(f\"Authentication expires in {time_left:.0f} seconds\")\nelse:\n    print(\"No expiry tracking (user-provided credentials)\")\n</code></pre> <p>Expiry Tracking</p> <p>The <code>expiry</code> property returns <code>None</code> for user-provided certificates or tokens since the client cannot track their expiry automatically.</p>"},{"location":"client/client/#error-handling","title":"Error Handling","text":"<p>The client includes built-in error handling for HTTP responses:</p> <pre><code>from httpx import HTTPStatusError\n\ntry:\n    response = client.client.get(\"/invalid-endpoint\")\n    response.raise_for_status()\nexcept HTTPStatusError as e:\n    print(f\"HTTP error: {e.response.status_code}\")\n</code></pre>"},{"location":"client/client/#api-reference","title":"API Reference","text":""},{"location":"client/client/#canfar.client.HTTPClient","title":"<code>canfar.client.HTTPClient</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>HTTP Client for interacting with CANFAR Science Platform services (V2).</p> <p>This client uses a composition-based approach and inherits from Pydantic's BaseSettings to allow for flexible configuration via arguments, environment variables, or a configuration file.</p> <p>The client prioritizes credentials in the following order:</p> <ol> <li>Runtime Arguments/Environment Variables: A <code>token</code> or <code>certificate</code>     provided at instantiation (e.g., <code>CANFAR_TOKEN=\"...\"</code>).</li> <li>Active Configuration Context: The context specified by <code>active_context</code>     in the loaded configuration file.</li> </ol> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration is invalid.</p>"},{"location":"client/client/#canfar.client.HTTPClient.client","title":"<code>client: Client</code>  <code>property</code>","text":"<p>Get the synchronous HTTPx Client.</p> <p>Returns:</p> Name Type Description <code>Client</code> <code>Client</code> <p>The synchronous HTTPx client.</p>"},{"location":"client/client/#canfar.client.HTTPClient.asynclient","title":"<code>asynclient: AsyncClient</code>  <code>property</code>","text":"<p>Get the asynchronous HTTPx Async Client.</p>"},{"location":"client/context/","title":"Context API","text":"<p>Overview</p> <p>The Context API allows the user to get information about the resources available to be requested for a session on the CANFAR Science Platform. This information can be used to configure the session to request the appropriate resources for your session.</p> Get context information<pre><code>from canfar.context import Context\n\ncontext = Context()\ncontext.resources()\n</code></pre> <pre><code>{\n    \"cores\": {\n        \"default\": 1,\n        \"defaultRequest\": 1,\n        \"defaultLimit\": 16,\n        \"defaultHeadless\": 1,\n        \"options\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16],\n    },\n    \"memoryGB\": {\n        \"default\": 2,\n        \"defaultRequest\": 4,\n        \"defaultLimit\": 192,\n        \"defaultHeadless\": 4,\n        \"options\": [1, 2, ..., 192],\n    },\n    \"gpus\": {\n        \"options\": [1, ..., 8],\n    },\n}\n</code></pre> <p>               Bases: <code>HTTPClient</code></p> <p>CANFAR Context.</p> <p>Parameters:</p> Name Type Description Default <code>HTTPClient</code> <code>HTTPClient</code> <p>Configured HTTP Client.</p> required Source code in <code>canfar/context.py</code> <pre><code>class Context(HTTPClient):\n    \"\"\"CANFAR Context.\n\n    Args:\n        HTTPClient (canfar.client.HTTPClient): Configured HTTP Client.\n    \"\"\"\n\n    def resources(self) -&gt; dict[str, Any]:\n        \"\"\"Get available resources from the canfar server.\n\n        Returns:\n            A dictionary of available resources.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.context import Context\n            &gt;&gt;&gt; context = Context()\n            &gt;&gt;&gt; context.resources()\n            {'cores': {\n              'default': 1,\n              'defaultRequest': 1,\n              'defaultLimit': 16,\n              'defaultHeadless': 1,\n              'options': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n              },\n             'memoryGB': {\n              'default': 2,\n              'defaultRequest': 4,\n              'defaultLimit': 192,\n              'defaultHeadless': 4,\n              'options': [1,2,4...192]\n             },\n            'gpus': {\n             'options': [1,2, ... 28]\n             }\n            }\n        \"\"\"\n        response: Response = self.client.get(url=\"context\")\n        return dict(response.json())\n</code></pre>"},{"location":"client/context/#canfar.context.Context.resources","title":"<code>resources()</code>","text":"<p>Get available resources from the canfar server.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary of available resources.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.context import Context\n&gt;&gt;&gt; context = Context()\n&gt;&gt;&gt; context.resources()\n{'cores': {\n  'default': 1,\n  'defaultRequest': 1,\n  'defaultLimit': 16,\n  'defaultHeadless': 1,\n  'options': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n  },\n 'memoryGB': {\n  'default': 2,\n  'defaultRequest': 4,\n  'defaultLimit': 192,\n  'defaultHeadless': 4,\n  'options': [1,2,4...192]\n },\n'gpus': {\n 'options': [1,2, ... 28]\n }\n}\n</code></pre> Source code in <code>canfar/context.py</code> <pre><code>def resources(self) -&gt; dict[str, Any]:\n    \"\"\"Get available resources from the canfar server.\n\n    Returns:\n        A dictionary of available resources.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.context import Context\n        &gt;&gt;&gt; context = Context()\n        &gt;&gt;&gt; context.resources()\n        {'cores': {\n          'default': 1,\n          'defaultRequest': 1,\n          'defaultLimit': 16,\n          'defaultHeadless': 1,\n          'options': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n          },\n         'memoryGB': {\n          'default': 2,\n          'defaultRequest': 4,\n          'defaultLimit': 192,\n          'defaultHeadless': 4,\n          'options': [1,2,4...192]\n         },\n        'gpus': {\n         'options': [1,2, ... 28]\n         }\n        }\n    \"\"\"\n    response: Response = self.client.get(url=\"context\")\n    return dict(response.json())\n</code></pre>"},{"location":"client/examples/","title":"Python Client Examples","text":"<p>These examples use the asynchronous API for best performance and scalability.</p> <p>Assumption</p> Authenticated via CLI<pre><code>canfar auth login\n</code></pre>"},{"location":"client/examples/#create-sessions","title":"Create Sessions","text":""},{"location":"client/examples/#notebook","title":"Notebook","text":"Basic Notebook<code>sync context</code><code>async</code><code>async context</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nids = session.create(\n    name=\"my-notebook\",\n    image=\"images.canfar.net/skaha/base-notebook:latest\",\n    kind=\"notebook\",\n    cores=2,\n    ram=4,\n)\nprint(ids)  # [\"d1tsqexh\"]\nsession.connect(ids)\n</code></pre> <pre><code>from canfar.sessions import Session\n\nwith Session() as session:\n    ids = session.create(\n        name=\"my-notebook\",\n        image=\"images.canfar.net/skaha/base-notebook:latest\",\n        kind=\"notebook\",\n        cores=2,\n        ram=4,\n    )\n    print(ids)  # [\"d1tsqexh\"]\n    session.connect(ids)\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nsession = AsyncSession()\nids = await session.create(\n    name=\"my-notebook\",\n    image=\"images.canfar.net/skaha/base-notebook:latest\",\n    kind=\"notebook\",\n    cores=2,\n    ram=4,\n)\nprint(ids)  # [\"d1tsqexh\"]\nawait session.connect(ids)\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    ids = await session.create(\n        name=\"my-notebook\",\n        image=\"images.canfar.net/skaha/base-notebook:latest\",\n        kind=\"notebook\",\n        cores=2,\n        ram=4,\n    )\n    print(ids)  # [\"d1tsqexh\"]\n    await session.connect(ids)\n</code></pre>"},{"location":"client/examples/#headless","title":"Headless","text":"<ul> <li>Headless sessions are are containers that execute a command and exit when complete without user interaction.</li> <li>They are useful for batch processing and distributed computing.</li> </ul> Replicated Headless Sessions<code>sync context</code><code>async</code><code>async context</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nids = session.create(\n    name=\"my-headless\",\n    image=\"images.canfar.net/skaha/base-notebook:latest\",\n    kind=\"headless\",\n    cmd=\"echo\",\n    args=[\"Hello, World!\"],\n)\nprint(ids)  # [\"d1tsqexh\"]\n</code></pre> <pre><code>from canfar.sessions import Session\n\nwith Session() as session:\n    ids = session.create(\n        name=\"my-headless\",\n        image=\"images.canfar.net/skaha/base-notebook:latest\",\n        kind=\"headless\",\n        cmd=\"echo\",\n        args=[\"Hello, World!\"],\n    )\n    print(ids)  # [\"d1tsqexh\"]\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nsession = AsyncSession()\nids = await session.create(\n    name=\"my-headless\",\n    image=\"images.canfar.net/skaha/base-notebook:latest\",\n    kind=\"headless\",\n    cmd=\"echo\",\n    args=[\"Hello, World!\"],\n)\nprint(ids)  # [\"d1tsqexh\"]\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    ids = await session.create(\n        name=\"my-headless\",\n        image=\"images.canfar.net/skaha/base-notebook:latest\",\n        kind=\"headless\",\n        cmd=\"echo\",\n        args=[\"Hello, World!\"],\n    )\n    print(ids)  # [\"d1tsqexh\"]\n</code></pre> <p>Replica Environment Variables</p> <p>All containers receive the following environment variables: - <code>REPLICA_COUNT</code> \u2014 common total number of replicas - <code>REPLICA_ID</code> \u2014 1-based index of the replica (1..N)</p> <p>Use these to partition work deterministically. See Helpers API Reference for <code>chunk</code> and <code>stripe</code>.</p> <p>Private Container Registry Access</p> <p>Use a private Harbor image by providing registry credentials via configuration. <pre><code>import asyncio\nfrom canfar.sessions import AsyncSession\nfrom canfar.models.registry import ContainerRegistry\nfrom canfar.models.config import Configuration\n\nasync def main():\n    cfg = Configuration(registry=ContainerRegistry(username=\"username\", secret=\"CLI_SECRET\"))\n    session = AsyncSession(config=cfg)\n    ids = await session.create(\n        name=\"private-job\",\n        image=\"images.canfar.net/your/private-image:latest\",\n        kind=\"headless\",\n        cmd=\"python\",\n        args=[\"/app/run.py\"],\n    )\n    print(ids)\n\nasyncio.run(main())\n</code></pre></p>"},{"location":"client/examples/#discover-and-filter-sessions","title":"Discover and Filter Sessions","text":"Fetch All Sessions<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nall_sessions = session.fetch()\nprint(len(all_sessions))\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nwith AsyncSession() as session:\n    all_sessions = await session.fetch()\n    print(len(all_sessions))\n</code></pre> Fetch Running Notebooks<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nrunning = session.fetch(kind=\"notebook\", status=\"Running\")\nprint(running)\nsession.connect(running)\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    running = await session.fetch(kind=\"notebook\", status=\"Running\")\n    print(running)\n    await session.connect(running)\n</code></pre> Fetch Completed Headless Sessions<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\ncompleted = session.fetch(kind=\"headless\", status=\"Succeeded\")\nprint(completed)\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    completed = await session.fetch(kind=\"headless\", status=\"Succeeded\")\n    print(completed)\n</code></pre> <p>Kinds &amp; Status</p> <p>You can use any combination of the following kinds and status to filter sessions:</p> <ul> <li>Kinds: <code>desktop</code>, <code>notebook</code>, <code>carta</code>, <code>headless</code>, <code>firefly</code>, <code>desktop-app</code>, <code>contributed</code></li> <li>Statuses: <code>Pending</code>, <code>Running</code>, <code>Terminating</code>, <code>Succeeded</code>, <code>Error</code>, <code>Failed</code></li> </ul>"},{"location":"client/examples/#inspect-sessions","title":"Inspect Sessions","text":"<p>Detailed information about the session, including resource usage, user IDs, and more.</p> Detailed Session Information<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\ninfo = session.info(ids)\nprint(info)\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    info = await session.info(ids)\n    print(info)\n</code></pre>"},{"location":"client/examples/#events","title":"Events","text":"<p>Events describe the steps taken by the Science Platform to launch your session</p> Session Events<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nevents = session.events(ids, verbose=True)\nprint(events)\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    events = await session.events(ids, verbose=True)\n    print(events)\n</code></pre>"},{"location":"client/examples/#logs","title":"Logs","text":"<p>Logs contain the output from your session's containers. </p> <p>Log Retention</p> <p>Logs are retained until your session is deleted. A completed session, i.e., <code>Succeeded</code>, <code>Failed</code>, or <code>Error</code> is kept for 24 hours before being deleted.</p> Session Logs<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nlogs = session.logs(ids, verbose=True)\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    logs = await session.logs(ids, verbose=True)\n</code></pre>"},{"location":"client/examples/#cleanup-sessions","title":"Cleanup Sessions","text":"<p>Permanent Action</p> <p>Deleted sessions cannot be recovered.</p> Destroy Session(s)<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nresult = session.destroy(ids)\nprint(result)  # {\"id\": True, ...}\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    result = await session.destroy(ids)\n    print(result)  # {\"id\": True, ...}\n</code></pre> <p></p> Bulk Destroy<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nresult = session.destroy_with(prefix=\"test-\", kind=\"headless\", status=\"Succeeded\")\nprint(result)  # {\"id\": True, ...}\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    result = await session.destroy_with(prefix=\"test-\", kind=\"headless\", status=\"Succeeded\")\n    print(result)  # {\"id\": True, ...}\n</code></pre>"},{"location":"client/get-started/","title":"Installation &amp; Setup","text":"<p>This guide covers everything you need to install and start using CANFAR Science Platform servers worldwide.</p> <p>New to Canfar?</p> <p>If you want to jump right in with a hands-on tutorial, check out our 5-Minute Quick Start guide first!</p>"},{"location":"client/get-started/#prerequisites","title":"Prerequisites","text":"<p>Before you can use canfar, you need:</p> <ul> <li>Python 3.10+ installed on your system</li> <li>A Science Platform account - For CANFAR, request an account with CADC</li> </ul>"},{"location":"client/get-started/#installation","title":"Installation","text":"<p>Install canfar using <code>pip</code>:</p> <pre><code>pip install canfar --upgrade\n</code></pre> <p>Virtual Environments</p> <p>We recommend using a virtual environment to avoid conflicts with other packages: <pre><code>python -m venv canfar-env\nsource canfar-env/bin/activate  # On Windows: canfar-env\\Scripts\\activate\npip install canfar\n</code></pre></p>"},{"location":"client/get-started/#authentication-setup","title":"Authentication Setup","text":"<p>Canfar uses an authentication context system to manage connections to multiple Science Platform servers. The easiest way to get started is with the CLI login command.</p>"},{"location":"client/get-started/#quick-authentication","title":"Quick Authentication","text":"<p>To authenticate with a Science Platform server:</p> <pre><code>canfar auth login\n</code></pre> <p>This command will:</p> <ol> <li>Discover available servers worldwide</li> <li>Guide you through server selection</li> <li>Handle the authentication process (X.509 or OIDC)</li> <li>Save your credentials for future use</li> </ol> <p>Example Login Flow</p> <pre><code>$ canfar auth login\nStarting Science Platform Login\nDiscovery completed in 2.1s (5/18 active)\n\nSelect a Canfar Server:\n\u00bb \ud83d\udfe2 CANFAR  CADC\n  \ud83d\udfe2 Canada  SRCnet\n  \ud83d\udfe2 UK-CAM  SRCnet\n\nX509 Certificate Authentication\nUsername: your-username\nPassword: ***********\n\u2713 Login completed successfully!\n</code></pre>"},{"location":"client/get-started/#using-canfar-programmatically","title":"Using Canfar Programmatically","text":"<p>Once authenticated via CLI, you can use Canfar in your Python code:</p> <pre><code>from canfar.session import Session\nfrom canfar.images import Images\n\n# Uses your active authentication context\nsession = Session()\nimages = Images()\n\n# List available images\ncontainer_images = images.fetch()\nprint(f\"Found {len(container_images)} container images\")\n\n# Create a notebook session\nsession_info = session.create(\n    kind=\"notebook\",\n    image=\"images.canfar.net/skaha/base-notebook:latest\",\n    name=\"my-analysis\",\n    cores=2,\n    ram=4\n)\nprint(f\"Created session: {session_info.id}\")\n</code></pre>"},{"location":"client/get-started/#private-container-images","title":"Private Container Images","text":"<p>To access private container images from registries like CANFAR Harbor, provide registry credentials:</p> <pre><code>from canfar.models import ContainerRegistry\nfrom canfar.session import Session\n\n# Configure registry access\nregistry = ContainerRegistry(\n    username=\"your-username\",\n    password=\"**************\"\n)\n\n# Use with session\nsession = Session(registry=registry)\n\n# Now you can use private images\nsession_info = session.create(\n    kind=\"notebook\",\n    image=\"images.canfar.net/private/my-image:latest\",\n    cores=1,\n    ram=2\n)\n</code></pre> <p>Registry Credentials</p> <p>The registry credentials are base64 encoded and passed to the server via the <code>X-Skaha-Registry-Auth</code> header.</p>"},{"location":"client/get-started/#next-steps","title":"Next Steps","text":"<p>Now that you have canfar installed and configured:</p> <ul> <li> Try our 5-Minute Quick Start for a hands-on introduction to creating and managing sessions.</li> <li> Learn about Authentication Contexts for managing multiple servers and advanced authentication scenarios.</li> <li> Explore Basic Examples and Advanced Examples for common use cases.</li> <li> Check out the Python API Reference for detailed documentation of all available methods.</li> <li> Refer to the FAQ for answers to common questions.</li> </ul> <p>Getting Help</p> <ul> <li>\ud83d\udcac Community Discussions</li> <li>\ud83d\udc1b Report Issues</li> </ul>"},{"location":"client/helpers/","title":"Helpers API","text":"<p>Overview</p> <p>The Helpers API provides utility functions to partition work across replicas of a CANFAR session. Containers receive <code>REPLICA_ID</code> and <code>REPLICA_COUNT</code> environment variables, and these helpers make using them simple and correct.</p>"},{"location":"client/helpers/#practical-examples","title":"Practical Examples","text":""},{"location":"client/helpers/#stripe-take-every-nth-item-with-an-offset","title":"Stripe: take every Nth item with an offset","text":"<pre><code>from canfar.helpers import distributed\n\n# Assume REPLICA_ID=2 and REPLICA_COUNT=4\n# Replica 2 (1-based) will see indices 1, 5, 9, ...\nitems = list(range(12))\nshard = list(distributed.stripe(items, replica=2, total=4))\nprint(shard)  # [1, 5, 9]\n</code></pre>"},{"location":"client/helpers/#chunk-contiguous-chunks-of-roughly-equal-size","title":"Chunk: contiguous chunks of roughly equal size","text":"<pre><code>from canfar.helpers import distributed\n\n# Assume 10 items, 4 replicas\nitems = list(range(10))\n# Replica 1 gets [0,1], 2-&gt;[2,3], 3-&gt;[4,5], 4-&gt;[6,7,8,9] (last takes remainder)\nprint(list(distributed.chunk(items, replica=1, total=4)))\nprint(list(distributed.chunk(items, replica=4, total=4)))\n</code></pre> <p>Sparse distribution</p> <p>When items &lt; replicas, <code>chunk</code> assigns exactly one item to each of the first <code>len(items)</code> replicas, and later replicas get nothing. This avoids duplication.</p>"},{"location":"client/helpers/#using-container-provided-environment-variables","title":"Using container-provided environment variables","text":"<pre><code># Inside CANFAR container replicas, you can omit replica/total and read from env\nfrom canfar.helpers import distributed\nwork = list(range(1000))\nfor item in distributed.chunk(work):\n    process(item)\n</code></pre>"},{"location":"client/helpers/#validation-and-errors","title":"Validation and errors","text":"<ul> <li><code>replica</code> must be &gt;= 1 and &lt;= <code>total</code></li> <li><code>total</code> must be &gt; 0</li> </ul>"},{"location":"client/helpers/#api-reference","title":"API Reference","text":""},{"location":"client/helpers/#canfar.helpers.distributed","title":"<code>canfar.helpers.distributed</code>","text":"<p>Helper functions for distributed computing.</p>"},{"location":"client/helpers/#canfar.helpers.distributed.stripe","title":"<code>stripe(iterable: Iterable[T], replica: int = int(os.environ.get('REPLICA_ID', '1')), total: int = int(os.environ.get('REPLICA_COUNT', '1'))) -&gt; Iterator[T]</code>","text":"<p>Returns every <code>total</code>-th item from the iterable with a <code>replica</code>-th offset.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable[T]</code> <p>The iterable to partition.</p> required <code>replica</code> <code>int</code> <p>The replica number. Defaults to int(os.environ.get(\"REPLICA_ID\", 1)).</p> <code>int(get('REPLICA_ID', '1'))</code> <code>total</code> <code>int</code> <p>The total number of replicas. Defaults to int(os.environ.get(\"REPLICA_COUNT\", 1)).</p> <code>int(get('REPLICA_COUNT', '1'))</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.helpers import distributed\n&gt;&gt;&gt; dataset = range(100)\n&gt;&gt;&gt; for data in distributed.partition(dataset, 1, 10):\n        print(data)\n0, 10, 20, 30, 40, 50, 60, 70, 80, 90\n</code></pre> <p>Yields:</p> Type Description <code>T</code> <p>Iterator[T]: The <code>replica</code>-th partition of the iterable.</p>"},{"location":"client/helpers/#canfar.helpers.distributed.chunk","title":"<code>chunk(iterable: Iterable[T], replica: int = int(os.environ.get('REPLICA_ID', '1')), total: int = int(os.environ.get('REPLICA_COUNT', '1'))) -&gt; Iterator[T]</code>","text":"<p>Returns the <code>replica</code>-th chunk of the iterable split into <code>total</code> chunks.</p> <p>This function distributes items from an iterable across multiple replicas canfar provided container environment variables.</p> <p>Distribution Behavior:</p> <ul> <li>Standard Distribution (items &gt;= replicas): Items are divided into roughly   equal chunks, with the last replica receiving any remainder items.</li> <li>Sparse Distribution (items &lt; replicas): Each of the first N replicas gets   exactly one item (where N = number of items), remaining replicas get empty   results.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable[T]</code> <p>The iterable to distribute across replicas.</p> required <code>replica</code> <code>int</code> <p>The replica number using 1-based indexing. Must be &gt;= 1 and &lt;= total. Defaults to REPLICA_ID environment variable.</p> <code>int(get('REPLICA_ID', '1'))</code> <code>total</code> <code>int</code> <p>The total number of replicas. Must be &gt; 0. Defaults to REPLICA_COUNT environment variable.</p> <code>int(get('REPLICA_COUNT', '1'))</code> <p>Returns:</p> Type Description <code>Iterator[T]</code> <p>Iterator[T]: An iterator yielding items assigned to this replica.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If replica &lt; 1 (1-based indexing expected).</p> <code>ValueError</code> <p>If replica &gt; total (replica cannot exceed total replicas).</p> <code>ValueError</code> <p>If total &lt;= 0 (must have at least one replica).</p> Note <p>This function is designed for use in canfar containerized environments where REPLICA_ID and REPLICA_COUNT environment variables are automatically set. The 1-based indexing matches the container environment expectations.</p> <p>For optimal performance with large datasets, consider using this function with iterators rather than converting large datasets to lists beforehand.</p> <p>When items &lt; replicas, the sparse distribution ensures no replica receives an unfair share - each item goes to exactly one replica, and excess replicas receive empty results rather than duplicating data.</p>"},{"location":"client/home/","title":"CANFAR Clients","text":"<p>A powerful Python API and CLI for the CANFAR Science Platform.</p>  Client Download <p> API</p> <code>sync</code><code>async</code><code>async context</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nids = session.create(\n    name=\"test\",\n    image=\"images.canfar.net/skaha/base-notebook:latest\",\n    kind=\"headless\",\n    cmd=\"env\",\n    env={\"KEY\": \"VALUE\"},\n    replicas=3,\n)\nprint(ids)\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nsession = AsyncSession()\nids = await session.create(\n    name=\"test\",\n    image=\"images.canfar.net/skaha/base-notebook:latest\",\n    kind=\"headless\",\n    cmd=\"env\",\n    env={\"KEY\": \"VALUE\"},\n    replicas=3,\n)\nprint(ids)\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    ids = await session.create(\n        name=\"test\",\n        image=\"images.canfar.net/skaha/base-notebook:latest\",\n        kind=\"headless\",\n        cmd=\"env\",\n        env={\"KEY\": \"VALUE\"},\n        replicas=3,\n    )\n    print(ids)\n</code></pre> <p> CLI</p> Create a Session<pre><code>canfar launch headless --env KEY=VALUE --replicas 3 images.canfar.net/skaha/base-notebook:latest \n</code></pre> <p>Installation</p> Install from PyPI<pre><code>pip install canfar\n</code></pre> Add as Dependency<pre><code>uv add canfar\n</code></pre> <p> Python Client  Explore the CLI  Codebase</p>"},{"location":"client/images/","title":"Images API","text":"<p>Overview</p> <p>The Image API allows you to get information about the publicly available images on the CANFAR Science Platform through the CANFAR Harbor Registry. It can be used to get information about all images, or filter by a specific image kind.</p>"},{"location":"client/images/#getting-image-information","title":"Getting Image Information","text":"Get image information<pre><code>from canfar.images import Images\n\nimages = Images()\nimages.fetch()\n[\n    \"images.canfar.net/canfar/base-3.12:v0.4.1\",\n    \"images.canfar.net/canucs/test:1.2.5\",\n    \"images.canfar.net/canucs/canucs:1.2.9\",\n    ...,\n]\n</code></pre> <p>But most of the time, you are only interested in images of a particular type. For example, if you want to get all the images that are available for <code>headless</code> sessions, you can do the following:</p> Get headless image information<pre><code>images.fetch(kind=\"headless\")\n</code></pre> <pre><code>[\n    \"images.canfar.net/chimefrb/testing:keep\",\n    \"images.canfar.net/lsst/lsst_v19_0_0:0.1\",\n    \"images.canfar.net/skaha/lensfit:22.11\",\n    \"images.canfar.net/skaha/lensfit:22.10\",\n    \"images.canfar.net/skaha/lensingsim:22.07\",\n    \"images.canfar.net/skaha/phosim:5.6.11\",\n    \"images.canfar.net/skaha/terminal:1.1.2\",\n    \"images.canfar.net/skaha/terminal:1.1.1\",\n    \"images.canfar.net/uvickbos/pycharm:0.1\",\n    \"images.canfar.net/uvickbos/swarp:0.1\",\n    \"images.canfar.net/uvickbos/isis:2.2\",\n    \"images.canfar.net/uvickbos/find_moving:0.1\",\n]\n</code></pre>"},{"location":"client/images/#api-reference","title":"API Reference","text":"<p>               Bases: <code>HTTPClient</code></p> <p>CANFAR Image Management.</p> <p>Parameters:</p> Name Type Description Default <code>HTTPClient</code> <code>HTTPClient</code> <p>Configured HTTP Client.</p> required <p>Returns:</p> Name Type Description <code>Images</code> <p>CANFAR Image Management Object.</p> Source code in <code>canfar/images.py</code> <pre><code>class Images(HTTPClient):\n    \"\"\"CANFAR Image Management.\n\n    Args:\n        HTTPClient (canfar.client.HTTPClient): Configured HTTP Client.\n\n    Returns:\n        Images: CANFAR Image Management Object.\n    \"\"\"\n\n    def fetch(self, kind: str | None = None) -&gt; list[str]:\n        \"\"\"Get images from CANFAR Server.\n\n        Args:\n            kind (str | None, optional): Type of image. Defaults to None.\n\n        Returns:\n            list[str]: A list of images on the server.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.images import Images\n            &gt;&gt;&gt; images = Images()\n            &gt;&gt;&gt; images.fetch(kind=\"headless\")\n            ['images.canfar.net/skaha/terminal:1.1.1']\n        \"\"\"\n        data: dict[str, str] = {}\n        # If kind is not None, add it to the data dictionary\n        if kind:\n            data[\"type\"] = kind\n        response: Response = self.client.get(\"image\", params=data)\n        payload: list[dict[str, str]] = response.json()\n        return [str(image[\"id\"]) for image in payload]\n</code></pre>"},{"location":"client/images/#canfar.images.Images.fetch","title":"<code>fetch(kind=None)</code>","text":"<p>Get images from CANFAR Server.</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str | None</code> <p>Type of image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of images on the server.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.images import Images\n&gt;&gt;&gt; images = Images()\n&gt;&gt;&gt; images.fetch(kind=\"headless\")\n['images.canfar.net/skaha/terminal:1.1.1']\n</code></pre> Source code in <code>canfar/images.py</code> <pre><code>def fetch(self, kind: str | None = None) -&gt; list[str]:\n    \"\"\"Get images from CANFAR Server.\n\n    Args:\n        kind (str | None, optional): Type of image. Defaults to None.\n\n    Returns:\n        list[str]: A list of images on the server.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.images import Images\n        &gt;&gt;&gt; images = Images()\n        &gt;&gt;&gt; images.fetch(kind=\"headless\")\n        ['images.canfar.net/skaha/terminal:1.1.1']\n    \"\"\"\n    data: dict[str, str] = {}\n    # If kind is not None, add it to the data dictionary\n    if kind:\n        data[\"type\"] = kind\n    response: Response = self.client.get(\"image\", params=data)\n    payload: list[dict[str, str]] = response.json()\n    return [str(image[\"id\"]) for image in payload]\n</code></pre>"},{"location":"client/migration/","title":"skaha \u2192 canfar","text":"<p>In summer 2025, this project migrated from shinybrar/skaha to opencadc/canfar to be officially supported by the Canadian Astronomy Data Centre (CADC). As part of this move, the Python package was renamed from <code>skaha</code> to <code>canfar</code> to better reflect a unified naming scheme across the CANFAR Science Platform.</p> <p>This guide helps you migrate from the <code>skaha</code> Python package to <code>canfar</code>.</p>"},{"location":"client/migration/#summary-of-changes","title":"Summary of changes","text":"<ul> <li>Package name: <code>skaha</code> \u2192 <code>canfar</code>.</li> <li>Public API Changes</li> <li>Configuration path: <code>~/.skaha/config.yaml</code> \u2192 <code>~/.canfar/config.yaml</code>.</li> <li>Logger name and location: logger <code>canfar</code>; logs under <code>~/.canfar/client.log</code>.</li> <li>Environment variables: prefix change <code>SKAHA_\u2026</code> \u2192 <code>CANFAR_\u2026</code>.</li> <li>CLI entry point: <code>canfar</code> (single entry point).</li> <li>User-Agent header: <code>python-canfar/{version}</code>.</li> <li>Protocol contracts: server URLs and custom headers remain unchanged (see notes below).</li> </ul>"},{"location":"client/migration/#code-examples","title":"Code Examples","text":"<ul> <li> <p>Python client session</p> Before<pre><code>from skaha.session import AsyncSession, Session\n</code></pre> After<pre><code>from canfar.sessions import AsyncSession, Session\n</code></pre> </li> <li> <p>Client composition</p> Before<pre><code>from skaha.client import SkahaClient\n\nclient = SkahaClient(...)\n</code></pre> After<pre><code>from canfar.client import HTTPClient\n\nclient = HTTPClient(...)\n</code></pre> </li> </ul>"},{"location":"client/migration/#environment-variables","title":"Environment variables","text":"<p>Before<pre><code>`SKAHA_TIMEOUT`, `SKAHA_CONCURRENCY`, `SKAHA_TOKEN`, `SKAHA_URL`, `SKAHA_LOGLEVEL\n</code></pre> After<pre><code>`CANFAR_TIMEOUT`, `CANFAR_CONCURRENCY`, `CANFAR_TOKEN`, `CANFAR_URL`, `CANFAR_LOGLEVEL`\n</code></pre></p>"},{"location":"client/migration/#configuration","title":"Configuration","text":"<ul> <li>The default config file moves from <code>~/.skaha/config.yaml</code> to <code>~/.canfar/config.yaml</code>.</li> <li>The structure of the YAML file remains the same.</li> </ul>"},{"location":"client/migration/#documentation-and-links","title":"Documentation and links","text":"<ul> <li>Repo: <code>https://github.com/opencadc/canfar</code></li> <li>Docs: <code>https://opencadc.github.io/canfar/</code></li> <li>Changelog: <code>https://opencadc.github.io/canfar/changelog/</code></li> </ul>"},{"location":"client/migration/#notes-on-protocol-stability","title":"Notes on protocol stability","text":"<ul> <li>Server base path segments under <code>/skaha</code> are server-side contracts and remain unchanged (for example, <code>https://ws-uv.canfar.net/skaha</code>).</li> <li>Historical header names remain unchanged (for example, <code>X-Skaha-Authentication-Type</code>, <code>X-Skaha-Registry-Auth</code>).</li> </ul>"},{"location":"client/overview/","title":"Overview","text":"<p>Overview API</p> <p>The Overview API provides information about the availability of the CANFAR Science Platform.</p> <p>               Bases: <code>HTTPClient</code></p> <p>Overview of the CANFAR Server.</p> <p>Parameters:</p> Name Type Description Default <code>HTTPClient</code> <code>HTTPClient</code> <p>Configured HTTP Client.</p> required Source code in <code>canfar/overview.py</code> <pre><code>class Overview(HTTPClient):\n    \"\"\"Overview of the CANFAR Server.\n\n    Args:\n        HTTPClient (canfar.client.HTTPClient): Configured HTTP Client.\n    \"\"\"\n\n    @model_validator(mode=\"after\")\n    def _update_base_url(self) -&gt; Self:\n        \"\"\"Update base URL for the server.\n\n        Returns:\n            Self: The current object.\n        \"\"\"\n        url: str = str(self.client.base_url)\n        base: str = url.split(\"/v\", maxsplit=1)[0]\n        # The overview endpoint is not versioned, so need to remove it\n        self.client.base_url = URL(base)\n        self.asynclient.base_url = URL(base)\n        return self\n\n    def availability(self) -&gt; bool:\n        \"\"\"Check if the server backend is available.\n\n        Returns:\n            bool: True if the server is available, False otherwise.\n        \"\"\"\n        response: Response = self.client.get(\"availability\")\n        data: str = response.text\n        if not data:\n            log.error(\"No data returned from availability endpoint.\")\n            return False\n        root = ElementTree.fromstring(data)\n        available = root.find(\n            \".//{http://www.ivoa.net/xml/VOSIAvailability/v1.0}available\",\n        )\n        availaibility: str | None = available.text if available is not None else None\n\n        note = root.find(\n            \".//{http://www.ivoa.net/xml/VOSIAvailability/v1.0}note\",\n        )\n        notify: str | None = note.text if note is not None else None\n        if availaibility is None:\n            log.error(\"No availability information found in the response.\")\n            return False\n        log.info(notify if notify else \"No additional information provided.\")\n        return availaibility == \"true\"\n</code></pre>"},{"location":"client/overview/#canfar.overview.Overview.availability","title":"<code>availability()</code>","text":"<p>Check if the server backend is available.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the server is available, False otherwise.</p> Source code in <code>canfar/overview.py</code> <pre><code>def availability(self) -&gt; bool:\n    \"\"\"Check if the server backend is available.\n\n    Returns:\n        bool: True if the server is available, False otherwise.\n    \"\"\"\n    response: Response = self.client.get(\"availability\")\n    data: str = response.text\n    if not data:\n        log.error(\"No data returned from availability endpoint.\")\n        return False\n    root = ElementTree.fromstring(data)\n    available = root.find(\n        \".//{http://www.ivoa.net/xml/VOSIAvailability/v1.0}available\",\n    )\n    availaibility: str | None = available.text if available is not None else None\n\n    note = root.find(\n        \".//{http://www.ivoa.net/xml/VOSIAvailability/v1.0}note\",\n    )\n    notify: str | None = note.text if note is not None else None\n    if availaibility is None:\n        log.error(\"No availability information found in the response.\")\n        return False\n    log.info(notify if notify else \"No additional information provided.\")\n    return availaibility == \"true\"\n</code></pre>"},{"location":"client/quick-start/","title":"5-Minute Quick Start (Python Client)","text":"<p>Goal</p> <p>By the end of this guide, you'll authenticate, launch a compute Session on CANFAR programmatically, inspect it, read logs/events, and clean it up \u2014 all from Python.</p> <p>Prerequisites</p> <ul> <li>CADC Account \u2014 Sign up</li> <li>You have logged in at least once to the CANFAR Science Platform and the Harbor Container Registry</li> <li>Python 3.10+</li> </ul>"},{"location":"client/quick-start/#installation","title":"Installation","text":"&gt; pip install canfar --upgradeInstalled"},{"location":"client/quick-start/#authentication","title":"Authentication","text":"<p>The Python client automatically uses your active authentication context created by the CLI.</p> Login to CANFAR Science Platform<pre><code>canfar auth login\n</code></pre> <p>Login Pathways</p> <ul> <li>If you already have a valid CADC X509 certificate at <code>~/.ssl/cadcproxy.pem</code>, the CLI will reuse it automatically.</li> <li>If you're an SRCnet user, you'll be guided through an OIDC device flow in your browser.</li> </ul> Force Re-Login (optional)<pre><code>canfar auth login --force\n</code></pre> <p>What just happened?</p> <ul> <li>The CLI discovered available CANFAR/SRCnet servers</li> <li>You authenticated and obtained a certificate/token</li> <li>The active context was saved for the Python client to use</li> </ul>"},{"location":"client/quick-start/#your-first-notebook-session","title":"Your First Notebook Session","text":"<p>Launch a Jupyter notebook session programmatically.</p> Notebook Session<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nsession_ids = session.create(\n    name=\"my-first-notebook\",\n    image=\"images.canfar.net/skaha/astroml-notebook:latest\",\n    kind=\"notebook\",\n    cores=2,\n    ram=4,\n)\nprint(session_ids)  # e.g., [\"d1tsqexh\"]\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nsession = AsyncSession()\nids = await session.create(\n    name=\"my-first-notebook\",\n    image=\"images.canfar.net/skaha/astroml-notebook:latest\",\n    kind=\"notebook\",\n    cores=2,\n    ram=4,\n)\nprint(ids)  # e.g., [\"d1tsqexh\"]\n</code></pre> <p>What just happened?</p> <ul> <li>We connected to CANFAR using your active auth context</li> <li>A notebook container was requested with 2 CPU cores and 4 GB RAM</li> <li>The API returned the newly created session ID(s)</li> </ul>"},{"location":"client/quick-start/#get-connection-url","title":"Get Connection URL","text":"<p>Fetch details and extract the connect URL to open your notebook.</p> Connect to Session<code>async</code> <pre><code>session.connect(ids)\n</code></pre> <pre><code>await session.connect(ids)\n</code></pre>"},{"location":"client/quick-start/#peek-under-the-hood","title":"Peek Under the Hood","text":"<p>When a session is created, it goes through a series of steps to be fully deployed. You can inspect the events to understand the progress, or capture them for monitoring.</p> Deployment Events<code>async</code> <pre><code>session.events(ids, verbose=True)\n</code></pre> <pre><code>await session.events(ids, verbose=True)\n</code></pre> <p>At any point, you can also inspect the logs from the session. This is especially useful when launching long-running batch jobs.</p> Session Logs<code>async</code> <pre><code>session.logs(ids, verbose=True)\n</code></pre> <pre><code>await session.logs(ids, verbose=True)\n</code></pre>"},{"location":"client/quick-start/#clean-up","title":"Clean Up","text":"<p>When you're done, delete your session(s) to free resources for other users. </p> Destroy Session(s)<code>async</code> <pre><code>session.destroy(ids)\n</code></pre> <pre><code>await session.destroy(ids)\n</code></pre>"},{"location":"client/quick-start/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>Session won't start?</p> <p>Check available resources<pre><code>session.stats()\n</code></pre> Check events/logs<pre><code>session.events(ids, verbose=True)\nsession.logs(ids, verbose=True)\n</code></pre> Try smaller resources or different image<pre><code>session.create(..., cores=1, ram=2, image=\"images.canfar.net/skaha/base-notebook:latest\")\n</code></pre></p> </li> <li> <p>Authentication issues?</p> Force re-authentication<pre><code>canfar auth login --force --debug\n</code></pre> </li> </ul>"},{"location":"client/session/","title":"Session API","text":"<p>Overview</p> <p>The <code>Session</code> API is the core of canfar, enabling you to create, manage, and destroy sessions on the CANFAR Science Platform.</p> <p>               Bases: <code>HTTPClient</code></p> <p>Session Management Client.</p> <p>This class provides methods to manage sessions, including fetching session details, creating new sessions, retrieving logs, and destroying existing sessions.</p> <p>Parameters:</p> Name Type Description Default <code>HTTPClient</code> <code>HTTPClient</code> <p>Base HTTP client for making API requests.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import Session\n&gt;&gt;&gt; session = Session(\n        timeout=120,\n        concurrency=100, # No effect on sync client\n        loglevel=40,\n    )\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>class Session(HTTPClient):\n    \"\"\"Session Management Client.\n\n    This class provides methods to manage sessions, including fetching\n    session details, creating new sessions, retrieving logs, and\n    destroying existing sessions.\n\n    Args:\n        HTTPClient (canfar.client.HTTPClient): Base HTTP client for making API requests.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import Session\n        &gt;&gt;&gt; session = Session(\n                timeout=120,\n                concurrency=100, # No effect on sync client\n                loglevel=40,\n            )\n    \"\"\"\n\n    def fetch(\n        self,\n        kind: Kind | None = None,\n        status: Status | None = None,\n        view: View | None = None,\n    ) -&gt; list[dict[str, str]]:\n        \"\"\"Fetch open sessions for the user.\n\n        Args:\n            kind (Kind | None, optional): Session kind. Defaults to None.\n            status (Status | None, optional): Session status. Defaults to None.\n            view (View | None, optional): View leve. Defaults to None.\n\n        Returns:\n            list[dict[str, str]]: Session[s] information.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import Session\n            &gt;&gt;&gt; session = Session()\n            &gt;&gt;&gt; session.fetch(kind=\"notebook\")\n            [{'id': 'ikvp1jtp',\n              'userid': 'username',\n              'image': 'image-server/image/label:latest',\n              'type': 'notebook',\n              'status': 'Running',\n              'name': 'example-notebook',\n              'startTime': '2222-12-14T02:24:06Z',\n              'connectURL': 'https://something.example.com/ikvp1jtp',\n              'requestedRAM': '16G',\n              'requestedCPUCores': '2',\n              'requestedGPUCores': '&lt;none&gt;',\n              'coresInUse': '0m',\n              'ramInUse': '101Mi'}]\n        \"\"\"\n        parameters: dict[str, Any] = build.fetch_parameters(kind, status, view)\n        response: Response = self.client.get(url=\"session\", params=parameters)\n        data: list[dict[str, str]] = response.json()\n        return data\n\n    def stats(self) -&gt; dict[str, Any]:\n        \"\"\"Get statistics for the entire platform.\n\n        Returns:\n            Dict[str, Any]: Cluster statistics.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import Session\n            &gt;&gt;&gt; session = Session()\n            &gt;&gt;&gt; session.stats()\n            {'instances': {\n             'session': 88, 'desktopApp': 30, 'headless': 0, 'total': 118},\n             'cores': {'requestedCPUCores': 377,\n             'coresAvailable': 960,\n             'maxCores': {'cores': 32, 'withRam': '147Gi'}},\n             'ram': {'maxRAM': {'ram': '226Gi', 'withCores': 32}}}\n        \"\"\"\n        parameters = {\"view\": \"stats\"}\n        response: Response = self.client.get(\"session\", params=parameters)\n        data: dict[str, Any] = response.json()\n        return data\n\n    def info(self, ids: list[str] | str) -&gt; list[dict[str, Any]]:\n        \"\"\"Get information about session[s].\n\n        Args:\n            ids (Union[List[str], str]): Session ID[s].\n\n        Returns:\n            Dict[str, Any]: Session information.\n\n        Examples:\n            &gt;&gt;&gt; session.info(session_id=\"hjko98yghj\")\n            &gt;&gt;&gt; session.info(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        # Convert id to list if it is a string\n        if isinstance(ids, str):\n            ids = [ids]\n        parameters: dict[str, str] = {\"view\": \"event\"}\n        results: list[dict[str, Any]] = []\n        for value in ids:\n            try:\n                response: Response = self.client.get(\n                    url=f\"session/{value}\",\n                    params=parameters,\n                )\n                results.append(response.json())\n            except HTTPError:\n                err = f\"failed to fetch session info for {value}\"\n                log.exception(err)\n        return results\n\n    def logs(\n        self,\n        ids: list[str] | str,\n        verbose: bool = False,\n    ) -&gt; dict[str, str] | None:\n        \"\"\"Get logs from a session[s].\n\n        Args:\n            ids (Union[List[str], str]): Session ID[s].\n            verbose (bool, optional): Print logs to stdout. Defaults to False.\n\n        Returns:\n            Dict[str, str]: Logs in text/plain format.\n\n        Examples:\n            &gt;&gt;&gt; session.logs(id=\"hjko98yghj\")\n            &gt;&gt;&gt; session.logs(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        if isinstance(ids, str):\n            ids = [ids]\n        parameters: dict[str, str] = {\"view\": \"logs\"}\n        results: dict[str, str] = {}\n\n        for value in ids:\n            try:\n                response: Response = self.client.get(\n                    url=f\"session/{value}\",\n                    params=parameters,\n                )\n                results[value] = response.text\n            except HTTPError:\n                err = f\"failed to fetch logs for session {value}\"\n                log.exception(err)\n\n        if verbose:\n            for key, value in results.items():\n                log.info(\"Session ID: %s\\n\", key)\n                log.info(value)\n            return None\n\n        return results\n\n    def create(\n        self,\n        name: str,\n        image: str,\n        cores: int = 2,\n        ram: int = 4,\n        kind: Kind = \"headless\",\n        gpu: int | None = None,\n        cmd: str | None = None,\n        args: str | None = None,\n        env: dict[str, Any] | None = None,\n        replicas: int = 1,\n    ) -&gt; list[str]:\n        \"\"\"Launch a canfar session.\n\n        Args:\n            name (str): A unique name for the session.\n            image (str): Container image to use for the session.\n            cores (int, optional): Number of cores. Defaults to 2.\n            ram (int, optional): Amount of RAM (GB). Defaults to 4.\n            kind (str, optional): Type of canfar session. Defaults to \"headless\".\n            gpu (Optional[int], optional): Number of GPUs. Defaults to None.\n            cmd (Optional[str], optional): Command to run. Defaults to None.\n            args (Optional[str], optional): Arguments to the command. Defaults to None.\n            env (Optional[Dict[str, Any]], optional): Environment variables to inject.\n                Defaults to None.\n            replicas (int, optional): Number of sessions to launch. Defaults to 1.\n\n        Notes:\n            The name of the session suffixed with the replica number. eg. test-1, test-2\n            Each container will have the following environment variables injected:\n                * REPLICA_ID - The replica number\n                * REPLICA_COUNT - The total number of replicas\n\n        Returns:\n            List[str]: A list of session IDs for the launched sessions.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import Session\n            &gt;&gt;&gt; session = Session()\n            &gt;&gt;&gt; session.create(\n                    name=\"test\",\n                    image='images.canfar.net/skaha/terminal:1.1.1',\n                    cores=2,\n                    ram=8,\n                    gpu=1,\n                    kind=\"headless\",\n                    cmd=\"env\",\n                    env={\"TEST\": \"test\"},\n                    replicas=2,\n                )\n            &gt;&gt;&gt; [\"hjko98yghj\", \"ikvp1jtp\"]\n        \"\"\"\n        payloads = build.create_parameters(\n            name,\n            image,\n            cores,\n            ram,\n            kind,\n            gpu,\n            cmd,\n            args,\n            env,\n            replicas,\n        )\n        results: list[str] = []\n        log.debug(\"Creating %d %s session[s].\", replicas, kind)\n        for payload in payloads:\n            try:\n                response: Response = self.client.post(url=\"session\", params=payload)\n                results.append(response.text.rstrip(\"\\r\\n\"))\n            except HTTPError:\n                err = f\"Failed to create session with payload: {payload}\"\n                log.exception(err)\n        return results\n\n    def events(\n        self,\n        ids: str | list[str],\n        verbose: bool = False,\n    ) -&gt; list[dict[str, str]] | None:\n        \"\"\"Get deployment events for a session[s].\n\n        Args:\n            ids (Union[str, List[str]]): Session ID[s].\n            verbose (bool, optional): Print events to stdout. Defaults to False.\n\n        Returns:\n            Optional[List[Dict[str, str]]]: A list of events for the session[s].\n\n        Notes:\n            When verbose is True, the events will be printed to stdout only.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import Session\n            &gt;&gt;&gt; session = Session()\n            &gt;&gt;&gt; session.events(ids=\"hjko98yghj\")\n            &gt;&gt;&gt; session.events(ids=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        if isinstance(ids, str):\n            ids = [ids]\n        results: list[dict[str, str]] = []\n        parameters: dict[str, str] = {\"view\": \"events\"}\n        for value in ids:\n            try:\n                response: Response = self.client.get(\n                    url=f\"session/{value}\",\n                    params=parameters,\n                )\n                results.append({value: response.text})\n            except HTTPError:\n                err = f\"Failed to fetch events for session {value}\"\n                log.exception(err)\n        if verbose and results:\n            for result in results:\n                for key, value in result.items():\n                    log.info(\"Session ID: %s\", key)\n                    log.info(\"\\n %s\", value)\n        return results if not verbose else None\n\n    def destroy(self, ids: str | list[str]) -&gt; dict[str, bool]:\n        \"\"\"Destroy canfar session[s].\n\n        Args:\n            ids (Union[str, List[str]]): Session ID[s].\n\n        Returns:\n            Dict[str, bool]: A dictionary of session IDs\n            and a bool indicating if the session was destroyed.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import Session\n            &gt;&gt;&gt; session = Session()\n            &gt;&gt;&gt; session.destroy(id=\"hjko98yghj\")\n            &gt;&gt;&gt; session.destroy(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        if isinstance(ids, str):\n            ids = [ids]\n        results: dict[str, bool] = {}\n        for value in ids:\n            try:\n                self.client.delete(url=f\"session/{value}\")\n                results[value] = True\n            except HTTPError:\n                msg = f\"Failed to destroy session {value}\"\n                log.exception(msg)\n                results[value] = False\n        return results\n\n    def destroy_with(\n        self,\n        prefix: str,\n        kind: Kind = \"headless\",\n        status: Status = \"Succeeded\",\n    ) -&gt; dict[str, bool]:\n        \"\"\"Destroy session[s] matching search criteria.\n\n        Args:\n            prefix (str): Prefix to match in the session name.\n            kind (Kind): Type of session. Defaults to \"headless\".\n            status (Status): Status of the session. Defaults to \"Succeeded\".\n\n\n        Returns:\n            Dict[str, bool]: A dictionary of session IDs\n            and a bool indicating if the session was destroyed.\n\n        Notes:\n            The prefix is case-sensitive.\n            This method is useful for destroying multiple sessions at once.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import Session\n            &gt;&gt;&gt; session = Session()\n            &gt;&gt;&gt; session.destroy_with(prefix=\"test\")\n            &gt;&gt;&gt; session.destroy_with(prefix=\"test\", kind=\"desktop\")\n            &gt;&gt;&gt; session.destroy_with(prefix=\"test\", kind=\"headless\", status=\"Running\")\n\n        \"\"\"\n        sessions = self.fetch(kind=kind, status=status)\n        ids: list[str] = [\n            session[\"id\"] for session in sessions if session[\"name\"].startswith(prefix)\n        ]\n        return self.destroy(ids)\n\n    def connect(self, ids: list[str] | str) -&gt; None:\n        \"\"\"Open session[s] in a web browser.\n\n        Args:\n            ids (Union[List[str], str]): Session ID[s].\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import Session\n            &gt;&gt;&gt; session = Session()\n            &gt;&gt;&gt; session.browse(id=\"hjko98yghj\")\n            &gt;&gt;&gt; session.browse(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        if isinstance(ids, str):\n            ids = [ids]\n        for value in ids:\n            info = self.info(value)\n            connect_url = info[0][\"connectURL\"]\n            open_new_tab(connect_url)\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.connect","title":"<code>connect(ids)</code>","text":"<p>Open session[s] in a web browser.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>Union[List[str], str]</code> <p>Session ID[s].</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import Session\n&gt;&gt;&gt; session = Session()\n&gt;&gt;&gt; session.browse(id=\"hjko98yghj\")\n&gt;&gt;&gt; session.browse(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def connect(self, ids: list[str] | str) -&gt; None:\n    \"\"\"Open session[s] in a web browser.\n\n    Args:\n        ids (Union[List[str], str]): Session ID[s].\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import Session\n        &gt;&gt;&gt; session = Session()\n        &gt;&gt;&gt; session.browse(id=\"hjko98yghj\")\n        &gt;&gt;&gt; session.browse(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    if isinstance(ids, str):\n        ids = [ids]\n    for value in ids:\n        info = self.info(value)\n        connect_url = info[0][\"connectURL\"]\n        open_new_tab(connect_url)\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.create","title":"<code>create(name, image, cores=2, ram=4, kind='headless', gpu=None, cmd=None, args=None, env=None, replicas=1)</code>","text":"<p>Launch a canfar session.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A unique name for the session.</p> required <code>image</code> <code>str</code> <p>Container image to use for the session.</p> required <code>cores</code> <code>int</code> <p>Number of cores. Defaults to 2.</p> <code>2</code> <code>ram</code> <code>int</code> <p>Amount of RAM (GB). Defaults to 4.</p> <code>4</code> <code>kind</code> <code>str</code> <p>Type of canfar session. Defaults to \"headless\".</p> <code>'headless'</code> <code>gpu</code> <code>Optional[int]</code> <p>Number of GPUs. Defaults to None.</p> <code>None</code> <code>cmd</code> <code>Optional[str]</code> <p>Command to run. Defaults to None.</p> <code>None</code> <code>args</code> <code>Optional[str]</code> <p>Arguments to the command. Defaults to None.</p> <code>None</code> <code>env</code> <code>Optional[Dict[str, Any]]</code> <p>Environment variables to inject. Defaults to None.</p> <code>None</code> <code>replicas</code> <code>int</code> <p>Number of sessions to launch. Defaults to 1.</p> <code>1</code> Notes <p>The name of the session suffixed with the replica number. eg. test-1, test-2 Each container will have the following environment variables injected:     * REPLICA_ID - The replica number     * REPLICA_COUNT - The total number of replicas</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: A list of session IDs for the launched sessions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import Session\n&gt;&gt;&gt; session = Session()\n&gt;&gt;&gt; session.create(\n        name=\"test\",\n        image='images.canfar.net/skaha/terminal:1.1.1',\n        cores=2,\n        ram=8,\n        gpu=1,\n        kind=\"headless\",\n        cmd=\"env\",\n        env={\"TEST\": \"test\"},\n        replicas=2,\n    )\n&gt;&gt;&gt; [\"hjko98yghj\", \"ikvp1jtp\"]\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def create(\n    self,\n    name: str,\n    image: str,\n    cores: int = 2,\n    ram: int = 4,\n    kind: Kind = \"headless\",\n    gpu: int | None = None,\n    cmd: str | None = None,\n    args: str | None = None,\n    env: dict[str, Any] | None = None,\n    replicas: int = 1,\n) -&gt; list[str]:\n    \"\"\"Launch a canfar session.\n\n    Args:\n        name (str): A unique name for the session.\n        image (str): Container image to use for the session.\n        cores (int, optional): Number of cores. Defaults to 2.\n        ram (int, optional): Amount of RAM (GB). Defaults to 4.\n        kind (str, optional): Type of canfar session. Defaults to \"headless\".\n        gpu (Optional[int], optional): Number of GPUs. Defaults to None.\n        cmd (Optional[str], optional): Command to run. Defaults to None.\n        args (Optional[str], optional): Arguments to the command. Defaults to None.\n        env (Optional[Dict[str, Any]], optional): Environment variables to inject.\n            Defaults to None.\n        replicas (int, optional): Number of sessions to launch. Defaults to 1.\n\n    Notes:\n        The name of the session suffixed with the replica number. eg. test-1, test-2\n        Each container will have the following environment variables injected:\n            * REPLICA_ID - The replica number\n            * REPLICA_COUNT - The total number of replicas\n\n    Returns:\n        List[str]: A list of session IDs for the launched sessions.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import Session\n        &gt;&gt;&gt; session = Session()\n        &gt;&gt;&gt; session.create(\n                name=\"test\",\n                image='images.canfar.net/skaha/terminal:1.1.1',\n                cores=2,\n                ram=8,\n                gpu=1,\n                kind=\"headless\",\n                cmd=\"env\",\n                env={\"TEST\": \"test\"},\n                replicas=2,\n            )\n        &gt;&gt;&gt; [\"hjko98yghj\", \"ikvp1jtp\"]\n    \"\"\"\n    payloads = build.create_parameters(\n        name,\n        image,\n        cores,\n        ram,\n        kind,\n        gpu,\n        cmd,\n        args,\n        env,\n        replicas,\n    )\n    results: list[str] = []\n    log.debug(\"Creating %d %s session[s].\", replicas, kind)\n    for payload in payloads:\n        try:\n            response: Response = self.client.post(url=\"session\", params=payload)\n            results.append(response.text.rstrip(\"\\r\\n\"))\n        except HTTPError:\n            err = f\"Failed to create session with payload: {payload}\"\n            log.exception(err)\n    return results\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.destroy","title":"<code>destroy(ids)</code>","text":"<p>Destroy canfar session[s].</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>Union[str, List[str]]</code> <p>Session ID[s].</p> required <p>Returns:</p> Type Description <code>dict[str, bool]</code> <p>Dict[str, bool]: A dictionary of session IDs</p> <code>dict[str, bool]</code> <p>and a bool indicating if the session was destroyed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import Session\n&gt;&gt;&gt; session = Session()\n&gt;&gt;&gt; session.destroy(id=\"hjko98yghj\")\n&gt;&gt;&gt; session.destroy(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def destroy(self, ids: str | list[str]) -&gt; dict[str, bool]:\n    \"\"\"Destroy canfar session[s].\n\n    Args:\n        ids (Union[str, List[str]]): Session ID[s].\n\n    Returns:\n        Dict[str, bool]: A dictionary of session IDs\n        and a bool indicating if the session was destroyed.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import Session\n        &gt;&gt;&gt; session = Session()\n        &gt;&gt;&gt; session.destroy(id=\"hjko98yghj\")\n        &gt;&gt;&gt; session.destroy(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    if isinstance(ids, str):\n        ids = [ids]\n    results: dict[str, bool] = {}\n    for value in ids:\n        try:\n            self.client.delete(url=f\"session/{value}\")\n            results[value] = True\n        except HTTPError:\n            msg = f\"Failed to destroy session {value}\"\n            log.exception(msg)\n            results[value] = False\n    return results\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.destroy_with","title":"<code>destroy_with(prefix, kind='headless', status='Succeeded')</code>","text":"<p>Destroy session[s] matching search criteria.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Prefix to match in the session name.</p> required <code>kind</code> <code>Kind</code> <p>Type of session. Defaults to \"headless\".</p> <code>'headless'</code> <code>status</code> <code>Status</code> <p>Status of the session. Defaults to \"Succeeded\".</p> <code>'Succeeded'</code> <p>Returns:</p> Type Description <code>dict[str, bool]</code> <p>Dict[str, bool]: A dictionary of session IDs</p> <code>dict[str, bool]</code> <p>and a bool indicating if the session was destroyed.</p> Notes <p>The prefix is case-sensitive. This method is useful for destroying multiple sessions at once.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import Session\n&gt;&gt;&gt; session = Session()\n&gt;&gt;&gt; session.destroy_with(prefix=\"test\")\n&gt;&gt;&gt; session.destroy_with(prefix=\"test\", kind=\"desktop\")\n&gt;&gt;&gt; session.destroy_with(prefix=\"test\", kind=\"headless\", status=\"Running\")\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def destroy_with(\n    self,\n    prefix: str,\n    kind: Kind = \"headless\",\n    status: Status = \"Succeeded\",\n) -&gt; dict[str, bool]:\n    \"\"\"Destroy session[s] matching search criteria.\n\n    Args:\n        prefix (str): Prefix to match in the session name.\n        kind (Kind): Type of session. Defaults to \"headless\".\n        status (Status): Status of the session. Defaults to \"Succeeded\".\n\n\n    Returns:\n        Dict[str, bool]: A dictionary of session IDs\n        and a bool indicating if the session was destroyed.\n\n    Notes:\n        The prefix is case-sensitive.\n        This method is useful for destroying multiple sessions at once.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import Session\n        &gt;&gt;&gt; session = Session()\n        &gt;&gt;&gt; session.destroy_with(prefix=\"test\")\n        &gt;&gt;&gt; session.destroy_with(prefix=\"test\", kind=\"desktop\")\n        &gt;&gt;&gt; session.destroy_with(prefix=\"test\", kind=\"headless\", status=\"Running\")\n\n    \"\"\"\n    sessions = self.fetch(kind=kind, status=status)\n    ids: list[str] = [\n        session[\"id\"] for session in sessions if session[\"name\"].startswith(prefix)\n    ]\n    return self.destroy(ids)\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.events","title":"<code>events(ids, verbose=False)</code>","text":"<p>Get deployment events for a session[s].</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>Union[str, List[str]]</code> <p>Session ID[s].</p> required <code>verbose</code> <code>bool</code> <p>Print events to stdout. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[dict[str, str]] | None</code> <p>Optional[List[Dict[str, str]]]: A list of events for the session[s].</p> Notes <p>When verbose is True, the events will be printed to stdout only.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import Session\n&gt;&gt;&gt; session = Session()\n&gt;&gt;&gt; session.events(ids=\"hjko98yghj\")\n&gt;&gt;&gt; session.events(ids=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def events(\n    self,\n    ids: str | list[str],\n    verbose: bool = False,\n) -&gt; list[dict[str, str]] | None:\n    \"\"\"Get deployment events for a session[s].\n\n    Args:\n        ids (Union[str, List[str]]): Session ID[s].\n        verbose (bool, optional): Print events to stdout. Defaults to False.\n\n    Returns:\n        Optional[List[Dict[str, str]]]: A list of events for the session[s].\n\n    Notes:\n        When verbose is True, the events will be printed to stdout only.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import Session\n        &gt;&gt;&gt; session = Session()\n        &gt;&gt;&gt; session.events(ids=\"hjko98yghj\")\n        &gt;&gt;&gt; session.events(ids=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    if isinstance(ids, str):\n        ids = [ids]\n    results: list[dict[str, str]] = []\n    parameters: dict[str, str] = {\"view\": \"events\"}\n    for value in ids:\n        try:\n            response: Response = self.client.get(\n                url=f\"session/{value}\",\n                params=parameters,\n            )\n            results.append({value: response.text})\n        except HTTPError:\n            err = f\"Failed to fetch events for session {value}\"\n            log.exception(err)\n    if verbose and results:\n        for result in results:\n            for key, value in result.items():\n                log.info(\"Session ID: %s\", key)\n                log.info(\"\\n %s\", value)\n    return results if not verbose else None\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.fetch","title":"<code>fetch(kind=None, status=None, view=None)</code>","text":"<p>Fetch open sessions for the user.</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>Kind | None</code> <p>Session kind. Defaults to None.</p> <code>None</code> <code>status</code> <code>Status | None</code> <p>Session status. Defaults to None.</p> <code>None</code> <code>view</code> <code>View | None</code> <p>View leve. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict[str, str]]</code> <p>list[dict[str, str]]: Session[s] information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import Session\n&gt;&gt;&gt; session = Session()\n&gt;&gt;&gt; session.fetch(kind=\"notebook\")\n[{'id': 'ikvp1jtp',\n  'userid': 'username',\n  'image': 'image-server/image/label:latest',\n  'type': 'notebook',\n  'status': 'Running',\n  'name': 'example-notebook',\n  'startTime': '2222-12-14T02:24:06Z',\n  'connectURL': 'https://something.example.com/ikvp1jtp',\n  'requestedRAM': '16G',\n  'requestedCPUCores': '2',\n  'requestedGPUCores': '&lt;none&gt;',\n  'coresInUse': '0m',\n  'ramInUse': '101Mi'}]\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def fetch(\n    self,\n    kind: Kind | None = None,\n    status: Status | None = None,\n    view: View | None = None,\n) -&gt; list[dict[str, str]]:\n    \"\"\"Fetch open sessions for the user.\n\n    Args:\n        kind (Kind | None, optional): Session kind. Defaults to None.\n        status (Status | None, optional): Session status. Defaults to None.\n        view (View | None, optional): View leve. Defaults to None.\n\n    Returns:\n        list[dict[str, str]]: Session[s] information.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import Session\n        &gt;&gt;&gt; session = Session()\n        &gt;&gt;&gt; session.fetch(kind=\"notebook\")\n        [{'id': 'ikvp1jtp',\n          'userid': 'username',\n          'image': 'image-server/image/label:latest',\n          'type': 'notebook',\n          'status': 'Running',\n          'name': 'example-notebook',\n          'startTime': '2222-12-14T02:24:06Z',\n          'connectURL': 'https://something.example.com/ikvp1jtp',\n          'requestedRAM': '16G',\n          'requestedCPUCores': '2',\n          'requestedGPUCores': '&lt;none&gt;',\n          'coresInUse': '0m',\n          'ramInUse': '101Mi'}]\n    \"\"\"\n    parameters: dict[str, Any] = build.fetch_parameters(kind, status, view)\n    response: Response = self.client.get(url=\"session\", params=parameters)\n    data: list[dict[str, str]] = response.json()\n    return data\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.info","title":"<code>info(ids)</code>","text":"<p>Get information about session[s].</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>Union[List[str], str]</code> <p>Session ID[s].</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>Dict[str, Any]: Session information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; session.info(session_id=\"hjko98yghj\")\n&gt;&gt;&gt; session.info(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def info(self, ids: list[str] | str) -&gt; list[dict[str, Any]]:\n    \"\"\"Get information about session[s].\n\n    Args:\n        ids (Union[List[str], str]): Session ID[s].\n\n    Returns:\n        Dict[str, Any]: Session information.\n\n    Examples:\n        &gt;&gt;&gt; session.info(session_id=\"hjko98yghj\")\n        &gt;&gt;&gt; session.info(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    # Convert id to list if it is a string\n    if isinstance(ids, str):\n        ids = [ids]\n    parameters: dict[str, str] = {\"view\": \"event\"}\n    results: list[dict[str, Any]] = []\n    for value in ids:\n        try:\n            response: Response = self.client.get(\n                url=f\"session/{value}\",\n                params=parameters,\n            )\n            results.append(response.json())\n        except HTTPError:\n            err = f\"failed to fetch session info for {value}\"\n            log.exception(err)\n    return results\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.logs","title":"<code>logs(ids, verbose=False)</code>","text":"<p>Get logs from a session[s].</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>Union[List[str], str]</code> <p>Session ID[s].</p> required <code>verbose</code> <code>bool</code> <p>Print logs to stdout. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, str] | None</code> <p>Dict[str, str]: Logs in text/plain format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; session.logs(id=\"hjko98yghj\")\n&gt;&gt;&gt; session.logs(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def logs(\n    self,\n    ids: list[str] | str,\n    verbose: bool = False,\n) -&gt; dict[str, str] | None:\n    \"\"\"Get logs from a session[s].\n\n    Args:\n        ids (Union[List[str], str]): Session ID[s].\n        verbose (bool, optional): Print logs to stdout. Defaults to False.\n\n    Returns:\n        Dict[str, str]: Logs in text/plain format.\n\n    Examples:\n        &gt;&gt;&gt; session.logs(id=\"hjko98yghj\")\n        &gt;&gt;&gt; session.logs(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    if isinstance(ids, str):\n        ids = [ids]\n    parameters: dict[str, str] = {\"view\": \"logs\"}\n    results: dict[str, str] = {}\n\n    for value in ids:\n        try:\n            response: Response = self.client.get(\n                url=f\"session/{value}\",\n                params=parameters,\n            )\n            results[value] = response.text\n        except HTTPError:\n            err = f\"failed to fetch logs for session {value}\"\n            log.exception(err)\n\n    if verbose:\n        for key, value in results.items():\n            log.info(\"Session ID: %s\\n\", key)\n            log.info(value)\n        return None\n\n    return results\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.stats","title":"<code>stats()</code>","text":"<p>Get statistics for the entire platform.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: Cluster statistics.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import Session\n&gt;&gt;&gt; session = Session()\n&gt;&gt;&gt; session.stats()\n{'instances': {\n 'session': 88, 'desktopApp': 30, 'headless': 0, 'total': 118},\n 'cores': {'requestedCPUCores': 377,\n 'coresAvailable': 960,\n 'maxCores': {'cores': 32, 'withRam': '147Gi'}},\n 'ram': {'maxRAM': {'ram': '226Gi', 'withCores': 32}}}\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get statistics for the entire platform.\n\n    Returns:\n        Dict[str, Any]: Cluster statistics.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import Session\n        &gt;&gt;&gt; session = Session()\n        &gt;&gt;&gt; session.stats()\n        {'instances': {\n         'session': 88, 'desktopApp': 30, 'headless': 0, 'total': 118},\n         'cores': {'requestedCPUCores': 377,\n         'coresAvailable': 960,\n         'maxCores': {'cores': 32, 'withRam': '147Gi'}},\n         'ram': {'maxRAM': {'ram': '226Gi', 'withCores': 32}}}\n    \"\"\"\n    parameters = {\"view\": \"stats\"}\n    response: Response = self.client.get(\"session\", params=parameters)\n    data: dict[str, Any] = response.json()\n    return data\n</code></pre>"},{"location":"client/testing/","title":"Testing","text":"<p>This document provides comprehensive information about testing opencadc/canfar.</p>"},{"location":"client/testing/#overview","title":"Overview","text":"<p>Canfar uses pytest as its testing framework. The test suite includes unit tests, integration tests, and end-to-end tests that verify the functionality of the client library.</p>"},{"location":"client/testing/#prerequisites","title":"Prerequisites","text":"<p>To run tests for Canfar, you need:</p> <ol> <li>Valid CANFAR Account: Access to the CANFAR Science Platform</li> <li>X.509 Certificate: For authentication with CANFAR services</li> <li>Python Environment: Set up with uv</li> </ol> <p>For certificate generation, refer to the get started section.</p>"},{"location":"client/testing/#running-tests","title":"Running Tests","text":""},{"location":"client/testing/#basic-test-execution","title":"Basic Test Execution","text":"<p>Run all tests: <pre><code>uv run pytest\n</code></pre></p> <p>Run tests with verbose output: <pre><code>uv run pytest -v\n</code></pre></p> <p>Run tests with coverage report: <pre><code>uv run pytest --cov\n</code></pre></p>"},{"location":"client/testing/#test-categories","title":"Test Categories","text":"<p>Canfar tests are organized with markers to help you run specific subsets:</p>"},{"location":"client/testing/#slow-tests","title":"Slow Tests","text":"<p>Some tests are marked as \"slow\" because they involve: - Network operations with CANFAR services - Waiting for session state changes - Authentication timeouts - Long-running operations</p> <p>Skip slow tests for faster development: <pre><code>uv run pytest -m \"not slow\"\n</code></pre></p> <p>Run only slow tests: <pre><code>uv run pytest -m \"slow\"\n</code></pre></p>"},{"location":"client/testing/#integration-tests","title":"Integration Tests","text":"<p>Tests that interact with external services: <pre><code>uv run pytest -m \"integration\"\n</code></pre></p>"},{"location":"client/testing/#unit-tests","title":"Unit Tests","text":"<p>Fast, isolated tests: <pre><code>uv run pytest -m \"unit\"\n</code></pre></p>"},{"location":"client/testing/#test-methodology","title":"Test Methodology","text":"<p>Tests are organized in the <code>tests/</code> directory and follow a specific naming convention that mirrors the source code structure. This approach ensures that tests are easy to locate and maintain.</p> <p>The naming convention is as follows:</p> <ul> <li>If the source file is <code>canfar/path/to/file.py</code>, the corresponding test file will be <code>tests/test_path_to_file.py</code>.</li> <li>If the source file is <code>canfar/module.py</code>, the corresponding test file will be <code>tests/test_module.py</code>.</li> </ul> <p>For example:</p> <ul> <li>The tests for <code>canfar/client.py</code> are located in <code>tests/test_client.py</code>.</li> <li>The tests for <code>canfar/auth/oidc.py</code> are located in <code>tests/test_auth_oidc.py</code>.</li> </ul> <p>This structure makes it straightforward to find the tests associated with a particular module or file.</p>"},{"location":"client/testing/#development-workflow","title":"Development Workflow","text":"<p>For efficient development, follow this testing workflow:</p> <ol> <li> <p>During Development: Run fast tests only    <pre><code>uv run pytest -m \"not slow\"\n</code></pre></p> </li> <li> <p>Before Committing: Run the full test suite    <pre><code>uv run pytest\n</code></pre></p> </li> <li> <p>Debugging Specific Issues: Run individual test files    <pre><code>uv run pytest tests/test_session.py\n</code></pre></p> </li> </ol>"},{"location":"client/testing/#test-configuration","title":"Test Configuration","text":"<p>Test configuration is defined in <code>pyproject.toml</code>:</p> <pre><code>[tool.pytest.ini_options]\nmarkers = [\n    \"integration: marks tests as integration tests\",\n    \"unit: marks tests as unit tests\",\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\n    \"order: marks tests that need to run in a specific order\",\n]\n</code></pre>"},{"location":"client/testing/#continuous-integration","title":"Continuous Integration","text":"<p>In CI environments, all tests (including slow ones) are executed to ensure complete validation. The CI pipeline:</p> <ol> <li>Sets up authentication with CANFAR</li> <li>Runs the complete test suite</li> <li>Generates coverage reports</li> <li>Cleans up authentication artifacts</li> </ol>"},{"location":"client/testing/#writing-tests","title":"Writing Tests","text":"<p>When contributing new tests:</p> <ol> <li>Follow the naming convention: Create a test file that mirrors the source file's path and name.</li> <li>Mark slow tests: Add <code>@pytest.mark.slow</code> to any test that involves network operations, interacts with external services, or has long execution times. This allows developers to skip these tests for a faster development cycle.</li> <li>Use appropriate markers: Mark tests as <code>unit</code>, <code>integration</code>, etc.</li> <li>Add docstrings: Document what each test verifies.</li> </ol> <p>Example of a slow test: <pre><code>import pytest\n\n@pytest.mark.slow\ndef test_long_running_operation():\n    \"\"\"Test that involves waiting or network operations.\"\"\"\n    # Test implementation\n    pass\n</code></pre></p>"},{"location":"client/testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"client/testing/#authentication-issues","title":"Authentication Issues","text":"<ul> <li>Ensure your X.509 certificate is valid and not expired</li> <li>Check that you have access to the CANFAR Science Platform</li> <li>Verify your certificate is in the correct location (<code>~/.ssl/</code>)</li> </ul>"},{"location":"client/testing/#slow-test-timeouts","title":"Slow Test Timeouts","text":"<ul> <li>Slow tests have built-in timeouts (typically 60 seconds)</li> <li>If tests consistently timeout, check your network connection</li> <li>Platform availability may affect test execution times</li> </ul>"},{"location":"client/testing/#test-failures","title":"Test Failures","text":"<ul> <li>Check if the CANFAR Science Platform is accessible</li> <li>Verify your authentication credentials</li> <li>Review test logs for specific error messages</li> </ul>"},{"location":"client/updates/","title":"What's New in CANFAR","text":"<p>Stay up to date with the latest features, improvements, and changes in CANFAR.</p> <p>v1.0</p> <p> Breaking Changes</p> <ul> <li>Deprecation of support for Python 3.8 and 3.9.</li> <li>The Python package has been renamed from <code>skaha</code> to <code>canfar</code>.</li> <li>The <code>skaha.session</code> API has been deprecated in favor of <code>canfar.sessions</code>.</li> <li>See Migration guide to migrate from skaha \u2192 canfar.</li> </ul> <p> CLI Support</p> <ul> <li>Comprehensive CLI support has been added to the client under the <code>canfar</code> entry point. See CLI Reference for more information.</li> <li>The <code>canfar</code> CLI is the recommended way to manage authentication. See Authentication Contexts for more information.</li> </ul> <p>\ud83c\udf0e SRCnet Support</p> <ul> <li>CANFAR now supports launching sessions on all the SRCnet CANFAR Science Platform instances worldwide.</li> </ul> <p> OIDC Authentication</p> <ul> <li>OpenID Connect (OIDC) authentication is now supported for all SRCnet Science Platform servers where applicable.</li> </ul> <p> Documentation</p> <ul> <li>Complete overhaul to bring all documentation sources under a single roof.</li> <li>Significant improvements to the Python client and brand new CLI documentation.</li> </ul>"},{"location":"client/updates/#recent-updates","title":"Recent Updates","text":"<p>New in v0.7+</p> <p>New in v0.4+</p>"},{"location":"client/updates/#enhanced-authentication-system","title":"\ud83d\udd10 Enhanced Authentication System","text":"<p>Canfar now features a comprehensive authentication system with support for multiple authentication modes and automatic credential management.</p> Authentication Examples<pre><code>from canfar.client import HTTPClient\nfrom pathlib import Path\n\n# X.509 certificate authentication\nclient = HTTPClient(certificate=Path(\"/path/to/cert.pem\"))\n\n# OIDC token authentication (configured)\nclient = HTTPClient()  # Uses auth.mode = \"oidc\"\n\n# Bearer token authentication\nfrom pydantic import SecretStr\nclient = HTTPClient(token=SecretStr(\"your-token\"))\n</code></pre>"},{"location":"client/updates/#asynchronous-sessions","title":"\ud83d\ude80 Asynchronous Sessions","text":"<p>Canfar now supports asynchronous sessions using the <code>AsyncSession</code> class while maintaining 1-to-1 compatibility with the <code>Session</code> class.</p> Asynchronous Session Creation<pre><code>from canfar.session import AsyncSession\n\nasession = AsyncSession()\nresponse = await asession.create(\n    name=\"test\",\n    image=\"images.canfar.net/skaha/base-notebook:latest\",\n    cores=2,\n    ram=8,\n    gpu=1,\n    kind=\"headless\",\n    cmd=\"env\",\n    env={\"KEY\": \"VALUE\"},\n    replicas=3,\n)\n</code></pre>"},{"location":"client/updates/#backend-upgrades","title":"\ud83d\uddc4\ufe0f Backend Upgrades","text":"<ul> <li>\ud83d\udce1 Canfar now uses the <code>httpx</code> library for making HTTP requests instead of <code>requests</code>. This adds asynchronous support and also to circumvent the <code>requests</code> dependence on <code>urllib3</code> which was causing SSL issues on MacOS. See this issue for more details.</li> <li>\ud83d\udd11 Canfar now supports multiple authentication methods including X.509 certificates, OIDC tokens, and bearer tokens with automatic SSL context management.</li> <li>\ud83c\udfce\ufe0f\ud83d\udca8 Added <code>loglevel</code> and <code>concurrency</code> support to manage the new explosion in functionality!</li> <li>\ud83d\udd0d Comprehensive debug logging for authentication flow and client creation troubleshooting.</li> </ul>"},{"location":"client/updates/#logs-to-stdout","title":"\ud83e\uddfe Logs to <code>stdout</code>","text":"<p>The <code>[Session|AsyncSession].logs</code> method now prints colored output to <code>stdout</code> instead of returning them as a string with <code>verbose=True</code> flag.</p> Session Logs<pre><code>from canfar.session import AsyncSession\n\nasession = AsyncSession()\nawait asession.logs(ids=[\"some-uuid\"], verbose=True)\n</code></pre>"},{"location":"client/updates/#firefly-support","title":"\ud83e\udeb0 Firefly Support","text":"<p>Canfar now supports launching <code>firefly</code> session on the CANFAR Science Platform.</p> Firefly Session Creation<pre><code>session.create(\n    name=\"firefly\",\n    image=\"images.canfar.net/skaha/firefly:latest\",\n)\n</code></pre>"},{"location":"client/updates/#private-images","title":"\ud83d\udd10 Private Images","text":"<p>Starting October 2024, to create a session with a private container image from the CANFAR Harbor Registry, you will need to provide your harbor <code>username</code> and the <code>CLI Secret</code> through a <code>ContainerRegistry</code> object.</p> Private Image Registry Configuration<pre><code>from canfar.models import ContainerRegistry\nfrom canfar.session import Session\n\nregistry = ContainerRegistry(username=\"username\", secret=\"sUp3rS3cr3t\")\nsession = Session(registry=registry)\n</code></pre> <p>Alternatively, if you have environment variables, <code>CANFAR_REGISTRY_USERNAME</code> and <code>CANFAR_REGISTRY_SECRET</code>, you can create a <code>ContainerRegistry</code> object without providing the <code>username</code> and <code>secret</code>.</p> Private Image Registry with Environment Variables<pre><code>from canfar.models import ContainerRegistry\n\nregistry = ContainerRegistry()\n</code></pre>"},{"location":"client/updates/#destroy-sessions","title":"\ud83d\udca3 Destroy Sessions","text":"Destroying Sessions<pre><code>from canfar.session import Session\n\nsession = Session()\nsession.destroy_with(prefix=\"test\", kind=\"headless\", status=\"Running\")\nsession.destroy_with(prefix=\"test\", kind=\"headless\", status=\"Pending\")\n</code></pre>"},{"location":"client/updates/#previous-versions","title":"Previous Versions","text":"<p>For a complete history of changes, see the Changelog.</p>"},{"location":"client/updates/#stay-updated","title":"Stay Updated","text":"<ul> <li>\ud83d\udce2 GitHub Releases</li> <li>\ud83d\udcac Discussions</li> </ul>"},{"location":"platform/accounts/","title":"Accounts &amp; Permissions","text":"<p>Managing users, groups, and access control on CANFAR</p> <p>This section covers everything you need to know about user management, group permissions, and access control on the CANFAR platform. Whether you're setting up a new collaboration or managing an existing team, this guide will help you understand and configure permissions effectively.</p> <p>\ud83c\udfaf What You'll Learn</p> <p>By the end of this guide, you'll understand: - How CANFAR's permission system works - How to create and manage research groups - How to control access to files and containers - How to use APIs for programmatic access</p>"},{"location":"platform/accounts/#permissions-system","title":"\ud83d\udd13 Permissions System","text":"<p>CANFAR's permission system is built on several layers that work together to provide secure, flexible access control:</p> <p>Permission Layers</p> <ul> <li>CADC Accounts - Your base identity for accessing Canadian astronomy services</li> <li>Groups - Collections of users for collaborative access </li> <li>Harbor Permissions - Container registry access control</li> <li>ACL (Access Control Lists) - File-level permissions on <code>/arc</code> shared file system</li> <li>API Authentication - Programmatic access control</li> </ul>"},{"location":"platform/accounts/#group-management","title":"\ud83d\udc65 Group Management","text":"<p>Groups are the foundation of collaboration on CANFAR. A group defines who can access shared resources, what projects and storage they can use, and how they can interact.</p>"},{"location":"platform/accounts/#group-hierarchy","title":"Group Hierarchy","text":"<pre><code>graph TD\n    Admin[\"\ud83d\udc51 Group Administrator\"]\n    Members[\"\ud83d\udc64 Group Members\"]\n    Resources[\"\ud83d\udcbe Shared Resources\"]\n\n    Admin --&gt; |\"Manages\"| Members\n    Admin --&gt; |\"Controls access to\"| Resources\n    Members --&gt; |\"Access\"| Resources\n\n    Resources --&gt; Projects[\"\ud83d\udcc1 /arc/projects/groupname/\"]\n    Resources --&gt; Storage[\"\ud83d\udcbe Storage Quotas\"]\n    Resources --&gt; Containers[\"\ud83d\udc33 Container Access\"]</code></pre> <p>Key Concept</p> <p>Groups enable collaborative research by providing shared access to storage, computing resources, and container images while maintaining security boundaries.</p>"},{"location":"platform/accounts/#creating-and-managing-groups","title":"Creating and Managing Groups","text":"<p>Access the Group Management Interface:</p> <p>\ud83d\udd17 CADC Group Management</p>"},{"location":"platform/accounts/#step-1-create-a-new-group","title":"Step 1: Create a New Group","text":"<ol> <li>Click \"New Group\"</li> <li>Provide a meaningful group name (e.g., <code>myproject-team</code>)</li> <li>Add a brief description of the project or collaboration</li> <li>Click \"Create\"</li> </ol>"},{"location":"platform/accounts/#step-2-add-members","title":"Step 2: Add Members","text":"<ol> <li>Find your group in the list</li> <li>Click \"Edit\" in the Membership column</li> <li>Type the name (not username) of your collaborator</li> <li>Select from the search results</li> <li>Click \"Add member\"</li> </ol> <p>Finding Users</p> <p>The search function uses real names, not CADC usernames. Search for \"John Smith\" rather than \"jsmith\".</p>"},{"location":"platform/accounts/#step-3-assign-administrators","title":"Step 3: Assign Administrators","text":"<ol> <li>Click \"Edit\" in the Administrators column  </li> <li>Add users who should be able to manage the group</li> <li>Administrators can add/remove members and modify permissions</li> </ol>"},{"location":"platform/accounts/#member-roles","title":"Member Roles","text":"Role Permissions Best For Administrator Full group management, resource allocation Project PIs, senior team members Member Access shared resources, collaborate Researchers, grad students"},{"location":"platform/accounts/#harbor-permissions","title":"\ud83d\udd10 Harbor Permissions","text":"<p>Harbor is CANFAR's container registry where container images are stored and managed.</p> <p>Registry Access</p> <p>Registry URL: https://images.canfar.net</p>"},{"location":"platform/accounts/#access-levels","title":"Access Levels","text":"Permission Level Can Do Cannot Do Guest Pull public images Push images, see private repos Developer Pull all group images, push to group repos Delete images, manage projects Master Full project management System administration"},{"location":"platform/accounts/#managing-harbor-access","title":"Managing Harbor Access","text":"<p>Harbor permissions are typically managed by CANFAR administrators. Contact support@canfar.net to:</p> <ul> <li>Request access to a project repository</li> <li>Set up a new project for your container images</li> <li>Modify permissions for team members</li> </ul>"},{"location":"platform/accounts/#using-harbor","title":"Using Harbor","text":"<pre><code># Login to Harbor\ndocker login images.canfar.net\n\n# Pull a container\ndocker pull images.canfar.net/skaha/astroml:latest\n\n# Push your container (if you have permissions)\ndocker push images.canfar.net/myproject/custom-container:v1.0\n</code></pre>"},{"location":"platform/accounts/#acl-access-control-lists","title":"\ud83d\udee1\ufe0f Access Control Lists","text":""},{"location":"platform/accounts/#what-are-acls","title":"What are ACLs?","text":"<p>Access Control Lists (ACLs) provide fine-grained file and directory permissions beyond traditional POSIX permissions. While POSIX permissions only support owner/group/other with read/write/execute, ACLs allow you to grant specific permissions to individual users and groups.</p> <p>Important Distinction</p> <p>ACLs extend traditional POSIX permissions, allowing multiple users and groups to have different permissions on the same file or directory.</p>"},{"location":"platform/accounts/#why-acls-matter-in-astronomy","title":"Why ACLs Matter in Astronomy","text":"<p>Traditional POSIX Limitations: - Only one group can own a file - No granular control over multiple collaborators - Difficult to share data across research groups</p> <p>ACL Advantages: - Multiple users and groups can have different permissions on the same file - Grant specific researchers read access to your dataset - Allow collaborators to write to specific directories - Maintain security while enabling flexible collaboration</p> <p>Research Collaboration</p> <p>ACLs enable flexible data sharing across research groups while maintaining security boundaries - perfect for multi-institutional astronomy projects.</p>"},{"location":"platform/accounts/#acl-vs-posix-comparison","title":"ACL vs POSIX Comparison","text":"Scenario POSIX Permissions ACL Permissions Single collaboration <code>rwxrwx---</code> (group access) Same as POSIX Multi-group project Must choose one group Grant specific access to multiple groups Guest researcher access Add to group or world-readable Grant individual read access Selective write access All group members can write Grant write access only to specific users"},{"location":"platform/accounts/#viewing-acls","title":"Viewing ACLs","text":"<pre><code># View ACL permissions\ngetfacl /arc/projects/myproject/sensitive_data/\n\n# Output example:\n# file: sensitive_data/\n# owner: alice\n# group: myproject-team\n# user::rwx\n# user:bob:r--           # Bob has read-only access\n# user:carol:rw-         # Carol can read and write\n# group::r--             # Group has read-only\n# group:external-team:r-- # External group has read access\n# mask::rwx\n# other::---             # No access for others\n</code></pre>"},{"location":"platform/accounts/#setting-acls","title":"Setting ACLs","text":"<pre><code># Grant user 'bob' read access to a directory\nsetfacl -m u:bob:r /arc/projects/myproject/shared_data/\n\n# Grant group 'external-collab' read access\nsetfacl -m g:external-collab:r /arc/projects/myproject/public_results/\n\n# Grant user 'alice' full access to a file\nsetfacl -m u:alice:rw /arc/projects/myproject/scripts/analysis.py\n\n# Remove ACL entry\nsetfacl -x u:bob /arc/projects/myproject/sensitive_data/\n\n# Remove all ACLs\nsetfacl -b /arc/projects/myproject/temp_data/\n</code></pre>"},{"location":"platform/accounts/#acl-best-practices","title":"ACL Best Practices","text":"<p>\ud83d\udcc1 Directory Structure with ACLs:</p> <pre><code>/arc/projects/myproject/\n\u251c\u2500\u2500 public/          # World-readable results\n\u2502   \u2514\u2500\u2500 (ACL: group:world:r)\n\u251c\u2500\u2500 team/           # Full team access\n\u2502   \u2514\u2500\u2500 (ACL: group:myproject-team:rw)\n\u251c\u2500\u2500 admin/          # Admin-only access\n\u2502   \u2514\u2500\u2500 (ACL: user:pi:rw, group:admins:rw)\n\u2514\u2500\u2500 external/       # Controlled external access\n    \u2514\u2500\u2500 (ACL: user:collaborator:r, group:external-team:r)\n</code></pre> <p>ACL Best Practices</p> <ul> <li>Principle of least privilege - Grant minimum necessary access</li> <li>Regular audits - Review ACLs periodically with <code>getfacl</code></li> <li>Document permissions - Keep notes on why specific ACLs were set</li> <li>Use groups when possible - Easier to manage than individual user ACLs</li> </ul>"},{"location":"platform/accounts/#api-authentication","title":"\ud83d\udd0c API Authentication","text":""},{"location":"platform/accounts/#overview","title":"Overview","text":"<p>CANFAR provides REST APIs for programmatic access to platform features. All API calls require proper authentication.</p> <p>API Access</p> <p>APIs enable automation and integration with external tools and workflows.</p>"},{"location":"platform/accounts/#authentication-methods","title":"Authentication Methods","text":""},{"location":"platform/accounts/#method-1-bearer-tokens-recommended","title":"Method 1: Bearer Tokens (Recommended)","text":"<p>Best for: Short-term automation, development, interactive use</p> <pre><code># Get a 48-hour token\ncurl https://ws-cadc.canfar.net/ac/login \\\n  -d \"username=your_username\" \\\n  -d \"password=your_password\"\n\n# Use token in API calls\ncurl -H \"Authorization: Bearer YOUR_TOKEN\" \\\n  https://ws-uv.canfar.net/skaha/v0/session\n</code></pre>"},{"location":"platform/accounts/#method-2-proxy-certificates","title":"Method 2: Proxy Certificates","text":"<p>Best for: Long-term automation, file transfers, production scripts</p> <pre><code># Install CADC utilities\npip install cadcutils\n\n# Generate proxy certificate\ncadc-get-cert -u your_username\n\n# Certificate stored in ~/.ssl/cadcproxy.pem\n# Valid for 10 days, automatically used by CADC tools\n</code></pre>"},{"location":"platform/accounts/#api-examples","title":"API Examples","text":""},{"location":"platform/accounts/#session-management","title":"Session Management","text":"<pre><code># List active sessions\ncurl -H \"Authorization: Bearer TOKEN\" \\\n  https://ws-uv.canfar.net/skaha/v0/session\n\n# Launch new session  \ncurl -H \"Authorization: Bearer TOKEN\" \\\n  -d \"name=my-analysis\" \\\n  -d \"image=images.canfar.net/skaha/astroml:latest\" \\\n  https://ws-uv.canfar.net/skaha/v0/session\n\n# Delete session\ncurl -X DELETE \\\n  -H \"Authorization: Bearer TOKEN\" \\\n  https://ws-uv.canfar.net/skaha/v0/session/SESSION_ID\n</code></pre>"},{"location":"platform/accounts/#file-operations-vospace","title":"File Operations (VOSpace)","text":"<pre><code># List files\ncurl -H \"Authorization: Bearer TOKEN\" \\\n  https://ws-cadc.canfar.net/vospace/nodes/myproject\n\n# Upload file\ncurl -X PUT \\\n  -H \"Authorization: Bearer TOKEN\" \\\n  -H \"Content-Type: application/octet-stream\" \\\n  --data-binary @local_file.fits \\\n  https://ws-cadc.canfar.net/vospace/data/myproject/remote_file.fits\n</code></pre>"},{"location":"platform/accounts/#api-resources","title":"API Resources","text":"Service Documentation Purpose Skaha ws-uv.canfar.net Session management VOSpace CADC VOSpace File operations CADC Auth CADC Services Authentication"},{"location":"platform/accounts/#common-issues","title":"\ud83d\udea8 Common Issues","text":"<p>Troubleshooting Guide</p> <p>These are the most common permission issues and their solutions.</p>"},{"location":"platform/accounts/#problem-permission-denied-accessing-arcprojects","title":"Problem: \"Permission Denied\" accessing <code>/arc/projects/</code>","text":"<p>Cause: Not a member of the project group</p> <p>Solution: 1. Contact project administrator of your team to add you to the group 2. Verify group membership at CADC Group Management</p>"},{"location":"platform/accounts/#problem-cannot-push-to-harbor-container-registry","title":"Problem: Cannot push to Harbor container registry","text":"<p>Cause: Insufficient Harbor permissions</p> <p>Solution: 1. Contact support@canfar.net to request developer access 2. Verify you're logged into Harbor: <code>docker login images.canfar.net</code></p>"},{"location":"platform/accounts/#problem-api-calls-return-401-unauthorized","title":"Problem: API calls return 401 Unauthorized","text":"<p>Cause: Invalid or expired authentication token</p> <p>Solution: 1. Generate new token: <code>curl https://ws-cadc.canfar.net/ac/login -d \"username=...\" -d \"password=...\"</code> 2. Check token format in Authorization header: <code>Bearer YOUR_TOKEN</code></p>"},{"location":"platform/accounts/#problem-acl-changes-not-taking-effect","title":"Problem: ACL changes not taking effect","text":"<p>Cause: ACL mask or inheritance issues</p> <p>Solution: 1. Check effective permissions: <code>getfacl filename</code> 2. Update ACL mask: <code>setfacl -m m::rwx filename</code> 3. Set default ACLs for directories: <code>setfacl -d -m g:groupname:rw directory/</code></p>"},{"location":"platform/accounts/#whats-next","title":"\ud83d\udd17 What's Next?","text":"<p>Now that you understand permissions and access control:</p> <ul> <li>Storage Guide \u2192 - Apply permissions to manage data</li> <li>Container Guide \u2192 - Access and build container images  </li> <li>API Guide \u2192 - Use programmatic access</li> <li>Help &amp; Support \u2192 - Get assistance with user management</li> </ul> <p>Security Reminder</p> <p>Never share your CADC password or authentication tokens. Use group-based permissions for collaboration, and regularly review access permissions for sensitive data.</p>"},{"location":"platform/batch-jobs/","title":"Batch Jobs and Headless Processing","text":"<p>Batch jobs enable automated, non-interactive processing of astronomical data at scale. This section covers headless execution, API access, job scheduling, and workflow automation on the CANFAR Science Platform.</p> <p>\ud83c\udfaf What You'll Learn</p> <ul> <li>The difference between headless and interactive containers</li> <li>How to submit jobs via API and Python client</li> <li>Resource planning, queue behavior, and monitoring</li> <li>Best practices for automation, logging, and data management</li> </ul>"},{"location":"platform/batch-jobs/#what-is-batch-processing","title":"What is Batch Processing?","text":"<p>Batch processing refers to the execution of computational tasks without user interaction, typically running in the background to process large datasets or perform repetitive analysis tasks. In the context of the CANFAR Science Platform, batch jobs run as headless containers - containerized environments that execute your code without graphical interfaces or interactive terminals.</p>"},{"location":"platform/batch-jobs/#headless-vs-interactive-containers","title":"Headless vs Interactive Containers","text":"<p>The key difference between headless and interactive containers lies not in the container images themselves, but in how they are executed. The same container image can be launched in either mode depending on your needs.</p> <p>Headless containers execute a user-specified command or script directly. When you submit a headless job, you specify exactly what command should run - whether it's a Python script, a shell command, or any executable available in the container. The container starts, runs your specified command, and terminates when the command completes. For example, submitting a headless job with the <code>astroml</code> container might execute <code>python /arc/projects/myproject/analysis.py</code> directly.</p> <p>Interactive containers launch predefined interactive services that you can access through your web browser. The same <code>astroml</code> container, when launched interactively, would start Jupyter Lab, providing you with a notebook interface for development and exploration. These containers run indefinitely until you manually stop them, allowing for real-time interaction and iterative development.</p> <p>This distinction makes headless containers ideal for production workflows and automated processing, while interactive containers excel for development, prototyping, and exploratory data analysis.</p>"},{"location":"platform/batch-jobs/#overview","title":"Overview","text":"<p>Batch processing is essential for:</p> <ul> <li>Large dataset processing: Handle terabytes of astronomical data</li> <li>Automated pipelines: Run standardized reduction workflows</li> <li>Parameter studies: Execute multiple analysis runs with different parameters</li> <li>Resource optimization: Run during off-peak hours for better performance</li> <li>Reproducible science: Documented, automated workflows</li> </ul> <p>When to Use Batch Jobs</p> <ul> <li>Use interactive sessions to develop and test</li> <li>Switch to headless jobs for production-scale runs</li> <li>Schedule jobs during off-peak hours for faster starts</li> </ul>"},{"location":"platform/batch-jobs/#batch-processing-methods","title":"Batch Processing Methods","text":""},{"location":"platform/batch-jobs/#1-api-based-execution","title":"1. API-Based Execution","text":"<p>Execute containers programmatically using the CANFAR API:</p> <pre><code># Submit a job via API\ncurl -X POST \"https://ws-uv.canfar.net/skaha/v0/session\" \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/x-www-form-urlencoded\" \\\n  -d \"name=data-reduction-job\" \\\n  -d \"image=images.canfar.net/skaha/astroml:latest\" \\\n  -d \"cores=4\" \\\n  -d \"ram=16\" \\\n  -d \"cmd=python /arc/projects/myproject/scripts/reduce_data.py\"\n</code></pre>"},{"location":"platform/batch-jobs/#2-job-submission-scripts","title":"2. Job Submission Scripts","text":"<p>Create shell scripts for common workflows using the API or Python client:</p> <pre><code>#!/bin/bash\n# submit_reduction.sh - API-based job submission\n\n# Set job parameters\nJOB_NAME=\"nightly-reduction-$(date +%Y%m%d)\"\nIMAGE=\"images.canfar.net/skaha/casa:6.5\"\nCMD=\"python /arc/projects/survey/pipelines/reduce_night.py /arc/projects/survey/data/$(date +%Y%m%d)\"\n\n# Submit job via API\ncurl -X POST \"https://ws-uv.canfar.net/skaha/v0/session\" \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/x-www-form-urlencoded\" \\\n  -d \"name=$JOB_NAME\" \\\n  -d \"image=$IMAGE\" \\\n  -d \"cores=8\" \\\n  -d \"ram=32\" \\\n  -d \"cmd=$CMD\"\n</code></pre> <p>Or using the Python skaha client:</p> <pre><code>#!/usr/bin/env python3\n# submit_reduction.py - Python client-based submission\n\nfrom skaha.session import Session\nfrom datetime import datetime\n\n# Initialize session manager\nsession = Session()\n\n# Set job parameters\njob_name = f\"nightly-reduction-{datetime.now().strftime('%Y%m%d')}\"\nimage = \"images.canfar.net/skaha/casa:6.5\"\ndata_path = f\"/arc/projects/survey/data/{datetime.now().strftime('%Y%m%d')}\"\n\n# Submit job\njob_id = session.create(\n    name=job_name,\n    image=image,\n    cores=8,\n    ram=32,\n    cmd=\"python\",\n    args=[\"/arc/projects/survey/pipelines/reduce_night.py\", data_path]\n)\n\nprint(f\"Submitted job: {job_id}\")\n</code></pre>"},{"location":"platform/batch-jobs/#3-workflow-automation","title":"3. Workflow Automation","text":"<p>Use workflow managers like Prefect or Snakemake:</p> <pre><code># Snakemake example: workflow.smk\nrule all:\n    input:\n        \"results/final_catalog.fits\"\n\nrule calibrate:\n    input:\n        \"data/raw/{observation}.fits\"\n    output:\n        \"data/calibrated/{observation}.fits\"\n    shell:\n        \"python scripts/calibrate.py {input} {output}\"\n\nrule source_extract:\n    input:\n        \"data/calibrated/{observation}.fits\"\n    output:\n        \"catalogs/{observation}_sources.fits\"\n    shell:\n        \"python scripts/extract_sources.py {input} {output}\"\n</code></pre>"},{"location":"platform/batch-jobs/#job-types-and-use-cases","title":"Job Types and Use Cases","text":""},{"location":"platform/batch-jobs/#data-reduction-pipelines","title":"Data Reduction Pipelines","text":"<p>Optical/IR Surveys: - Bias, dark, and flat field correction - Astrometric calibration - Photometric calibration - Source extraction and cataloging</p> <p>Radio Astronomy: - Flagging and calibration - Imaging and deconvolution - Spectral line analysis - Polarization processing</p>"},{"location":"platform/batch-jobs/#scientific-analysis","title":"Scientific Analysis","text":"<p>Large-scale Surveys: - Cross-matching catalogs - Statistical analysis - Machine learning training - Population studies</p> <p>Time-domain Astronomy: - Light curve analysis - Period finding - Variability classification - Transient detection</p>"},{"location":"platform/batch-jobs/#simulation-and-modeling","title":"Simulation and Modeling","text":"<p>N-body Simulations: - Galaxy formation models - Stellar dynamics - Dark matter simulations</p> <p>Synthetic Observations: - Mock catalog generation - Instrument simulation - Survey planning</p>"},{"location":"platform/batch-jobs/#resource-planning","title":"Resource Planning","text":""},{"location":"platform/batch-jobs/#job-sizing","title":"Job Sizing","text":"<p>Choose appropriate resources based on your workload:</p> Job Type Cores Memory Storage Duration Single image reduction 1-2 4-8GB 10GB 5-30 min Survey night processing 4-8 16-32GB 100GB 1-4 hours Catalog cross-matching 2-4 8-16GB 50GB 30min-2hr ML model training 8-16 32-64GB 200GB 4-24 hours Large simulations 16-32 64-128GB 1TB Days-weeks <p>Queue Behavior</p> <p>Small jobs (\u22644 cores, \u226416GB) start faster. Large jobs (\u226516 cores, \u226564GB) may queue longer. Off-peak hours often improve start times.</p>"},{"location":"platform/batch-jobs/#queue-management","title":"Queue Management","text":"<p>Understand job priorities and scheduling:</p> <ul> <li>Small jobs (&lt;4 cores, &lt;16GB): Higher priority, faster start</li> <li>Large jobs (16+ cores, 64GB+): Lower priority, may queue longer</li> <li>Off-peak hours: Better resource availability (evenings, weekends)</li> <li>Resource limits: Per-user and per-group limits apply</li> </ul>"},{"location":"platform/batch-jobs/#api-access","title":"API Reference","text":""},{"location":"platform/batch-jobs/#method-1-direct-curl-commands","title":"Method 1: Direct curl Commands","text":""},{"location":"platform/batch-jobs/#submit-job","title":"Submit Job","text":"<pre><code>curl -X POST \"https://ws-uv.canfar.net/skaha/v0/session\" \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/x-www-form-urlencoded\" \\\n  -d \"name=my-analysis-job\" \\\n  -d \"image=images.canfar.net/skaha/astroml:latest\" \\\n  -d \"cores=4\" \\\n  -d \"ram=16\" \\\n  -d \"cmd=python /arc/projects/myproject/analysis.py\"\n</code></pre>"},{"location":"platform/batch-jobs/#list-jobs","title":"List Jobs","text":"<pre><code>curl -X GET \"https://ws-uv.canfar.net/skaha/v0/session\" \\\n  -H \"Authorization: Bearer $TOKEN\"\n</code></pre>"},{"location":"platform/batch-jobs/#get-job-status","title":"Get Job Status","text":"<pre><code>curl -X GET \"https://ws-uv.canfar.net/skaha/v0/session/{session-id}\" \\\n  -H \"Authorization: Bearer $TOKEN\"\n</code></pre>"},{"location":"platform/batch-jobs/#cancel-job","title":"Cancel Job","text":"<pre><code>curl -X DELETE \"https://ws-uv.canfar.net/skaha/v0/session/{session-id}\" \\\n  -H \"Authorization: Bearer $TOKEN\"\n</code></pre>"},{"location":"platform/batch-jobs/#get-job-logs","title":"Get Job Logs","text":"<pre><code>curl -X GET \"https://ws-uv.canfar.net/skaha/v0/session/{session-id}/logs\" \\\n  -H \"Authorization: Bearer $TOKEN\"\n</code></pre>"},{"location":"platform/batch-jobs/#get-resource-usage","title":"Get Resource Usage","text":"<pre><code>curl -X GET \"https://ws-uv.canfar.net/skaha/v0/session/{session-id}/stats\" \\\n  -H \"Authorization: Bearer $TOKEN\" | jq .\n</code></pre>"},{"location":"platform/batch-jobs/#method-2-python-skaha-client","title":"Method 2: Python skaha Client","text":"<p>The skaha Python client provides a more convenient interface for batch job management and automation.</p>"},{"location":"platform/batch-jobs/#installation","title":"Installation","text":"<pre><code>mamba activate base\npip install skaha\n</code></pre>"},{"location":"platform/batch-jobs/#basic-python-client-usage","title":"Basic Python Client Usage","text":"<pre><code>from skaha.session import Session\nfrom skaha.image import Image\nimport time\n\n# Initialize session manager\nsession = Session()\n\n# Simple job submission\njob_id = session.create(\n    name=\"python-analysis\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    kind=\"headless\",\n    cmd=\"python\",\n    args=[\"/arc/projects/myproject/analysis.py\"]\n)\n\nprint(f\"Submitted job: {job_id}\")\n</code></pre>"},{"location":"platform/batch-jobs/#advanced-job-submission","title":"Advanced Job Submission","text":"<pre><code># Job with custom resources and environment\njob_id = session.create(\n    name=\"heavy-computation\",\n    image=\"images.canfar.net/myproject/processor:latest\", \n    kind=\"headless\",\n    cores=8,\n    ram=32,\n    cmd=\"/opt/scripts/heavy_process.sh\",\n    args=[\"/arc/projects/data/large_dataset.h5\", \"/arc/projects/results/\"],\n    env={\n        \"PROCESSING_THREADS\": \"8\",\n        \"OUTPUT_FORMAT\": \"hdf5\",\n        \"VERBOSE\": \"true\"\n    }\n)\n</code></pre>"},{"location":"platform/batch-jobs/#private-image-authentication","title":"Private Image Authentication","text":"<pre><code># For private images, set registry authentication\nimport base64\n\nusername = \"username\"\ncli_secret = \"************\"\nauth_string = base64.b64encode(f\"{username}:{cli_secret}\".encode()).decode()\n\njob_id = session.create(\n    name=\"private-image-job\",\n    image=\"images.canfar.net/myproject/private:latest\",\n    kind=\"headless\",\n    cmd=\"python /opt/analysis.py\",\n    registry_auth=auth_string\n)\n</code></pre> <p>Private Images</p> <p>For private Harbor projects, ensure your CLI credentials are valid and your account has access to the project repository.</p>"},{"location":"platform/batch-jobs/#job-monitoring-and-management","title":"Job Monitoring and Management","text":"<pre><code># List all your sessions\nsessions = session.fetch()\nprint(f\"Active sessions: {len(sessions)}\")\n\n# Get session details\nsession_info = session.fetch(job_id)\nprint(f\"Status: {session_info['status']}\")\nprint(f\"Start time: {session_info['startTime']}\")\n\n# Wait for completion\nwhile True:\n    status = session.fetch(job_id)['status']\n    if status in ['Succeeded', 'Failed', 'Terminated']:\n        print(f\"Job completed with status: {status}\")\n        break\n    time.sleep(30)\n\n# Get logs\nlogs = session.logs(job_id)\nprint(\"Job output:\")\nprint(logs)\n\n# Clean up\nsession.delete(job_id)\n</code></pre>"},{"location":"platform/batch-jobs/#bulk-job-management","title":"Bulk Job Management","text":"<pre><code># Submit multiple related jobs\njob_ids = []\nfor i in range(10):\n    job_id = session.create(\n        name=f\"parameter-study-{i}\",\n        image=\"images.canfar.net/skaha/astroml:latest\",\n        kind=\"headless\",\n        cmd=\"python\",\n        args=[\"/arc/projects/study/analyze.py\", f\"--param={i}\"]\n    )\n    job_ids.append(job_id)\n    print(f\"Submitted job {i}: {job_id}\")\n\n# Monitor all jobs\ncompleted = 0\nwhile completed &lt; len(job_ids):\n    completed = 0\n    for job_id in job_ids:\n        status = session.fetch(job_id)['status']\n        if status in ['Succeeded', 'Failed', 'Terminated']:\n            completed += 1\n\n    print(f\"Completed: {completed}/{len(job_ids)}\")\n    if completed &lt; len(job_ids):\n        time.sleep(60)\n\nprint(\"All jobs completed!\")\n</code></pre>"},{"location":"platform/batch-jobs/#method-3-higher-level-python-api","title":"Method 3: Higher-Level Python API","text":"<pre><code># Example: Higher-level API for common tasks\nfrom skaha import Session\n\n# Create a session object\nsession = Session()\n\n# Submit a data reduction job\njob_id = session.submit(\n    name=\"data-reduction\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    cmd=\"python /arc/projects/myproject/scripts/reduce_data.py\",\n    cores=4,\n    ram=16\n)\n\n# Monitor the job\nsession.monitor(job_id)\n\n# Fetch and print logs\nlogs = session.logs(job_id)\nprint(logs)\n\n# Delete the job after completion\nsession.delete(job_id)\n</code></pre>"},{"location":"platform/batch-jobs/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"platform/batch-jobs/#log-analysis","title":"Log Analysis","text":"<p>Monitor job progress through logs:</p> <pre><code># Real-time log monitoring\ncurl -s \"https://ws-uv.canfar.net/skaha/v0/session/$SESSION_ID/logs\" \\\n  -H \"Authorization: Bearer $TOKEN\" | tail -f\n\n# Search for errors\ncurl -s \"https://ws-uv.canfar.net/skaha/v0/session/$SESSION_ID/logs\" \\\n  -H \"Authorization: Bearer $TOKEN\" | grep -i error\n</code></pre>"},{"location":"platform/batch-jobs/#resource-monitoring","title":"Resource Monitoring","text":"<p>Track resource usage:</p> <pre><code># Get session statistics\ncurl -s \"https://ws-uv.canfar.net/skaha/v0/session/$SESSION_ID/stats\" \\\n  -H \"Authorization: Bearer $TOKEN\" | jq .\n</code></pre>"},{"location":"platform/batch-jobs/#common-issues","title":"Common Issues","text":"<p>Job fails to start: - Check resource availability - Verify container image exists - Check command syntax</p> <p>Job crashes: - Review logs for error messages - Check memory usage patterns - Verify input file accessibility</p> <p>Job hangs: - Monitor CPU usage - Check for infinite loops - Verify network connectivity</p>"},{"location":"platform/batch-jobs/#best-practices","title":"Best Practices","text":""},{"location":"platform/batch-jobs/#script-design","title":"Script Design","text":"<ul> <li>Error handling: Use try-catch blocks and meaningful error messages</li> <li>Logging: Include progress indicators and debugging information</li> <li>Checkpointing: Save intermediate results for long-running jobs</li> <li>Resource monitoring: Track memory and CPU usage</li> </ul>"},{"location":"platform/batch-jobs/#data-management","title":"Data Management","text":"<ul> <li>Input validation: Check file existence and format before processing</li> <li>Output organization: Use consistent naming and directory structures</li> <li>Cleanup: Remove temporary files to save storage</li> <li>Metadata: Include processing parameters in output headers</li> </ul> <p>Persistence Reminder</p> <p>Headless containers do not persist changes to the container filesystem. Always write outputs to <code>/arc/projects/</code> or <code>/arc/home/</code>.</p>"},{"location":"platform/batch-jobs/#security-and-efficiency","title":"Security and Efficiency","text":"<ul> <li>Token management: Use secure token storage and rotation</li> <li>Resource limits: Don't request more resources than needed</li> <li>Parallel processing: Use appropriate parallelization strategies</li> <li>Cost optimization: Run large jobs during off-peak hours</li> </ul>"},{"location":"platform/batch-jobs/#getting-help","title":"Getting Help","text":"<ul> <li>API Documentation: CANFAR API Reference</li> <li>Support: Email support@canfar.net</li> <li>Community: Join our Discord for batch processing discussions</li> <li>Examples: Check the CANFAR GitHub for more examples</li> </ul>"},{"location":"platform/batch-jobs/#next-steps","title":"Next Steps","text":"<ul> <li>Container Development: Build custom containers for your workflows</li> <li>Storage Optimization: Efficient data management strategies</li> <li>Interactive Sessions: Develop and test scripts interactively</li> <li>Radio Astronomy Workflows: Specialized batch processing for radio data</li> </ul>"},{"location":"platform/concepts/","title":"CANFAR Platform Concepts","text":"<p>Understanding the architecture and core concepts behind the CANFAR Science Platform</p> <p>This section covers the fundamental concepts you need to understand to effectively use CANFAR. Whether you are a student starting your first analysis or a project manager setting up a team workspace, these concepts will help you understand how the platform works.</p> <p>\ud83c\udfaf What You'll Learn</p> <p>By the end of this guide, you'll understand:</p> <ul> <li>How CANFAR's cloud architecture works</li> <li>The role of containers in your research workflow</li> <li>How sessions and storage systems interact</li> <li>When to use different platform features</li> </ul>"},{"location":"platform/concepts/#what-is-canfar","title":"\ud83d\ude80 What is CANFAR?","text":"<p>The Canadian Advanced Network for Astronomy Research (CANFAR) Science Platform is a cloud-based computing environment designed specifically for astronomical research. It provides:</p> <ul> <li>On-demand computing resources without needing your own servers</li> <li>Pre-built software environments with astronomy packages ready to use</li> <li>Shared storage systems for collaborative research</li> <li>Scalable infrastructure that grows with your project needs</li> </ul> <p>Key Benefit</p> <p>CANFAR eliminates the traditional barriers of software installation, hardware management, and infrastructure setup, letting you focus entirely on your research.</p>"},{"location":"platform/concepts/#who-benefits-from-canfar","title":"Who Benefits from CANFAR?","text":"Individual ResearchersResearch TeamsLarge Projects <ul> <li>No software installation headaches - pre-configured containers ready to use</li> <li>Access powerful computing resources without owning hardware</li> <li>Work from anywhere with just a web browser</li> <li>Automatic backups and data protection</li> </ul> <ul> <li>Share data and analysis environments seamlessly</li> <li>Standardized software stacks across the team</li> <li>Collaborative workspaces and session sharing</li> <li>Centralized project management</li> </ul> <ul> <li>Scale computing resources up or down as needed</li> <li>Batch processing for large datasets</li> <li>Custom software environments for specialized workflows</li> <li>Integration with astronomy data archives</li> </ul>"},{"location":"platform/concepts/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>CANFAR is built on modern cloud-native technologies designed for scalability and reliability. Here's how the components work together:</p> <pre><code>%%{init: {'flowchart': {'curve': 'linear'}}}%%\ngraph LR\n    %% User Entry Point\n    User[\"\ud83d\udc64 You\"]:::user\n\n    %% Portal Layer\n    Portal[\"\ud83c\udf10 Science Portal&lt;br/&gt;canfar.net\"]:::portal\n    Auth[\"\ud83d\udd10 CADC Authentication\"]:::auth\n    Sessions[\"\ud83d\udda5\ufe0f Session Manager&lt;br/&gt;Skaha\"]:::sessions\n\n    %% Infrastructure Layer\n    K8s[\"\u2638\ufe0f Kubernetes Cluster\"]:::k8s\n    Containers[\"\ud83d\udc33 Container Images&lt;br/&gt;Harbor Registry\"]:::containers\n    Storage[\"\ud83d\udcbe Storage Systems\"]:::storage\n\n    %% Storage Systems\n    arc[\"\ud83d\udcc1 arc Posix Storage&lt;br/&gt;Shared Filesystem\"]:::arc\n    VOSpace[\"\u2601\ufe0f VOSpace Object Store&lt;br/&gt;Long-term Storage\"]:::vospace\n    Scratch[\"\u26a1 Scratch&lt;br/&gt;Temporary SSDs\"]:::scratch\n\n    %% Session Types\n    Types[\"Session Types\"]:::types\n    Notebook[\"\ud83d\udcd3 Jupyter Notebooks\"]:::notebooks\n    Desktop[\"\ud83d\udda5\ufe0f Desktop Environment\"]:::desktop\n    CARTA[\"\ud83d\udcca CARTA Viewer\"]:::carta\n    Firefly[\"\ud83d\udd25 Firefly Viewer\"]:::firefly\n    Contrib[\"\u2699\ufe0f Contributed Apps\"]:::contrib\n    Batch[\"\ud83c\udfed Batch Jobs\"]:::batch\n\n    %% Connections\n    User --&gt; Portal\n    Portal --&gt; Auth\n    Portal --&gt; Sessions\n\n    Auth --&gt; K8s\n    Sessions --&gt; K8s\n\n    K8s --&gt; Containers\n    K8s --&gt; Storage\n\n    Storage --&gt; arc\n    Storage --&gt; VOSpace\n    Storage --&gt; Scratch\n\n    Sessions --&gt; Types\n    Types --&gt; Notebook\n    Types --&gt; Desktop\n    Types --&gt; CARTA\n    Types --&gt; Firefly\n    Types --&gt; Contrib\n    Types --&gt; Batch\n\n    %% Styling\n    classDef user fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    classDef portal fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    classDef auth fill:#ffebee,stroke:#c62828,stroke-width:2px,color:#000\n    classDef sessions fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#000\n    classDef k8s fill:#fff3e0,stroke:#ef6c00,stroke-width:3px,color:#000\n    classDef containers fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    classDef storage fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000\n    classDef arc fill:#fce4ec,stroke:#ad1457,stroke-width:2px,color:#000\n    classDef vospace fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px,color:#000\n    classDef scratch fill:#fff8e1,stroke:#f57f17,stroke-width:2px,color:#000\n    classDef types fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    classDef notebooks fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    classDef desktop fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000\n    classDef carta fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    classDef firefly fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#000\n    classDef contrib fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000\n    classDef batch fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000</code></pre> <p>Architecture Key Points</p> <ul> <li>Science Portal: Your web interface - no software installation required</li> <li>Kubernetes: Manages your computing requirements automatically</li> <li>Containers: Pre-built software environments with astronomy tools</li> <li>Storage Systems: Multiple types optimized for different use cases</li> <li>Authentication: Secure access via CADC integration</li> </ul>"},{"location":"platform/concepts/#containers","title":"\ud83d\udc33 Containers","text":"<p>Containers are at the heart of CANFAR's flexibility and power. Think of them as complete, portable software environments that include everything needed to run specific applications.</p> <p>Important Distinction</p> <p>Unlike virtual machines that include entire operating systems, containers share the host's kernel and only package the application and its dependencies. This makes them faster, more efficient, and easier to distribute.</p>"},{"location":"platform/concepts/#why-containers-matter-for-astronomy","title":"Why Containers Matter for Astronomy","text":"<p>Traditional Software Installation:</p> <ul> <li>Struggle with dependencies and conflicting versions</li> <li>Missing libraries and system requirements</li> <li>Different behavior across different machines</li> <li>Time-consuming setup and configuration</li> </ul> <p>CANFAR Containers:</p> <ul> <li>Consistent environment that works the same everywhere</li> <li>Pre-configured with astronomy packages</li> <li>No installation headaches</li> <li>Easy to share and reproduce results</li> </ul> <p>Research Reproducibility</p> <p>Containers ensure your analysis runs the same way for you, your collaborators, and future researchers. This is crucial for reproducible science.</p>"},{"location":"platform/concepts/#popular-canfar-containers","title":"Popular CANFAR Containers","text":"Container Purpose Best For astroml General astronomy analysis Python, NumPy, SciPy, Astropy, Matplotlib casa Radio interferometry CASA software, Python, astronomy tools desktop GUI applications Full Ubuntu desktop, Firefox, terminal carta Radio astronomy visualization CARTA viewer, analysis tools notebook Interactive computing JupyterLab, Python scientific stack <p>Getting Started</p> <p>Start with the astroml container for general astronomy work. It includes most common packages and is regularly updated.</p>"},{"location":"platform/concepts/#container-lifecycle","title":"Container Lifecycle","text":"<p>When you launch a session, here's what happens behind the scenes:</p> <ol> <li>Request: You choose a container type in the Science Portal</li> <li>Download: Kubernetes pulls the container image (first time: 2-3 minutes)</li> <li>Launch: Container starts with your storage connected</li> <li>Work: You use the pre-configured environment</li> <li>Cleanup: Container is destroyed when session ends (files persist in storage)</li> </ol> <p>Performance Note</p> <p>Subsequent launches of the same container are much faster (30-60 seconds) since the image is cached locally.</p>"},{"location":"platform/concepts/#sessions-and-computing-resources","title":"\u2638\ufe0f Sessions and Computing Resources","text":"<p>CANFAR uses Kubernetes to manage your computing sessions. You don't need to understand Kubernetes deeply, but here are the key concepts:</p> <p>Session Fundamentals</p> <ul> <li>Temporary: Each session creates a new container instance</li> <li>Persistent Data: Files persist through storage systems, not containers</li> <li>Resource Limits: CPU, memory, and storage based on your request</li> </ul>"},{"location":"platform/concepts/#session-types","title":"Session Types","text":"<p>Different session types provide different interfaces to the same underlying computing resources:</p> \ud83d\udcd3 Notebook Sessions\ud83d\udda5\ufe0f Desktop Sessions\ud83d\udcca CARTA Sessions\ud83d\udd25 Firefly Sessions\u2699\ufe0f Contributed Sessions <p>JupyterLab Interface for interactive analysis</p> <ul> <li>Perfect for data exploration and visualization</li> <li>Python, R, and other kernels available</li> <li>Rich text, code, and visualization in one interface</li> </ul> <p>Full Linux desktop environment for GUI applications</p> <ul> <li>CASA, DS9, and image viewers</li> <li>Traditional desktop workflow</li> <li>Multiple applications running simultaneously</li> </ul> <p>Specialized for radio astronomy visualization and analysis</p> <ul> <li>CARTA viewer for FITS files</li> <li>Radio astronomy workflows</li> <li>Interactive data exploration</li> </ul> <p>Table and image visualization tools</p> <ul> <li>Astronomical table viewing</li> <li>Image display and analysis</li> <li>Web-based interface</li> </ul> <p>Custom applications contributed by the community</p> <ul> <li>Specialized tools and workflows</li> <li>Community-maintained software</li> <li>Experimental features</li> </ul>"},{"location":"platform/concepts/#storage-systems","title":"\ud83d\udcbe Storage Systems","text":""},{"location":"platform/concepts/#data-persistence-rules","title":"Data Persistence Rules","text":"<p>CANFAR provides multiple storage systems optimized for different use cases:</p> <p>Critical: Where Your Files Are Saved</p> <p>Understanding where your files persist is crucial for not losing work:</p> Location Persistence Best For <code>/arc/projects/yourgroup/</code> \u2705 Permanent, backed up Datasets, results, shared code <code>/arc/home/yourusername/</code> \u2705 Permanent, backed up Personal configs, small files <code>/scratch/</code> \u274c Wiped at session end Large computations, temporary files <code>/tmp/</code> \u274c Lost when session ends Temporary processing only"},{"location":"platform/concepts/#arc-storage-arc","title":"ARC Storage (<code>/arc/</code>)","text":"<p>High-performance POSIX file system for active research:</p> <ul> <li>Speed: Fast, direct access for large computations</li> <li>Sharing: Group-based access control</li> <li>Backup: Daily snapshots</li> <li>Best For: Active analysis, large datasets, collaborative work</li> </ul>"},{"location":"platform/concepts/#vospace-vos","title":"VOSpace (<code>vos:</code>)","text":"<p>Web-accessible object store for long-term storage:</p> <ul> <li>IVOA: Based on the International Virtual Observatory Alliance (IVOA) standard</li> <li>Access: Web APIs and command-line tools</li> <li>Metadata: Astronomical metadata support</li> <li>Versioning: Track changes to datasets</li> <li>Best For: Archives, sharing, backups, metadata-rich data</li> </ul> <p>Storage Strategy</p> <p>Use ARC storage for active analysis and VOSpace for long-term archival and sharing.</p>"},{"location":"platform/concepts/#storage-comparison","title":"Storage Comparison","text":"Feature ARC Storage (<code>/arc/</code>) VOSpace (<code>vos:</code>) Access Method POSIX file system Web APIs, command tools Speed Fast (direct access) Medium (network-based) Best For Active analysis, large computations Archives, sharing, backups Quota Group-based User/project based Backup Daily snapshots Geo-redundant"},{"location":"platform/concepts/#programmatic-access","title":"\ud83c\udf10 Programmatic Access","text":"<p>CANFAR provides REST APIs for programmatic access, allowing you to:</p> <ul> <li>Launch and manage sessions from scripts</li> <li>Transfer files programmatically</li> <li>Integrate CANFAR into automated workflows</li> <li>Build custom applications using CANFAR resources</li> </ul>"},{"location":"platform/concepts/#key-api-endpoints","title":"Key API Endpoints","text":"Service Purpose Documentation CANFAR Python Client Session management <code>canfar</code> VOSpace File operations VOSpace API Access Control Authentication and Authorization CADC Services <p>Advanced Users</p> <p>The REST APIs enable automation and integration with external tools and workflows.</p>"},{"location":"platform/concepts/#whats-next","title":"\ud83d\udd17 What's Next?","text":"<p>Now that you understand the core concepts, dive into specific areas:</p> <ul> <li>Accounts &amp; Permissions \u2192 - Manage users and access</li> <li>Storage Systems \u2192 - Master data management</li> <li>Container Usage \u2192 - Work with software environments</li> <li>Interactive Sessions \u2192 - Start analyzing data</li> </ul> <p>Key Takeaway</p> <p>CANFAR provides the computing power of a research institution without the infrastructure overhead. Focus on your science - let CANFAR handle the computers, software, and data management.</p>"},{"location":"platform/containers/","title":"Containers","text":"<p>Working with astronomy software containers on CANFAR</p> <p>\ud83c\udfaf What You'll Learn</p> <ul> <li>What containers are and how they run on CANFAR</li> <li>How container types map to session types (Notebook, Desktop, Headless)</li> <li>How to choose and use existing containers effectively</li> <li>How containers are categorized (CANFAR-supported, community, team)</li> <li>How to build, manage, version, and distribute custom containers</li> <li>How containers integrate with CANFAR storage and workflows</li> </ul> <p>Containers provide pre-packaged software environments that include everything needed to run astronomy applications. On CANFAR, containers eliminate the \"works on my machine\" problem by ensuring consistent, reproducible computational environments across different sessions and workflows.</p> <p>Key Concept: Reproducible Environments</p> <p>Containers provide consistent, reproducible software environments for astronomy work across sessions and teams.</p>"},{"location":"platform/containers/#understanding-canfar-containers","title":"Understanding CANFAR Containers","text":"<p>Think of containers as complete software packages that bundle an operating system (typically Ubuntu Linux), astronomy software like CASA or Python packages, programming tools, system libraries, and environment configuration into a single portable unit. When you launch a session on CANFAR, you're essentially starting up one of these pre-configured environments with your data and home directory automatically mounted and accessible.</p> <p>The container ecosystem on CANFAR follows a layered approach. Base containers provide fundamental tools and the conda package manager, while specialized containers build upon these foundations to offer domain-specific software stacks. This architecture ensures consistency while allowing flexibility for different research needs.</p>"},{"location":"platform/containers/#runtime-environment","title":"Runtime Environment","text":"<p>How Containers Run on CANFAR</p> <ul> <li>Containers run as your CADC user (not root)</li> <li><code>/arc/home/[username]</code> is the container's home directory</li> <li>Project directories under <code>/arc/projects/</code> are mounted and accessible</li> <li><code>/scratch/</code> provides high-speed temporary storage</li> </ul> <pre><code># Inside a running container, check your environment\necho $USER                      # Your CADC username\necho $HOME                      # /arc/home/[username]\nls /arc/projects/               # Available project directories\nls /scratch/                    # Temporary high-speed storage\n</code></pre> <p>This runtime setup means there's an important compatibility consideration between code packaged in the container image and code stored on the <code>/arc</code> filesystem. Best practice involves keeping stable, tested code within the container image while placing development scripts and analysis notebooks in your <code>/arc/home</code> or project directories where they can be easily modified and version controlled.</p> <p>Persistence Reminder</p> <p>Software installed inside a running container (e.g., <code>pip install --user</code>) is temporary and lost when the session ends. Keep stable software in the image; keep notebooks/scripts on <code>/arc</code>.</p>"},{"location":"platform/containers/#container-types-and-session-integration","title":"Container Types and Session Integration","text":"<p>CANFAR containers are designed to work with different session types, each optimized for specific workflows and interaction patterns.</p>"},{"location":"platform/containers/#notebook-containers","title":"Notebook Containers","text":"<p>Notebook containers provide interactive Jupyter environments accessed through your web browser. These containers must include Jupyter Lab and are optimized for data analysis, visualization, and interactive computing. The astroml container exemplifies this type, offering a comprehensive Python astronomy stack with popular packages like Astropy, SciPy, and scikit-learn.</p> <pre><code># Example notebook session - check available packages\nimport astropy\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom astroquery import vizier\n\nprint(f\"Astropy version: {astropy.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\n\n# Access your data\nimport os\n\ndata_path = f\"/arc/projects/{os.environ.get('PROJECT_NAME', 'myproject')}\"\nprint(f\"Project data at: {data_path}\")\n</code></pre> <p>For GPU-accelerated computing, astroml-cuda includes the CUDA toolkit alongside the standard astronomy libraries, enabling machine learning and image processing workflows that leverage GPU acceleration.</p> <pre><code># Check GPU availability in astroml-cuda container\nimport torch\nimport tensorflow as tf\n\nprint(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n\nprint(f\"TensorFlow GPU devices: {len(tf.config.list_physical_devices('GPU'))}\")\n</code></pre>"},{"location":"platform/containers/#desktop-app-containers","title":"Desktop-App Containers","text":"<p>While CANFAR maintains the base desktop container that provides the Ubuntu desktop environment, users can create desktop-app containers that package specific GUI applications to run within desktop sessions. These containers focus on single applications or related tool suites rather than providing complete desktop environments.</p> <pre><code># Example desktop-app container for DS9 image viewer\nFROM ubuntu:22.04\n\n# Install DS9 and dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    saods9 \\\n    x11-apps \\\n    libx11-6 \\\n    libxft2 \\\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Create startup script\nRUN echo '#!/bin/bash\\nds9 \"$@\"' &gt; /usr/local/bin/start-ds9 &amp;&amp; \\\n    chmod +x /usr/local/bin/start-ds9\n\n# Default command for desktop session\nCMD [\"/usr/local/bin/start-ds9\"]\n</code></pre> <p>Desktop-app containers are particularly useful for legacy astronomy software, specialized visualization tools, or applications that require specific library versions or configurations. When launched in a desktop session, these applications integrate seamlessly with the desktop environment while maintaining their isolated software dependencies.</p>"},{"location":"platform/containers/#headless-containers","title":"Headless Containers","text":"<p>Headless containers run without graphical interfaces and are designed for batch processing and automated workflows. These containers execute through the batch job system and are optimized for non-interactive processing tasks like data reduction pipelines, large-scale analysis, or scheduled computations.</p> <pre><code># Example headless processing container\nFROM images.canfar.net/skaha/astroml:latest\n\n# Install additional processing tools\nUSER root\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    parallel \\\n    rsync \\\n    &amp;&amp; apt-get clean\n\nUSER ${NB_USER}\n\n# Copy processing scripts\nCOPY scripts/ /opt/processing/\nRUN chmod +x /opt/processing/*.sh\n\n# Default processing command\nCMD [\"/opt/processing/batch_process.sh\"]\n</code></pre>"},{"location":"platform/containers/#working-with-existing-containers","title":"Working with Existing Containers","text":"<p>Most astronomy work on CANFAR can be accomplished using existing containers without requiring custom builds. The astroml container covers the majority of Python-based astronomy analysis needs, while casa handles radio astronomy workflows. For GUI applications, the desktop container provides a complete environment with Firefox, file managers, and terminal access.</p> <p>Choosing the right container depends on your specific workflow requirements. General Python astronomy work benefits from astroml in either notebook or desktop terminal sessions. Radio astronomy tasks requiring CASA tools work best with the casa container in notebook or desktop modes. Legacy GUI applications or IDL-based workflows should use desktop sessions with the desktop container.</p> <pre><code># Launch different session types via API\nTOKEN=$(curl -s https://ws-cadc.canfar.net/ac/login \\\n  -d \"username=myuser\" -d \"password=mypass\" | tr -d '\"')\n\n# Notebook session with astroml\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  -d \"name=analysis-session\" \\\n  -d \"image=images.canfar.net/skaha/astroml:latest\" \\\n  -d \"type=notebook\" \\\n  -d \"cores=2\" -d \"ram=4\" \\\n  https://ws-uv.canfar.net/skaha/v0/session\n\n# Desktop session with CASA\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  -d \"name=casa-desktop\" \\\n  -d \"image=images.canfar.net/skaha/casa:latest\" \\\n  -d \"type=desktop\" \\\n  -d \"cores=4\" -d \"ram=8\" \\\n  https://ws-uv.canfar.net/skaha/v0/session\n</code></pre> <p>Best Practice: Code Placement</p> <p>Keep stable, tested code inside the container image. Keep frequently edited analysis code and notebooks in <code>/arc/home</code> or project directories.</p> <p>Temporary Installs</p> <p>Software installations using <code>pip install --user</code> or <code>apt</code> inside a running container are temporary and will be lost when the session ends.</p>"},{"location":"platform/containers/#container-categories","title":"Container Categories","text":"<p>CANFAR containers fall into three main categories, each serving different purposes and maintained by different groups.</p>"},{"location":"platform/containers/#canfar-supported-containers","title":"CANFAR-Supported Containers","text":"<p>These are officially maintained by the CANFAR team and provide the foundation for most astronomy work. The core offerings include astroml for general astronomy analysis with Python, Astropy, and machine learning libraries; casa for radio interferometry work; notebook for lightweight Jupyter environments; and desktop for full Ubuntu desktop sessions with GUI applications.</p> <pre><code># Browse available CANFAR containers\ncurl -s https://images.canfar.net/api/v2.0/projects/skaha/repositories | \\\n  jq -r '.[] | .name' | sort\n\n# Check container details\ndocker inspect images.canfar.net/skaha/astroml:latest\n</code></pre> <p>For specialized visualization needs, CANFAR provides carta for radio astronomy data visualization and firefly for optical and infrared data exploration. These containers receive regular updates and official support from the CANFAR team.</p>"},{"location":"platform/containers/#community-maintained-containers","title":"Community-Maintained Containers","text":"<p>The astronomy community contributes specialized containers for emerging tools and workflows. Examples include marimo for modern reproducible notebook environments, vscode for browser-based code development, and pluto for interactive Julia computing. These containers are maintained by community members with oversight from CANFAR.</p>"},{"location":"platform/containers/#team-and-individual-containers","title":"Team and Individual Containers","text":"<p>Research groups and individuals can create custom containers for specific projects or workflows. These might include proprietary software, custom analysis pipelines, or specialized configurations needed for particular research programs. While these containers use CANFAR infrastructure, they are maintained by their creators.</p> <pre><code># Example team container structure\nimages.canfar.net/myproject/custom-pipeline:latest\nimages.canfar.net/myteam/analysis-env:v2.1\nimages.canfar.net/user123/specialized-tool:dev\n</code></pre>"},{"location":"platform/containers/#harbor-registry-and-distribution","title":"Harbor Registry and Distribution","text":"<p>The Harbor registry at <code>images.canfar.net</code> serves as the central repository for CANFAR containers. Users can browse available containers, examine metadata and documentation, and access containers for their sessions through the registry interface.</p> <p>Container versioning follows semantic patterns with <code>latest</code> tags for current stable releases, dated tags like <code>2024.03</code> for monthly snapshots, and specific commit hashes for development builds. This versioning strategy supports both reproducible research requiring fixed environments and ongoing development needing current software versions.</p> <p>Access to containers varies by category. CANFAR-supported containers are publicly available to all users. Community-maintained containers may have broader access depending on their purpose and licensing. Team and individual containers can be configured with specific access controls to support proprietary or sensitive work.</p>"},{"location":"platform/containers/#building-custom-containers","title":"Building Custom Containers","text":"<p>Custom container development becomes necessary when existing containers don't meet specific software requirements or when creating standardized environments for research teams. The process involves creating a Dockerfile that defines the software stack, building and testing the container locally, and pushing it to the Harbor registry for use on CANFAR.</p>"},{"location":"platform/containers/#development-workflow","title":"Development Workflow","text":"<p>Successful container development follows an iterative workflow starting with local development and testing. Begin by extending existing CANFAR base images rather than starting from scratch, as this ensures compatibility with the CANFAR runtime environment and includes necessary system configurations.</p> <pre><code># Local development workflow\ngit clone https://github.com/myteam/custom-container.git\ncd custom-container\n\n# Build container locally\ndocker build -t myteam/analysis-env:latest .\n\n# Test locally with mounted data\ndocker run -it --rm \\\n  -v $(pwd)/test-data:/arc/projects/test \\\n  -v $(pwd)/home:/arc/home/testuser \\\n  myteam/analysis-env:latest \\\n  /bin/bash\n</code></pre> <p>Create your Dockerfile starting from an appropriate base image like <code>images.canfar.net/skaha/astroml:latest</code> for astronomy work. Install additional system packages as needed, then switch to the non-root user context for application installations. Python packages should be installed using <code>pip</code> or <code>mamba</code>, while system-level software requires <code>apt</code> or similar package managers during the build process.</p>"},{"location":"platform/containers/#container-architecture-considerations","title":"Container Architecture Considerations","text":"<p>All CANFAR containers run on Linux x86_64 architecture and must support the CANFAR user context system. Containers execute as the user who submitted the job, never as root, though they can be configured with sudo access for specific operations during the build process.</p> <p>The container filesystem integrates with CANFAR storage at runtime. The <code>/arc</code> filesystem containing home and project directories is mounted automatically, providing access to persistent data and development code. Temporary high-speed storage is available under <code>/scratch/</code> for intensive processing tasks.</p> <p>When designing containers, separate stable production code that belongs in the container image from development and analysis code that should reside on the <code>/arc</code> filesystem. This separation enables easier maintenance and allows users to modify analysis scripts without rebuilding containers.</p>"},{"location":"platform/containers/#building-and-testing-process","title":"Building and Testing Process","text":"<p>Local testing ensures containers work correctly before deployment. Build your container using Docker and test core functionality locally. For notebook containers, verify that Jupyter Lab starts correctly and that key Python packages import properly. Desktop-app containers should be tested to ensure the target application launches and functions as expected.</p> <pre><code># Example notebook container extension\nFROM images.canfar.net/skaha/astroml:latest\n\n# Install additional system dependencies\nUSER root\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    gfortran \\\n    libcfitsio-dev \\\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Switch to user context for application installs\nUSER ${NB_USER}\n\n# Install specialized astronomy packages\nRUN pip install --no-cache-dir \\\n    astroplan \\\n    photutils \\\n    reproject\n\n# Set up custom analysis tools\nRUN git clone https://github.com/myteam/analysis-tools.git /tmp/analysis-tools &amp;&amp; \\\n    cd /tmp/analysis-tools &amp;&amp; \\\n    pip install --no-cache-dir -e . &amp;&amp; \\\n    rm -rf /tmp/analysis-tools\n\nWORKDIR ${HOME}\n</code></pre> <pre><code># Example headless processing container\nFROM images.canfar.net/skaha/astroml:latest\n\nUSER root\n\n# Install processing dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    parallel \\\n    imagemagick \\\n    ffmpeg \\\n    &amp;&amp; apt-get clean\n\nUSER ${NB_USER}\n\n# Install Python processing packages\nRUN pip install --no-cache-dir \\\n    dask[complete] \\\n    zarr \\\n    xarray\n\n# Copy processing scripts\nCOPY --chown=${NB_USER}:${NB_GID} scripts/ /opt/processing/\nRUN chmod +x /opt/processing/*.py\n\n# Environment variables for processing\nENV PROCESSING_THREADS=4\nENV OUTPUT_FORMAT=fits\n\n# Default processing entry point\nCMD [\"python\", \"/opt/processing/main.py\"]\n</code></pre> <pre><code># Example desktop-app container for IRAF\nFROM ubuntu:22.04\n\n# Install IRAF and dependencies\nUSER root\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    iraf \\\n    saods9 \\\n    xgterm \\\n    tcsh \\\n    libx11-6 \\\n    libxaw7 \\\n    &amp;&amp; apt-get clean\n\n# Create IRAF user setup\nRUN useradd -m -s /bin/tcsh iraf\nCOPY iraf-setup.cl /home/iraf/\n\n# Setup script for CANFAR desktop session\nRUN echo '#!/bin/bash\\n\\\nexport IRAFARCH=linux64\\n\\\nexport TERM=xgterm\\n\\\ncd /arc/home/$USER\\n\\\nexec xgterm -sb -sl 1000 -j -ls -fn 9x15 -title \"IRAF\" &amp;\\n\\\nexec ds9 &amp;\\n\\\nwait\\n' &gt; /usr/local/bin/start-iraf &amp;&amp; \\\n    chmod +x /usr/local/bin/start-iraf\n\nCMD [\"/usr/local/bin/start-iraf\"]\n</code></pre>"},{"location":"platform/containers/#container-management-and-best-practices","title":"Container Management and Best Practices","text":"<p>Effective container management involves following established patterns for reproducibility, maintainability, and performance. Use specific version tags rather than <code>latest</code> for production workflows to ensure consistent environments over time. Layer Docker commands efficiently to minimize image size and build time.</p> <pre><code># Good layering practices\nFROM images.canfar.net/skaha/astroml:latest\n\n# Combine related operations\nUSER root\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    package1 \\\n    package2 \\\n    package3 \\\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/* /var/tmp/*\n\nUSER ${NB_USER}\n\n# Pin package versions for reproducibility\nRUN pip install --no-cache-dir \\\n    astroplan==0.8 \\\n    photutils==1.8.0 \\\n    reproject==0.10.0\n\n# Use multi-stage builds for complex installations\nFROM images.canfar.net/skaha/astroml:latest as builder\nCOPY source/ /build/\nRUN cd /build &amp;&amp; make install\n\nFROM images.canfar.net/skaha/astroml:latest\nCOPY --from=builder /build/bin/* /usr/local/bin/\n</code></pre> <pre><code># Version management\ndocker tag myproject/tool:latest myproject/tool:v1.2.3\ndocker tag myproject/tool:latest myproject/tool:2024.03\n\n# Automated builds with GitHub Actions\ncat &gt; .github/workflows/build.yml &lt;&lt; 'EOF'\nname: Build Container\non:\n  push:\n    tags: ['v*']\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n    - name: Build and push\n      run: |\n        docker build -t images.canfar.net/myproject/tool:${{ github.ref_name }} .\n        docker login images.canfar.net -u ${{ secrets.HARBOR_USER }} -p ${{ secrets.HARBOR_TOKEN }}\n        docker push images.canfar.net/myproject/tool:${{ github.ref_name }}\nEOF\n</code></pre> <p>Keep containers focused on their primary purpose rather than creating monolithic images that attempt to solve every possible use case. This approach makes containers easier to maintain and debug while providing clearer upgrade paths.</p> <p>For team containers, establish clear documentation including purpose, usage examples, and maintenance responsibilities. Version containers systematically and maintain compatibility with CANFAR's evolving infrastructure through regular updates and testing.</p> <pre><code># Container documentation template (README.md)\n# Custom Astronomy Analysis Container\n\n## Purpose\nThis container provides a specialized environment for X-ray astronomy analysis.\n\n## Usage\n```bash\n# Launch notebook session\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  -d \"image=images.canfar.net/myteam/xray-analysis:latest\" \\\n  -d \"type=notebook\" \\\n  https://ws-uv.canfar.net/skaha/v0/session\n</code></pre>"},{"location":"platform/containers/#included-software","title":"Included Software","text":"<ul> <li>XSPEC 12.12.1</li> <li>PyXspec</li> <li>Custom analysis tools v2.1</li> </ul>"},{"location":"platform/containers/#maintenance","title":"Maintenance","text":"<ul> <li>Maintainer: team@institution.edu</li> <li>Update schedule: Monthly</li> <li>Source: https://github.com/myteam/xray-container ```</li> </ul> <p>Regular maintenance includes updating base images, refreshing software dependencies, and testing compatibility with new CANFAR features. Community-maintained containers benefit from collaborative development practices including shared repositories and issue tracking.</p>"},{"location":"platform/containers/#integration-with-canfar-workflows","title":"Integration with CANFAR Workflows","text":"<p>Containers integrate seamlessly with CANFAR's interactive sessions and batch processing systems. Interactive sessions launch containers with full access to mounted storage and appropriate resource allocations. Batch jobs use headless containers for automated processing with results written back to persistent storage.</p> <p>The container system supports different resource configurations including CPU, memory, and GPU allocations based on computational requirements. GPU-enabled containers like astroml-cuda automatically gain access to GPU resources when launched on appropriate hardware nodes.</p> <p>Session persistence allows users to return to running containers across browser sessions while maintaining computational state. However, containers themselves are ephemeral - when sessions end, any changes made within the container filesystem are lost. This design encourages proper separation between stable container environments and dynamic analysis code.</p> <p>Understanding this integration helps optimize workflows by leveraging container strengths while working within system constraints. Plan computational workflows to use containers for consistent software environments while storing results, code, and data on the persistent <code>/arc</code> filesystem.</p> <p>Continue exploring CANFAR capabilities through Interactive Sessions for hands-on container usage, Batch Jobs for automated processing, or Storage Management for data organization strategies.</p>"},{"location":"platform/get-started/","title":"\ud83d\ude80 Quick Setup","text":"<p>This guide will walk you through the complete process of getting started with CANFAR, from account setup to your first analysis session.</p>"},{"location":"platform/get-started/#step-1-get-your-cadc-account","title":"Step 1: Get Your CADC Account","text":"<p>First time user? You need a Canadian Astronomy Data Centre (CADC) account:</p> <p>\ud83d\udd17 Request CADC Account</p> <p>Account Processing Time</p> <p>CADC accounts are typically approved within 1-2 business days.</p>"},{"location":"platform/get-started/#step-2-join-or-create-your-research-group","title":"Step 2: Join or Create Your Research Group","text":"<p>Once you have a CADC account:</p> Joining Existing GroupNew Collaboration <p>Ask your collaboration administrator to add you via the CADC Group Management Interface</p> <p>Email support@canfar.net with:</p> <ul> <li>Your research project description</li> <li>Expected team size</li> <li>Storage requirements</li> <li>Timeline</li> </ul>"},{"location":"platform/get-started/#step-3-first-login-and-setup","title":"Step 3: First Login and Setup","text":"<ol> <li>Login to CANFAR - Visit canfar.net with your CADC credentials</li> <li>Accept Terms of Service - Complete the initial setup</li> <li>Access Image Registry - Login to images.canfar.net (required for private containers)</li> </ol> <p>Pro Tip</p> <p>Ask your group administrator to grant you read access to private container images if your collaboration uses custom software.</p>"},{"location":"platform/get-started/#step-4-launch-your-first-session","title":"Step 4: Launch Your First Session","text":"<p>Try launching a Jupyter notebook to start analyzing data:</p> <ol> <li>Click Science Portal from the main menu</li> <li>Use the default settings as is</li> <li>Click Launch</li> <li>Wait ~30s and click to open your session</li> </ol> <p>\ud83c\udf89 You're ready to go! Your session includes Python, common astronomy packages, and access to shared storage.</p> <p>Recommended Starting Point</p> <p>Start with the default <code>astroml</code> container - it includes most common astronomy packages and is regularly updated with the latest software.</p>"},{"location":"platform/get-started/#understanding-your-workspace","title":"\ud83d\udcc1 Understanding Your Workspace","text":"<p>Now that you're logged in, here's how CANFAR organizes your data:</p> Location Purpose Persistence Best For <code>/arc/projects/yourgroup/</code> Shared research data \u2705 Permanent, backed up Datasets, results, shared code <code>/arc/home/yourusername/</code> Personal files \u2705 Permanent, backed up Personal configs, small files <code>/scratch/</code> Fast temporary space \u274c Wiped at session end Large computations, temporary files"},{"location":"platform/get-started/#collaboration-features","title":"\ud83e\udd1d Collaboration Features","text":""},{"location":"platform/get-started/#session-sharing","title":"Session Sharing","text":"<p>Share running sessions with collaborators:</p> <ol> <li>In your session, copy the session URL</li> <li>Share with team members (must be in same group)</li> <li>They can view and interact with your work in real-time</li> </ol>"},{"location":"platform/get-started/#storage-sharing","title":"Storage Sharing","text":"<p>All group members have access to <code>/arc/projects/yourgroup/</code> - perfect for:</p> <ul> <li>Sharing datasets and results</li> <li>Collaborative analysis scripts</li> <li>Common software environments</li> <li>Project documentation</li> </ul>"},{"location":"platform/get-started/#whats-next","title":"\ud83d\udd17 What's Next?","text":"<p>Ready to dive deeper? </p> <ul> <li>User Guide \u2192 - Comprehensive documentation</li> <li>Storage Guide \u2192 - Detailed storage management</li> <li>Container Guide \u2192 - Using and building containers</li> <li>Radio Astronomy \u2192 - CASA, ALMA workflows</li> </ul>"},{"location":"platform/get-started/#need-help","title":"\ud83d\udcac Need Help?","text":"<ul> <li>\ud83d\udcac Discord Community - Chat with other users</li> <li>\u2753 FAQ - Common questions and solutions</li> </ul>"},{"location":"platform/help/","title":"Getting Help and Support","text":"<p>The CANFAR Science Platform provides multiple channels for getting help, from self-service documentation to direct support. This section guides you to the right resources for your needs.</p>"},{"location":"platform/help/#quick-help","title":"Quick Help","text":""},{"location":"platform/help/#new-to-canfar","title":"New to CANFAR?","text":"<ul> <li>Get Started Guide: 10-minute quick start</li> <li>First Login: Account setup and access</li> <li>Choose Your Interface: Pick the right session type</li> </ul>"},{"location":"platform/help/#having-problems","title":"Having Problems?","text":"<ul> <li>FAQ: Common questions and solutions</li> <li>Troubleshooting: Diagnostic steps for common issues</li> <li>Contact Support: Direct help from CANFAR staff</li> </ul>"},{"location":"platform/help/#want-to-learn-more","title":"Want to Learn More?","text":"<ul> <li>User Guide: Comprehensive platform documentation</li> <li>Radio Astronomy Guide: Specialized astronomy workflows</li> <li>Community: Connect with other users</li> </ul>"},{"location":"platform/help/#self-help-resources","title":"Self-Help Resources","text":""},{"location":"platform/help/#documentation","title":"Documentation","text":"<p>User Guide Sections:</p> <ul> <li>Concepts: Understanding the platform architecture</li> <li>Storage: Managing your data effectively</li> <li>Containers: Using and building software environments</li> <li>Interactive Sessions: Jupyter, Desktop, CARTA</li> <li>Batch Jobs: Automated and large-scale processing</li> <li>Radio Astronomy: CASA and radio-specific workflows</li> </ul> <p>Tutorials:</p> <ul> <li>Data Analysis Examples: Common astronomy workflows</li> <li>Radio Astronomy Guide: CASA and interferometry</li> <li>Container Building: Create custom environments</li> </ul>"},{"location":"platform/help/#video-resources","title":"Video Resources","text":"<p>Getting Started Videos (coming soon):</p> <ul> <li>Platform overview and navigation</li> <li>Creating your first session</li> <li>Data management basics</li> <li>Collaboration features</li> </ul> <p>Workflow Demonstrations:</p> <ul> <li>Optical photometry pipeline</li> <li>Radio interferometry reduction</li> <li>Multi-wavelength analysis</li> </ul>"},{"location":"platform/help/#troubleshooting","title":"Troubleshooting","text":""},{"location":"platform/help/#quick-diagnostic-steps","title":"Quick Diagnostic Steps","text":"<p>When you encounter issues, try these steps first:</p> <ol> <li>Check System status: Look for maintenance announcements</li> <li>Try a different browser: Chrome and Firefox work best</li> <li>Clear browser cache: Remove cookies and cached data</li> <li>Try incognito mode: Eliminates browser extension conflicts</li> <li>Check your network: Ensure stable internet connection</li> </ol>"},{"location":"platform/help/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"platform/help/#session-wont-start","title":"Session Won't Start","text":"<p>Symptoms: Session creation fails or hangs</p> <p>Solutions: - Reduce resource requirements (memory/CPU) - Try during off-peak hours (evenings, weekends) - Select a different container image - Check group permissions</p>"},{"location":"platform/help/#cant-access-files","title":"Can't Access Files","text":"<p>Symptoms: Files missing or permission denied</p> <p>Solutions: <pre><code># Check file locations\nls /arc/home/$(whoami)/     # Personal storage\nls /arc/projects/           # Group storage\n\n# Check permissions\nls -la /arc/projects/mygroup/\n</code></pre></p> <ul> <li>Verify you're in the correct group</li> <li>Check file paths are correct</li> <li>Contact group administrator</li> </ul>"},{"location":"platform/help/#performance-issues","title":"Performance Issues","text":"<p>Symptoms: Slow processing or unresponsive interface</p> <p>Solutions: - Monitor resource usage with <code>htop</code> - Close unnecessary applications - Use scratch storage (<code>/tmp/</code>) for temporary files - Consider requesting more resources</p>"},{"location":"platform/help/#browser-compatibility","title":"Browser Compatibility","text":"<p>Symptoms: Interface doesn't load or behaves incorrectly</p> <p>Solutions: - Use Chrome or Firefox (recommended) - Enable JavaScript and cookies - Disable ad blockers for canfar.net - Update browser to latest version</p>"},{"location":"platform/help/#diagnostic-commands","title":"Diagnostic Commands","text":"<p>Use these commands to gather information for support requests:</p> <pre><code># System information\nuname -a\ncat /proc/cpuinfo | grep \"model name\" | head -1\nfree -h\ndf -h\n\n# Session information\necho $USER\ngroups\nenv | grep -E \"(CANFAR|SESSION)\"\n\n# Network connectivity\nping -c 3 canfar.net\ncurl -I https://canfar.net\n</code></pre>"},{"location":"platform/help/#contact-support","title":"Contact Support","text":""},{"location":"platform/help/#when-to-contact-support","title":"When to Contact Support","text":"<p>Contact support@canfar.net for:</p> <ul> <li>Account issues: Access problems, group membership</li> <li>Technical problems: Persistent errors, system failures</li> <li>Data recovery: Lost or corrupted files</li> <li>Resource requests: Increased storage or compute allocations</li> <li>Software installation: Help with complex software setups</li> </ul>"},{"location":"platform/help/#how-to-write-effective-support-requests","title":"How to Write Effective Support Requests","text":"<p>Include these details in your support email:</p> <p>Essential information: <pre><code>Subject: [Brief description of problem]\n\nCANFAR Username: your.email@domain.com\nDate/Time of issue: 2024-01-15 14:30 PST\nSession type: Desktop/Notebook/CARTA/Batch\nContainer used: astroml:latest\nBrowser: Chrome 120.0.6099\n\nProblem description:\n[Detailed description of what you were trying to do]\n\nError messages:\n[Copy/paste exact error text]\n\nSteps to reproduce:\n1. Login to Science Portal\n2. Create desktop session\n3. [etc.]\n\nWhat you've already tried:\n- Cleared browser cache\n- Tried different browser\n- [etc.]\n</code></pre></p> <p>Additional helpful information: - Screenshots of error messages - Session IDs for failed jobs - File paths for missing data - Group names for permission issues</p>"},{"location":"platform/help/#response-times","title":"Response Times","text":"<ul> <li>Standard support: 1-2 business days</li> <li>Urgent issues: Same day during business hours</li> <li>Emergency outages: Immediate response during business hours</li> </ul> <p>Business hours: Monday-Friday, 9 AM - 5 PM Pacific Time</p>"},{"location":"platform/help/#support-escalation","title":"Support Escalation","text":"<p>For urgent research deadlines or critical system issues:</p> <ol> <li>Mark email as urgent: Use \"URGENT\" in subject line</li> <li>Explain deadline: Include your research timeline</li> <li>Provide context: Explain impact of the issue</li> <li>Follow up: Call if no response within expected timeframe</li> </ol>"},{"location":"platform/help/#community-support","title":"Community Support","text":""},{"location":"platform/help/#discord-community","title":"Discord Community","text":"<p>Join our Discord server for peer support and community interaction:</p> <ul> <li>Quick questions: Get fast answers from other users</li> <li>Tips and tricks: Share and learn best practices</li> <li>Collaboration: Find research partners and collaborators</li> <li>Announcements: Stay updated on new features and maintenance</li> </ul> <p>Discord invite: Join CANFAR Discord</p> <p>Community guidelines: - Search previous messages before asking - Use appropriate channels and threads - Be respectful and helpful to other users - Don't share sensitive data or credentials</p>"},{"location":"platform/help/#github-issues","title":"GitHub Issues","text":"<p>For bug reports and feature requests, use our GitHub repositories:</p> <ul> <li>Platform issues: Report technical problems</li> <li>Documentation: Suggest improvements</li> <li>Feature requests: Propose new capabilities</li> <li>Community contributions: Submit code and examples</li> </ul>"},{"location":"platform/help/#office-hours","title":"Office Hours","text":"<p>Virtual office hours: Thursdays 2-3 PM Pacific Time</p> <ul> <li>Format: Video conference with screen sharing</li> <li>Topics: Any CANFAR-related questions</li> <li>Registration: Not required, drop-in welcome</li> <li>Recording: Sessions recorded for later viewing</li> </ul> <p>What to bring: - Specific questions or problems - Example code or workflows - Error messages or screenshots</p>"},{"location":"platform/help/#peer-mentoring","title":"Peer Mentoring","text":"<p>Experienced user program: Connect new users with experienced mentors</p> <ul> <li>Mentors: Volunteer researchers who use CANFAR regularly</li> <li>Support areas: Platform basics, specific software, research workflows</li> <li>Matching: Based on research area and experience level</li> <li>Contact: Email support to request mentor connection</li> </ul>"},{"location":"platform/help/#community-contributions","title":"Community Contributions","text":"<p>Ways to help other users:</p> <ul> <li>Answer questions: Respond to Discord and community discussions</li> <li>Share tutorials: Create workflow examples</li> <li>Report bugs: Help improve platform stability</li> <li>Suggest features: Propose improvements</li> </ul>"},{"location":"platform/help/#contributing-to-documentation","title":"Contributing to Documentation","text":"<p>The CANFAR Science Platform documentation is community-driven, and we welcome contributions from users who want to help improve the platform for everyone.</p>"},{"location":"platform/help/#how-to-contribute","title":"\ud83d\udcdd How to Contribute","text":"<p>Quick Edits: - Use the \"Edit this page\" link (pencil icon) on any documentation page - Makes suggestions directly on GitHub - Perfect for typos, clarifications, and small updates</p> <p>Larger Contributions: - Set up local development environment - Create comprehensive new sections - Major restructuring or new guides</p>"},{"location":"platform/help/#local-development-setup","title":"\ud83d\ude80 Local Development Setup","text":"<p>Prerequisites: - Python 3.x and pip - Git for version control</p> <p>Setup Steps:</p> <ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/opencadc/science-containers.git\ncd science-containers\n</code></pre></p> </li> <li> <p>Install dependencies (using Poetry):    <pre><code># Install Poetry if you don't have it\n# See: https://python-poetry.org/docs/#installation\n\npoetry install\n</code></pre></p> </li> <li> <p>Start development server:    <pre><code>poetry shell\nmkdocs serve\n</code></pre></p> </li> <li> <p>View documentation: Open <code>http://127.0.0.1:8000</code> in your browser</p> </li> </ol> <p>Changes to documentation files will automatically reload in your browser for real-time preview.</p>"},{"location":"platform/help/#documentation-structure","title":"\ud83d\udcc1 Documentation Structure","text":"<p>Our documentation follows a clear structure designed for different user needs:</p> <ul> <li><code>get-started/</code>: Quick setup for new users</li> <li><code>user-guide/</code>: Comprehensive platform documentation</li> <li><code>concepts/</code>: Platform architecture and core concepts</li> <li><code>accounts-permissions/</code>: User management and access control  </li> <li><code>storage/</code>: Data management and storage systems</li> <li><code>containers/</code>: Container usage and building</li> <li><code>interactive-sessions/</code>: Jupyter, desktop, and application sessions</li> <li><code>batch-jobs/</code>: Automated and large-scale processing</li> <li><code>radio-astronomy/</code>: CASA and radio-specific workflows</li> <li><code>faq/</code>: Frequently asked questions and troubleshooting</li> <li><code>help/</code>: Support resources and community information</li> </ul>"},{"location":"platform/help/#writing-guidelines","title":"\u270d\ufe0f Writing Guidelines","text":"<p>Markdown Style: - Use <code>#</code> for page titles, <code>##</code> for main sections, <code>###</code> for subsections - Code blocks with language specification: <code>```python</code> or <code>```bash</code> - Inline code with single backticks: <code>variable_name</code> or <code>command --option</code></p> <p>Admonitions for Important Information: <pre><code>!!! note\n    General information note\n\n!!! tip \"Pro Tip\"\n    Helpful advice for users\n\n!!! warning\n    Important cautions\n\n!!! danger \"Critical\"\n    Critical warnings\n\n!!! example\n    Code examples and demonstrations\n</code></pre></p> <p>Writing for Different Audiences:</p> <p>New Users: - Avoid jargon or explain technical terms clearly - Provide step-by-step instructions - Focus on common tasks and getting started - Include plenty of examples</p> <p>Advanced Users: - Provide technical details and configuration options - Include information on automation, APIs, and advanced workflows - Assume familiarity with relevant technologies - Link to detailed reference materials</p>"},{"location":"platform/help/#contribution-process","title":"\ud83d\udd04 Contribution Process","text":"<ol> <li>Make your changes in the appropriate documentation files</li> <li>Test locally using <code>mkdocs serve</code> to verify formatting</li> <li>Commit with clear messages: <code>git commit -m \"docs: Describe your change\"</code></li> <li>Submit a pull request to the main repository</li> <li>Collaborate with reviewers to refine your contribution</li> </ol>"},{"location":"platform/help/#documentation-philosophy","title":"\ud83d\udcda Documentation Philosophy","text":"<p>We aim for documentation that is: - Accurate: Technically correct and current - Clear: Easy to understand without unnecessary jargon - Complete: Covering essential aspects comprehensively - User-Friendly: Well-structured and accessible</p>"},{"location":"platform/help/#questions-about-contributing","title":"\u2753 Questions About Contributing?","text":"<ul> <li>Open an issue on GitHub</li> <li>Ask on Discord in the community channels</li> <li>Email the CANFAR team at support@canfar.net</li> </ul> <p>Your contributions help make CANFAR better for the entire astronomy community!</p>"},{"location":"platform/help/#additional-resources","title":"Additional Resources","text":""},{"location":"platform/help/#external-documentation","title":"External Documentation","text":"<p>Related platforms: - CADC Archive: cadc-ccda.hia-iha.nrc-cnrc.gc.ca - VOSpace: vospace.canfar.net - Science Portal: science.canfar.net</p> <p>Software documentation: - CASA: casa.nrao.edu - AstroPy: astropy.org - Jupyter: jupyter.org - Docker: docs.docker.com</p>"},{"location":"platform/help/#research-collaboration","title":"Research Collaboration","text":"<p>Finding collaborators: - Discord community channels - Conference networking - Shared project spaces - Research group connections</p> <p>Collaboration tools: - Real-time session sharing - Shared storage spaces - Version control with Git - Communication through Discord</p>"},{"location":"platform/help/#staying-updated","title":"Staying Updated","text":"<p>Announcements: - Email notifications for maintenance - Discord #announcements channel - Science Portal news section - Twitter @CANFAR_ACFC</p> <p>Feature updates: - Monthly platform updates - New container releases - Beta feature testing - User feedback integration</p>"},{"location":"platform/help/#emergency-contacts","title":"Emergency Contacts","text":""},{"location":"platform/help/#system-outages","title":"System Outages","text":"<p>Planned maintenance: Announced 48+ hours in advance via email and Discord</p> <p>Unplanned outages:  - Check status.canfar.net for current status (when available) - Follow @CANFAR_ACFC for real-time updates - Email support@canfar.net if status unclear</p>"},{"location":"platform/help/#critical-data-issues","title":"Critical Data Issues","text":"<p>Data loss or corruption: 1. Stop all activity: Prevent further damage 2. Document the issue: Note exactly what happened 3. Contact support immediately: Mark email as URGENT 4. Preserve evidence: Don't delete or modify files</p> <p>Backup and recovery: - Daily snapshots of <code>/arc/</code> storage - 30-day retention period - Point-in-time recovery available - Contact support for restoration requests</p>"},{"location":"platform/help/#security-incidents","title":"Security Incidents","text":"<p>Suspected security breach: 1. Change credentials: Update certificates immediately 2. Report incident: Email security@canfar.net 3. Document details: What you observed and when 4. Follow instructions: Wait for security team guidance</p> <p>Prevention: - Never share your certificates - Use secure networks - Keep software updated - Report suspicious activity</p>"},{"location":"platform/help/#contributing-to-documentation_1","title":"Contributing to Documentation","text":"<p>The CANFAR Science Platform documentation is community-driven, and we welcome contributions from users who want to help improve the platform for everyone.</p>"},{"location":"platform/help/#how-to-contribute_1","title":"\ud83d\udcdd How to Contribute","text":"<p>Quick Edits:</p> <ul> <li>Use the \"Edit this page\" link (pencil icon) on any documentation page</li> <li>Makes suggestions directly on GitHub</li> <li>Perfect for typos, clarifications, and small updates</li> </ul> <p>Larger Contributions:</p> <ul> <li>Set up local development environment</li> <li>Create comprehensive new sections</li> <li>Major restructuring or new guides</li> </ul>"},{"location":"platform/help/#local-development-setup_1","title":"\ud83d\ude80 Local Development Setup","text":"<p>Prerequisites:</p> <ul> <li>Python 3.x and pip</li> <li>Git for version control</li> </ul> <p>Setup Steps:</p> <ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/opencadc/science-containers.git\ncd science-containers\n</code></pre> <ol> <li>Install dependencies (using Poetry):</li> </ol> <pre><code># Install Poetry if you don't have it\n# See: https://python-poetry.org/docs/#installation\n\npoetry install\n</code></pre> <ol> <li>Start development server:</li> </ol> <pre><code>poetry shell\nmkdocs serve\n</code></pre> <ol> <li>View documentation: Open <code>http://127.0.0.1:8000</code> in your browser</li> </ol> <p>Changes to documentation files will automatically reload in your browser for real-time preview.</p>"},{"location":"platform/help/#documentation-structure_1","title":"\ud83d\udcc1 Documentation Structure","text":"<p>Our documentation follows a clear structure designed for different user needs:</p> <ul> <li><code>get-started/</code>: Quick setup for new users</li> <li><code>user-guide/</code>: Comprehensive platform documentation</li> <li><code>concepts/</code>: Platform architecture and core concepts</li> <li><code>accounts-permissions/</code>: User management and access control</li> <li><code>storage/</code>: Data management and storage systems</li> <li><code>containers/</code>: Container usage and building</li> <li><code>interactive-sessions/</code>: Jupyter, desktop, and application sessions</li> <li><code>batch-jobs/</code>: Automated and large-scale processing</li> <li><code>radio-astronomy/</code>: CASA and radio-specific workflows</li> <li><code>faq/</code>: Frequently asked questions and troubleshooting</li> <li><code>help/</code>: Support resources and community information</li> </ul>"},{"location":"platform/help/#writing-guidelines_1","title":"\u270d\ufe0f Writing Guidelines","text":"<p>Markdown Style:</p> <ul> <li>Use <code>#</code> for page titles, <code>##</code> for main sections, <code>###</code> for subsections</li> <li>Code blocks with language specification: <code>```python</code> or <code>```bash</code></li> <li>Inline code with single backticks: <code>variable_name</code> or <code>command --option</code></li> </ul> <p>Admonitions for Important Information:</p> <pre><code>!!! note\n    General information note\n\n!!! tip \"Pro Tip\"\n    Helpful advice for users\n\n!!! warning\n    Important cautions\n\n!!! danger \"Critical\"\n    Critical warnings\n\n!!! example\n    Code examples and demonstrations\n</code></pre> <p>Writing for Different Audiences:</p> <p>New Users:</p> <ul> <li>Avoid jargon or explain technical terms clearly</li> <li>Provide step-by-step instructions</li> <li>Focus on common tasks and getting started</li> <li>Include plenty of examples</li> </ul> <p>Advanced Users:</p> <ul> <li>Provide technical details and configuration options</li> <li>Include information on automation, APIs, and advanced workflows</li> <li>Assume familiarity with relevant technologies</li> <li>Link to detailed reference materials</li> </ul>"},{"location":"platform/help/#contribution-process_1","title":"\ud83d\udd04 Contribution Process","text":"<ol> <li>Make your changes in the appropriate documentation files</li> <li>Test locally using <code>mkdocs serve</code> to verify formatting</li> <li>Commit with clear messages: <code>git commit -m \"docs: Describe your change\"</code></li> <li>Submit a pull request to the main repository</li> <li>Collaborate with reviewers to refine your contribution</li> </ol>"},{"location":"platform/help/#documentation-philosophy_1","title":"\ud83d\udcda Documentation Philosophy","text":"<p>We aim for documentation that is:</p> <ul> <li>Accurate: Technically correct and current</li> <li>Clear: Easy to understand without unnecessary jargon</li> <li>Complete: Covering essential aspects comprehensively</li> <li>User-Friendly: Well-structured and accessible</li> </ul>"},{"location":"platform/help/#questions-about-contributing_1","title":"\u2753 Questions About Contributing?","text":"<ul> <li>Open an issue on GitHub</li> <li>Ask on Discord in the community channels</li> <li>Email the CANFAR team at support@canfar.net</li> </ul> <p>Your contributions help make CANFAR better for the entire astronomy community!</p>"},{"location":"platform/help/#contact-information-summary","title":"Contact Information Summary","text":"Need Contact Response Time General support support@canfar.net 1-2 business days Quick questions Discord Community Minutes to hours Security issues security@canfar.net Same day Documentation support@canfar.net 1-2 business days Office hours Video conference Thursdays 2-3 PM PT <p>Remember: The CANFAR team is here to help you succeed in your research. Don't hesitate to reach out with questions, no matter how basic they might seem!</p>"},{"location":"platform/home/","title":"New to CANFAR?","text":"<p>Get up and running with our, </p> <p>\ud83d\udcd6 Getting Started Guide</p>"},{"location":"platform/home/#existing-user","title":"Existing User?","text":"<p>Manage your workspace with,</p> <ul> <li>\ud83c\udf10 CANFAR Portal - Launch sessions</li> <li>\ud83d\udcc1 File Manager - Access and manage storage  </li> <li>\ud83d\udc65 Group Management - Manage teams and permissions</li> </ul>"},{"location":"platform/home/#checkout-our-user-guides","title":"\ud83d\udcd6 Checkout our User Guides","text":"<p>Comprehensive documentation organized by topic:</p> <ul> <li>Platform Concepts - Architecture and key concepts</li> <li>Accounts &amp; Permissions - Access control and collaboration</li> <li>Storage Systems - Data management and file operations</li> <li>Container Usage - Working with software containers</li> <li>Interactive Sessions - Jupyter, CARTA, Firefly, Desktop, and more</li> <li>Batch Jobs - Automated processing and workflows</li> <li>Radio Astronomy - CASA, CARTA, and interferometry workflows</li> </ul>"},{"location":"platform/home/#need-help","title":"\u2753Need Help?","text":"<p>Check out our FAQ and Help &amp; Support pages.</p>"},{"location":"platform/alma/casa/","title":"General notes on CASA containers","text":"<p>This page contains a compilation of useful notes, etc, on various CASA containers.</p>"},{"location":"platform/alma/casa/#astroquery-astropy","title":"Astroquery / astropy","text":"<p>The astroquery tool is presently only installed on newer CASA containers (6.4.4-6.6.3).  To use astroquery from an appropriate CASA container, type the following to initiate an astroquery-compatible version of python: <code>/opt/casa/bin/python3</code> As per the astroquery documentation, the tool can then be used on the command line within the python environment.  For example, the following sequence of commands</p> <p><code>from astroquery.simbad import Simbad</code></p> <p><code>result_table = Simbad.query_object(\"m1\")</code></p> <p><code>result_table.pprint()</code></p> <p>yield a one-line table listing some basic information about M1.</p>"},{"location":"platform/alma/casa/#analysis-utilities","title":"Analysis Utilities","text":"<p>The analysisUtils package package is pre-installed on every CASA container, and is ready to use.  You may need to type <code>import analysisUtils as au</code> to load it.</p>"},{"location":"platform/alma/casa/#admit","title":"ADMIT","text":"<p>ADMIT (ALMA Data Mining Tool) is pre-installed on CASA containers where possible (CASA versions 4.5 and higher) and should be ready to use, although it has not been tested.  Note that the newest CASA containers (6.6.1-pipeline and regular versions 6.6.4 and higher) exclude ADMIT because it does not properly compile.</p>"},{"location":"platform/alma/casa/#firefox","title":"Firefox","text":"<p>The Firefox web-browser, needed for CASA commands where you are interacting with the weblogs, should available for CASA versions 6.1.0 to 6.4.3.  Error messages will pop up in your terminal window, but minimal testing suggests that it is sufficiently functional.</p>"},{"location":"platform/alma/casa/#uvmultifit","title":"UVMultiFit","text":"<p>The UVMultiFit package is presently installed and working for all CASA 5.X versions except 5.8.  To load the UVMultiFit package, initiate casa and then type</p> <p><code>from NordicARC import uvmultifit as uvm</code></p>"},{"location":"platform/alma/casa/#known-container-bugs","title":"Known Container Bugs","text":"<p>1) CASA versions 6.5.0 to 6.5.2 initially launch with some display errors in the logger window.  Exiting casa (but not the container) and re-starting casa fixes the issue, i.e.,</p> <p><code>casa</code></p> <p><code>exit</code></p> <p><code>casa</code></p> <p>2) Running multi-thread pipeline scripts (MPI CASA) may generate error messages, as described here under the 'Running pipeline in non-interactive mode' section.  A CANFAR ALMA user reports success initiating MPI CASA in a Desktop container as follows:</p> <p><code>xvfb-run -a mpicasa casa \u2014nologger \u2014nogui -agg -c casa_script.py</code></p>"},{"location":"platform/alma/casa/#almacasa-adjacent-containers-galario","title":"ALMA/CASA Adjacent Containers : Galario","text":"<p>The UV data analysis package galario is available under the radio-submm menu.  Note that this container has had minimal testing, and the uvplot package commands in the quickstart.py script are not presently working, although all preceeding commands in the quickstart.py script do work.</p>"},{"location":"platform/alma/casa/#almacasa-adjacent-containers-starlink","title":"ALMA/CASA Adjacent Containers : Starlink","text":"<p>The JCMT's Starlink package is available under the radio-submm menu, including image analysis tools and the gaia image viewer.  Note that the starlink-pywrapper add-on package is presently not working.  Minimal testing has been done on the Starlink container.</p>"},{"location":"platform/guides/","title":"User Guides","text":"<p>This guides provides detailed information for astronomers, grad students, and project managers using CANFAR for research. Whether you're analyzing radio astronomy data, building custom software environments, or managing large collaborative projects, this guide has you covered.</p>"},{"location":"platform/guides/#guide-structure","title":"\ud83d\udcd6 Guide Structure","text":""},{"location":"platform/guides/#concepts","title":"\ud83e\udde0 Concepts","text":"<p>Understand the fundamentals: platform architecture, containers, Kubernetes, REST services, and VOSpace.</p>"},{"location":"platform/guides/#accounts-permissions","title":"\ud83d\udc65 Accounts &amp; Permissions","text":"<p>Manage users, groups, Harbor permissions, ACLs, and API access.</p>"},{"location":"platform/guides/#storage","title":"\ud83d\udcbe Storage","text":"<p>Master <code>/arc</code>, VOSpace <code>vault</code>, and scratch storage systems. Learn data transfers, <code>sshfs</code>, and the full VOSpace API.</p>"},{"location":"platform/guides/#containers","title":"\ud83d\udc33 Containers","text":"<p>Work with astronomy software containers, build custom environments, and upload to CANFAR Science Platform.</p>"},{"location":"platform/guides/#interactive-sessions","title":"\ud83d\udda5\ufe0f Interactive Sessions","text":"<p>Launch Jupyter notebooks, CARTA, Firefly, desktop environments, and contributed applications.</p>"},{"location":"platform/guides/#batch-jobs","title":"\u26a1 Batch Jobs","text":"<p>Run \"headless\" containers, understand batch systems, manage logs, and use APIs for automation.</p>"},{"location":"platform/guides/#radio-astronomy","title":"\ud83d\udce1 Radio Astronomy","text":"<p>Specialized workflows for CASA, ALMA data reduction, CARTA visualization, and other radio astronomy tools.</p>"},{"location":"platform/guides/#legacy-documentation","title":"\ud83d\uddc2\ufe0f Legacy Documentation","text":"<p>In addition to the core modules (Accounts, Storage, Containers, Interactive Sessions, etc.), a set of legacy documents is also available. These cover earlier CANFAR service portals and workflows - Cloud Services, which historically paralleled features such as Group Management, Science Portal, and CADC Search.  </p>"},{"location":"platform/guides/#choose-your-path","title":"\ud83c\udfaf Choose Your Path","text":""},{"location":"platform/guides/#new-users","title":"\ud83c\udf31 New Users","text":"<p>First time using CANFAR?</p> <p>Start with our Getting Started Guide for a structured learning path, then:</p> <ol> <li>Concepts - Understand the platform</li> <li>Storage - Manage your data</li> <li>Interactive Sessions - Start analyzing</li> </ol>"},{"location":"platform/guides/#scientists-researchers","title":"\ud83d\udd2c Scientists &amp; Researchers","text":"<p>Ready to analyze data? Jump to your workflow:</p> <ul> <li>\ud83d\udd2d Radio Astronomy - CASA, CARTA workflows and interferometry</li> <li>\ud83d\udcca Interactive Analysis - Jupyter notebooks, Python analysis</li> <li>\ud83d\udda5\ufe0f Desktop Applications - GUI tools, CASA, DS9</li> <li>\ud83d\udcc1 Data Management - Advanced transfer and organization</li> </ul>"},{"location":"platform/guides/#advanced-users","title":"\u26a1 Advanced Users","text":"<p>Looking for development and automation?</p> <ul> <li>\ud83d\udc33 Container Usage - Work with and build custom containers</li> <li>\u2699\ufe0f Batch Processing - Automated workflows and APIs</li> <li>\ud83d\udd10 Access Control - Groups and permissions management</li> </ul>"},{"location":"platform/guides/#quick-references","title":"\ud83d\udd17 Quick References","text":""},{"location":"platform/guides/#platform-access","title":"Platform Access","text":"<ul> <li>CANFAR Portal - Main interface</li> <li>File Manager - Browse storage on <code>/arc</code> and <code>vault</code></li> <li>Group Management - Manage teams and their permissions</li> </ul>"},{"location":"platform/guides/#apis-tools","title":"APIs &amp; Tools","text":"<ul> <li>REST API Docs - Programmatic access for <code>skaha</code></li> <li>VOSpace Tools - Advanced data management</li> <li>Container Registry - Container registry for teams (Harbor)</li> </ul>"},{"location":"platform/guides/#support","title":"Support","text":"<ul> <li>\ud83d\udcac Discord Community - Community support</li> <li>FAQ - Common solutions</li> </ul> <p>Documentation Structure</p> <p>This user guide is organized by workflow rather than by interface. Each section builds on previous concepts, so we recommend reading the Concepts section first if you're new to CANFAR.</p>"},{"location":"platform/guides/#citation","title":"Citation","text":"<p>If you use CANFAR Science Platform for your research, please acknowledge CANFAR in publications:</p> <p>Citation</p> <p>The authors acknowledge the use of the Canadian Advanced Network for Astronomy Research (CANFAR) Science Platform. Our work used the facilities of the Canadian Astronomy Data Center, operated by the National Research Council of Canada with the support of the Canadian Space Agency, and CANFAR, a consortium that serves the data-intensive storage, access, and processing needs of university groups and centers engaged in astronomy research. </p> <p>If you need to refer to a paper, you can use this  (SPIE 2024 citation).</p> <p></p>"},{"location":"platform/guides/interactive-sessions/","title":"Interactive Sessions","text":"<p>Launch and manage interactive computing environments on CANFAR</p> <p>\ud83c\udfaf What You'll Learn</p> <ul> <li>The differences between session types and when to use each</li> <li>How to launch and connect to sessions quickly</li> <li>How to size resources (RAM/CPU/GPU) appropriately</li> <li>How to manage, share, and troubleshoot sessions</li> </ul> <p>Interactive sessions provide web-based access to powerful computing resources with different interfaces optimized for specific workflows. Whether you're analyzing data in Jupyter notebooks, visualizing radio astronomy images, or running GUI applications, CANFAR's interactive sessions make it easy to get started.</p>"},{"location":"platform/guides/interactive-sessions/#session-types-overview","title":"\ud83c\udfaf Session Types Overview","text":"<p>CANFAR supports multiple session types, each optimized for different research workflows:</p> Session Type Interface Best For Key Features \ud83d\udcd3 Notebook JupyterLab Data analysis, coding, documentation Interactive Python, visualization, markdown \ud83d\udda5\ufe0f Desktop Linux desktop GUI applications, legacy software Full desktop environment, X11 apps \ud83d\udcca CARTA CARTA viewer Radio astronomy visualization Cube analysis, region tools, catalogs \ud83d\udd25 Firefly Firefly viewer Optical data, table visualization Image viewer, catalog overlay, cutouts \u2699\ufe0f Contributed Various Community applications Specialized tools, custom interfaces"},{"location":"platform/guides/interactive-sessions/#quick-start-guide","title":"\ud83d\ude80 Quick Start Guide","text":""},{"location":"platform/guides/interactive-sessions/#step-1-access-the-science-portal","title":"Step 1: Access the Science Portal","text":"<ol> <li>Login to CANFAR Portal</li> <li>Navigate to \"Science Portal\" </li> <li>Click the plus sign (+) to create a new session</li> </ol>"},{"location":"platform/guides/interactive-sessions/#step-2-choose-session-type","title":"Step 2: Choose Session Type","text":"<p>Select the interface that best matches your workflow:</p> \ud83d\udcd3 Data Analysis\ud83d\udce1 Radio Astronomy\ud83d\udda5\ufe0f GUI Applications\ud83d\udd2c Table Analysis <p>Session Type: <code>notebook</code> Container: <code>astroml</code> Use Case: Python analysis, Jupyter notebooks, data exploration</p> <p>Session Type: <code>carta</code> Container: <code>carta</code> Use Case: Radio cube visualization, source analysis, imaging</p> <p>Session Type: <code>desktop</code> Container: <code>desktop</code> or <code>astroml</code> Use Case: CASA, DS9, image viewers, legacy tools</p> <p>Session Type: <code>firefly</code> Container: <code>firefly</code> Use Case: Optical data, catalog overlays, image cutouts</p>"},{"location":"platform/guides/interactive-sessions/#step-3-configure-resources","title":"Step 3: Configure Resources","text":"<p>Session Name: Choose a descriptive name (e.g., \"galaxy-photometry\", \"alma-reduction\")</p> <p>Choosing Resources</p> <ul> <li>Start with defaults (2 cores, 8 GB RAM). Scale up only if needed</li> <li>Large data cubes or catalogs benefit from 32GB+ RAM</li> <li>GPU is only needed for ML or specialized GPU-enabled workflows</li> </ul> <p>Memory (RAM): - 8GB: Light analysis, small datasets - 16GB: Default, suitable for most work - 32GB+: Large datasets, memory-intensive tasks</p> <p>CPU Cores: - 2 cores: Default, recommended for most tasks - 4+ cores: Parallel processing, intensive computations</p> <p>GPU (if available): - None: Standard CPU-only work - 1 GPU: Machine learning, image processing</p>"},{"location":"platform/guides/interactive-sessions/#step-4-launch-and-connect","title":"Step 4: Launch and Connect","text":"<ol> <li>Click \"Launch\" and wait for initialization (~30-60 seconds)</li> <li>Session appears on your portal dashboard</li> <li>Click the session icon to connect</li> <li>Start working in your interactive environment</li> </ol>"},{"location":"platform/guides/interactive-sessions/#session-management","title":"\ud83d\udcf1 Session Management","text":""},{"location":"platform/guides/interactive-sessions/#session-lifecycle","title":"Session Lifecycle","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Launching: Click Launch\n    Launching --&gt; Running: Session ready\n    Running --&gt; Suspended: Close browser\n    Suspended --&gt; Running: Reconnect\n    Running --&gt; Terminated: Delete session\n    Terminated --&gt; [*]\n\n    note right of Running: Access from any device\n    note right of Suspended: Session continues running</code></pre>"},{"location":"platform/guides/interactive-sessions/#session-limits","title":"Session Limits","text":"Limit Value Notes Concurrent sessions 3 active sessions Across all interactive session types Session duration 4 days maximum Can be renewed indefinitely Idle timeout None Sessions run until manually deleted Storage Persistent Files saved to <code>/arc/</code> persist"},{"location":"platform/guides/interactive-sessions/#managing-active-sessions","title":"Managing Active Sessions","text":""},{"location":"platform/guides/interactive-sessions/#from-the-science-portal","title":"From the Science Portal","text":"<ul> <li>View all sessions: Portal dashboard shows active sessions</li> <li>Connect to session: Click session icon</li> <li>Extend session: Use \"Renew\" button before expiration</li> <li>Delete session: Click \"X\" to terminate and free resources</li> </ul>"},{"location":"platform/guides/interactive-sessions/#from-command-line","title":"From Command Line","text":"<pre><code># List your active sessions\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  https://ws-uv.canfar.net/skaha/v0/session\n\n# Delete specific session\ncurl -X DELETE \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  https://ws-uv.canfar.net/skaha/v0/session/SESSION_ID\n</code></pre>"},{"location":"platform/guides/interactive-sessions/#session-sharing","title":"Session Sharing","text":"<p>Share running sessions with collaborators in your group:</p> <ol> <li>Copy session URL from browser address bar</li> <li>Share with team member (must be in same CANFAR group)</li> <li>Collaborate in real-time - both can see and modify the same session</li> </ol> <p>Security Note</p> <p>Only share session URLs with trusted collaborators. Anyone with the URL and proper group membership can access your session.</p>"},{"location":"platform/guides/interactive-sessions/#advanced-session-features","title":"\ud83d\udd27 Advanced Session Features","text":""},{"location":"platform/guides/interactive-sessions/#resource-allocation","title":"Resource Allocation","text":""},{"location":"platform/guides/interactive-sessions/#memory-management","title":"Memory Management","text":"<pre><code># Check memory usage in session\nfree -h\nhtop\n\n# Monitor specific process\nps aux | grep python\n</code></pre> <p>Memory Tips: - Start with default 16GB, increase if needed - Large datasets may require 32GB+  - Memory is shared - be considerate of other users</p>"},{"location":"platform/guides/interactive-sessions/#cpu-usage","title":"CPU Usage","text":"<pre><code># Check CPU cores available\nnproc\n\n# Monitor CPU usage\ntop\nhtop\n\n# Run parallel processing\npython -c \"import multiprocessing; print(f'CPUs: {multiprocessing.cpu_count()}')\"\n</code></pre>"},{"location":"platform/guides/interactive-sessions/#gpu-access","title":"GPU Access","text":"<p>For containers with GPU support (e.g., <code>astroml-cuda</code>):</p> <pre><code># Check GPU availability\nnvidia-smi\n\n# Test GPU in Python\npython -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}')\"\n\n# Monitor GPU usage\nwatch -n 1 nvidia-smi\n</code></pre>"},{"location":"platform/guides/interactive-sessions/#persistent-configuration","title":"Persistent Configuration","text":"<p>Save personal settings that persist across sessions:</p> <pre><code># Jupyter configuration\nmkdir -p /arc/home/$USER/.jupyter\ncp jupyter_config.py /arc/home/$USER/.jupyter/\n\n# Shell configuration\necho \"alias ll='ls -la'\" &gt;&gt; /arc/home/$USER/.bashrc\n\n# Python packages (user installation)\npip install --user astroplan # in some containers, the --user may not be needed\n</code></pre>"},{"location":"platform/guides/interactive-sessions/#session-networking","title":"Session Networking","text":""},{"location":"platform/guides/interactive-sessions/#port-forwarding","title":"Port Forwarding","text":"<p>For custom web applications running in sessions:</p> <pre><code># In session: Run application on specific port\npython -m http.server 8080\n\n# Application accessible at: https://SESSION_URL/proxy/8080/\n</code></pre>"},{"location":"platform/guides/interactive-sessions/#external-access","title":"External Access","text":"<p>Sessions are accessible from anywhere with proper authentication:</p> <ul> <li>Same computer: Original browser window</li> <li>Different computer: Copy session URL, login with CADC credentials  </li> <li>Mobile device: Session URL works in mobile browsers</li> </ul>"},{"location":"platform/guides/interactive-sessions/#security-and-best-practices","title":"\ud83d\udee1\ufe0f Security and Best Practices","text":""},{"location":"platform/guides/interactive-sessions/#data-security","title":"Data Security","text":"<p>\u2705 Do: - Save important work to <code>/arc/projects/</code> or <code>/arc/home/</code> - Use group permissions for collaborative data - Regularly save and backup critical results - Log out of shared computers</p> <p>\u274c Don't: - Store sensitive data in <code>/scratch/</code> (wiped at session end) - Share session URLs publicly - Leave sessions running unnecessarily - Store passwords in plain text files</p>"},{"location":"platform/guides/interactive-sessions/#performance-optimization","title":"Performance Optimization","text":""},{"location":"platform/guides/interactive-sessions/#session-performance","title":"Session Performance","text":"<pre><code># Close unused applications to free memory\n# Kill runaway processes\nkill -9 PID\n\n# Clean temporary files\nrm -rf /scratch/temp_*\n</code></pre>"},{"location":"platform/guides/interactive-sessions/#storage-performance","title":"Storage Performance","text":"<pre><code># Use /scratch/ for intensive I/O\ncp /arc/projects/data.fits /scratch/\n# ... process in /scratch/ ...\ncp /scratch/results.fits /arc/projects/\n\n# Compress large files\ngzip large_dataset.fits\n</code></pre>"},{"location":"platform/guides/interactive-sessions/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"platform/guides/interactive-sessions/#session-wont-start","title":"Session Won't Start","text":"<p>Problem: Session stuck in \"Launching\" state</p> <p>Solutions: 1. Check resource availability - try lower memory/CPU 2. Wait 2-3 minutes for container download 3. Try different container image 4. Contact support if persistent</p>"},{"location":"platform/guides/interactive-sessions/#session-disconnected","title":"Session Disconnected","text":"<p>Problem: Lost connection to running session</p> <p>Solutions: 1. Refresh browser page 2. Click session icon again from portal 3. Check internet connection 4. Clear browser cache if needed</p>"},{"location":"platform/guides/interactive-sessions/#out-of-memory-errors","title":"Out of Memory Errors","text":"<p>Problem: Application crashes with memory errors</p> <p>Solutions: 1. Launch new session with more memory 2. Process data in smaller chunks 3. Use <code>/scratch/</code> for temporary files 4. Optimize code for memory efficiency</p>"},{"location":"platform/guides/interactive-sessions/#slow-performance","title":"Slow Performance","text":"<p>Problem: Session responding slowly</p> <p>Solutions: 1. Check system resources (<code>htop</code>, <code>free -h</code>) 2. Close unnecessary applications 3. Use <code>/scratch/</code> for I/O intensive tasks 4. Consider launching session with more resources</p>"},{"location":"platform/guides/interactive-sessions/#session-specific-guides","title":"\ud83d\udd17 Session-Specific Guides","text":"<p>Detailed guides for each session type:</p> <ul> <li>\ud83d\udcd3 Jupyter Notebooks \u2192 - Interactive data analysis and visualization</li> <li>\ud83d\udda5\ufe0f Desktop Environment \u2192 - GUI applications and legacy software</li> <li>\ud83d\udcca CARTA Viewer \u2192 - Radio astronomy cube visualization</li> <li>\ud83d\udd25 Firefly Viewer \u2192 - LSST image and table visualization  </li> <li>\u2699\ufe0f Contributed Apps \u2192 - Community-developed tools</li> </ul>"},{"location":"platform/guides/interactive-sessions/#whats-next","title":"\ud83d\udd17 What's Next?","text":"<p>Once you're comfortable with interactive sessions:</p> <ul> <li>Storage Guide \u2192 - Manage data across sessions</li> <li>Batch Jobs \u2192 - Automate workflows  </li> <li>Container Guide \u2192 - Customize software environments</li> <li>Radio Astronomy \u2192 - Specialized workflows</li> </ul> <p>Session Success Tips</p> <ol> <li>Start small - Use default resources and scale up if needed</li> <li>Save frequently - Important work should go in <code>/arc/</code> directories  </li> <li>Share wisely - Session URLs are powerful - only share with trusted collaborators</li> <li>Monitor resources - Keep an eye on memory and CPU usage with <code>htop</code></li> <li>Clean up - Delete finished sessions to free resources for others</li> </ol>"},{"location":"platform/guides/interactive-sessions/launch-carta/","title":"Launching CARTA Sessions","text":"<p>CARTA (Cube Analysis and Rendering Tool for Astronomy) is a specialized image visualization and analysis tool designed for radio astronomy data. This guide walks you through launching and using CARTA sessions on the CANFAR Science Platform.</p> <p>\ud83c\udfaf What You'll Learn</p> <ul> <li>How to launch a CARTA session and choose the right version</li> <li>How to size RAM/CPU for your datasets</li> <li>How to load data from <code>/arc</code> and work with radio data cubes</li> <li>Tips for analysis features, performance, and troubleshooting</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-carta/#overview","title":"Overview","text":"<p>CARTA provides advanced features for:</p> <ul> <li>Image visualization: Multi-dimensional data cube exploration</li> <li>Spectral analysis: Line profiles and moment maps</li> <li>Region analysis: Statistical analysis of image regions</li> <li>Animation: Time-series and frequency animations</li> <li>Collaboration: Real-time session sharing</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-carta/#creating-a-carta-session","title":"Creating a CARTA Session","text":""},{"location":"platform/guides/interactive-sessions/launch-carta/#step-1-select-session-type","title":"Step 1: Select Session Type","text":"<p>From the Science Portal dashboard, click the plus sign (+) to create a new session,  then select carta as your session type.</p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-carta/#step-2-choose-container-version","title":"Step 2: Choose Container Version","text":"<p>Note that the menu options update automatically after your session type selection.  Choose the CARTA version that meets your needs:</p> <ul> <li>CARTA 4.0 (recommended): Latest features and bug fixes (The screenshot is old and showed older versions only.) <p></p> </li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-carta/#step-3-configure-session","title":"Step 3: Configure Session","text":""},{"location":"platform/guides/interactive-sessions/launch-carta/#session-name","title":"Session Name","text":"<p>Give your session a descriptive name that will help you identify it later  (e.g., \"m87-analysis\", \"ngc-1300-cube\").</p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-carta/#memory-configuration","title":"Memory Configuration","text":"<p>Select the maximum amount of RAM for your analysis. Choose the smallest value  reasonable for your needs, as resources are shared among all users:</p> <ul> <li>8GB: Small images and simple analysis</li> <li>16GB (default): Most radio astronomy datasets</li> <li>32GB+: Large data cubes or complex multi-image analysis</li> </ul> <p>Choosing Resources</p> <p>Start with 8GB RAM and 2 CPU cores. Increase memory for very large FITS or CASA images and increase cores for CPU-intensive operations like moment map generation.</p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-carta/#cpu-cores","title":"CPU Cores","text":"<p>Select the number of computing cores. CARTA can benefit from multiple cores  for certain operations:</p> <ul> <li>1 core: Basic image viewing</li> <li>2 cores (default): Recommended for most tasks</li> <li>4+ cores: Large data processing and animations</li> </ul> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-carta/#step-4-launch-session","title":"Step 4: Launch Session","text":"<p>Click the Launch button and wait for your session to initialize.</p> <p></p> <p>Your session will appear on the Science Portal dashboard with your chosen name.  Click the CARTA icon to access your session.</p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-carta/#using-carta","title":"Using CARTA","text":""},{"location":"platform/guides/interactive-sessions/launch-carta/#initial-setup","title":"Initial Setup","text":"<p>Wait for CARTA to load completely. You'll see the main CARTA interface:</p> <p></p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-carta/#loading-data","title":"Loading Data","text":""},{"location":"platform/guides/interactive-sessions/launch-carta/#navigate-to-your-files","title":"Navigate to Your Files","text":"<ol> <li>Click the folder icon in the upper left to navigate directories</li> <li>Browse to your data location:</li> <li>Project data: <code>/arc/projects/[group]/</code></li> <li>Personal data: <code>/arc/home/[username]/</code></li> </ol>"},{"location":"platform/guides/interactive-sessions/launch-carta/#load-an-image","title":"Load an Image","text":"<ol> <li>Navigate through the directory structure to find your FITS file</li> <li>Select the file you want to visualize</li> <li>Click the Load button</li> </ol>"},{"location":"platform/guides/interactive-sessions/launch-carta/#basic-operations","title":"Basic Operations","text":""},{"location":"platform/guides/interactive-sessions/launch-carta/#image-display","title":"Image Display","text":"<p>Once loaded, your image appears in the main viewer with:</p> <ul> <li>Zoom controls: Mouse wheel or toolbar buttons</li> <li>Pan: Click and drag to move around the image</li> <li>Colormap: Adjust scaling and color scheme</li> </ul> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-carta/#data-cube-navigation","title":"Data Cube Navigation","text":"<p>For 3D data cubes:</p> <ul> <li>Channel slider: Navigate through frequency/velocity channels</li> <li>Animation controls: Play through channels automatically</li> <li>Spectral profile: Click on pixels to see spectral information</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-carta/#region-analysis","title":"Region Analysis","text":"<p>Create analysis regions:</p> <ol> <li>Select region tools from the toolbar</li> <li>Draw regions on your image</li> <li>View statistics in the statistics panel</li> <li>Export region data if needed</li> </ol>"},{"location":"platform/guides/interactive-sessions/launch-carta/#advanced-features","title":"Advanced Features","text":""},{"location":"platform/guides/interactive-sessions/launch-carta/#moment-maps","title":"Moment Maps","text":"<p>Create moment maps from data cubes:</p> <ol> <li>Load your data cube</li> <li>Go to File \u2192 Generate Moment Map</li> <li>Select moment type (0, 1, or 2)</li> <li>Set channel range</li> <li>Generate and display</li> </ol>"},{"location":"platform/guides/interactive-sessions/launch-carta/#spectral-line-analysis","title":"Spectral Line Analysis","text":"<p>Analyze spectral profiles:</p> <ol> <li>Click on any pixel to see the spectrum</li> <li>Use the spectral profiler for detailed analysis</li> <li>Fit Gaussian profiles to lines</li> <li>Measure line properties</li> </ol>"},{"location":"platform/guides/interactive-sessions/launch-carta/#contour-overlays","title":"Contour Overlays","text":"<p>Add contour overlays:</p> <ol> <li>Load a second image</li> <li>Go to View \u2192 Contours</li> <li>Adjust contour levels and styling</li> <li>Overlay on your main image</li> </ol>"},{"location":"platform/guides/interactive-sessions/launch-carta/#session-sharing","title":"Session Sharing","text":""},{"location":"platform/guides/interactive-sessions/launch-carta/#real-time-collaboration","title":"Real-time Collaboration","text":"<p>Share your CARTA session with collaborators:</p> <ol> <li>Click the session menu</li> <li>Select \"Share Session\"</li> <li>Add collaborator usernames</li> <li>Set permissions (view-only or full control)</li> </ol>"},{"location":"platform/guides/interactive-sessions/launch-carta/#export-and-save","title":"Export and Save","text":"<p>Save your work:</p> <ul> <li>Export images: File \u2192 Export Image (PNG, JPEG, PDF)</li> <li>Save regions: File \u2192 Export Regions (DS9, CRTF formats)</li> <li>Save session: File \u2192 Save Layout (restore later)</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-carta/#data-format-support","title":"Data Format Support","text":"<p>CARTA supports multiple astronomical image formats:</p> <ul> <li>FITS: Standard astronomical format</li> <li>CASA images: Radio astronomy standard</li> <li>HDF5: Large dataset format</li> <li>MIRIAD: Legacy radio format</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-carta/#performance-tips","title":"Performance Tips","text":""},{"location":"platform/guides/interactive-sessions/launch-carta/#optimization","title":"Optimization","text":"<ul> <li>Close unused images: Reduces memory usage</li> <li>Use appropriate data types: Float32 vs Float64</li> <li>Enable GPU acceleration: For supported operations</li> <li>Adjust cache settings: Balance memory vs speed</li> </ul> <p>Large Data Cubes</p> <p>Very large cubes can consume significant memory. Load subregions, work with decimated data, or increase RAM to avoid crashes.</p>"},{"location":"platform/guides/interactive-sessions/launch-carta/#large-dataset-handling","title":"Large Dataset Handling","text":"<p>For very large files:</p> <ul> <li>Use decimation: View subsampled versions first</li> <li>Load subregions: Focus on areas of interest</li> <li>Consider preprocessing: Use CASA to create smaller working files</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-carta/#troubleshooting","title":"Troubleshooting","text":""},{"location":"platform/guides/interactive-sessions/launch-carta/#common-issues","title":"Common Issues","text":"<p>CARTA won't load - Check browser compatibility (Chrome/Firefox recommended) - Disable browser extensions that might interfere - Clear browser cache and cookies</p> <p>Performance problems - Reduce image size or resolution - Close other applications - Check available memory</p> <p>File loading errors - Verify file format compatibility - Check file permissions in storage - Ensure file isn't corrupted</p> <p>Display issues - Try different colormaps - Adjust image scaling - Check graphics driver compatibility</p>"},{"location":"platform/guides/interactive-sessions/launch-carta/#getting-help","title":"Getting Help","text":"<ul> <li>CARTA Documentation: carta.casa.nrao.edu</li> <li>Support: Email support@canfar.net</li> <li>Community: Join our Discord for CARTA tips and tricks</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-carta/#best-practices","title":"Best Practices","text":""},{"location":"platform/guides/interactive-sessions/launch-carta/#workflow-organization","title":"Workflow Organization","text":"<ul> <li>Organize data: Keep raw and processed data separate</li> <li>Document analysis: Save session layouts for reproducibility</li> <li>Version control: Track analysis parameters and results</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-carta/#resource-management","title":"Resource Management","text":"<ul> <li>Monitor usage: Check memory consumption during analysis</li> <li>Clean up: Close sessions when finished</li> <li>Share efficiently: Use view-only sharing when appropriate</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-carta/#next-steps","title":"Next Steps","text":"<ul> <li>Radio Astronomy Workflows: CARTA in radio astronomy pipelines</li> <li>Batch Processing: Automate CARTA operations</li> <li>CASA Integration: Combine CARTA with CASA analysis</li> <li>Desktop Sessions: Full desktop environment with CARTA</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-contributed/","title":"Launching Contributed Applications","text":"<p>Access community-developed tools and specialized research applications</p> <p>\ud83c\udfaf What You'll Learn</p> <ul> <li>What contributed applications are and when to use them</li> <li>How to launch and size contributed application sessions</li> <li>How these apps integrate with CANFAR storage and authentication</li> <li>Best practices for collaboration, performance, and security</li> </ul> <p>Contributed applications represent an exciting expansion of CANFAR's capabilities beyond the standard notebook and desktop environments. These specialized tools have been developed by the CANFAR community and external collaborators to address specific astronomical workflows and research needs that aren't well-served by conventional interfaces.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#contributed-apps","title":"\ud83c\udfaf Contributed Apps","text":"<p>Think of contributed applications as purpose-built web tools that seamlessly integrate with CANFAR's infrastructure while offering unique capabilities. Unlike the general-purpose notebook or desktop sessions you might be familiar with, these applications are crafted for specific tasks and often provide interfaces that would be difficult or impossible to replicate in standard environments.</p> <p>These applications extend CANFAR's reach by providing specialized interfaces for targeted research tasks, integrating naturally with CANFAR's storage and authentication systems, and offering workflows that leverage the unique aspects of web-based scientific computing. What makes them particularly valuable is how they support community innovation and enable researchers to share specialized tools with colleagues worldwide.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#the-landscape-of-contributed-applications","title":"The Landscape of Contributed Applications","text":"<p>Current Ecosystem</p> <p>The catalog evolves regularly. Expect reactive notebooks (Pluto.jl, Marimo), browser IDEs (VSCode), and specialized computational interfaces contributed by the community.</p> <p>The current ecosystem of contributed applications focuses on interactive computing environments that offer alternatives to traditional Jupyter notebooks. You'll find reactive notebook systems that provide real-time feedback as you modify code, browser-based development environments that give you the full power of modern IDEs without local installation, and specialized computational interfaces designed for specific programming languages or workflows.</p> <p>Rather than trying to be everything to everyone, each contributed application excels in its particular domain. This focused approach means you can choose the tool that best matches your specific research workflow, whether you're doing exploratory data analysis, developing complex algorithms, or collaborating on code with distributed teams.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"platform/guides/interactive-sessions/launch-contributed/#accessing-the-application-catalog","title":"Accessing the Application Catalog","text":"<p>Beginning your journey with contributed applications starts just like any other CANFAR session. After logging into the CANFAR Science Portal, you'll click the familiar plus sign (+) to create a new session. The key difference comes when you select <code>contributed</code> as your session type, which opens up the specialized application catalog.</p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#exploring-available-applications","title":"Exploring Available Applications","text":"<p>Once you've selected the contributed session type, the container dropdown reveals the current catalog of available applications. This list represents the cutting edge of community-developed tools, and it evolves as new applications are contributed and existing ones are updated.</p> <p></p> <p>Currently Available Applications:</p> <p>Pluto.jl (<code>images.canfar.net/skaha/pluto:latest</code>) offers a revolutionary approach to Julia programming through reactive notebooks. Unlike traditional notebook environments where cells execute in sequence, Pluto creates a dynamic environment where changing any variable automatically updates all dependent computations throughout the notebook. This makes it exceptionally powerful for exploratory data analysis and interactive visualization.</p> <p>Marimo (<code>images.canfar.net/skaha/marimo:latest</code>) brings reactive computing to Python, offering a modern alternative to Jupyter notebooks. Marimo notebooks are stored as pure Python files, making them easy to version control and share. The reactive execution model ensures your notebook stays consistent as you develop and modify your analysis.</p> <p>VSCode Browser (<code>images.canfar.net/skaha/code-browser:latest</code>) provides the full Visual Studio Code experience directly in your web browser. This application is particularly valuable for software development projects, complex multi-file analyses, and situations where you need the rich editing capabilities and extensions ecosystem that VSCode provides.</p> <p>Session Sizing</p> <p>Start with 16GB RAM and 2-4 CPU cores for most contributed apps. Increase memory for large datasets or memory-intensive reactive notebooks.</p> <p>The beauty of this system lies in its dynamic nature. As the community develops new tools and contributes them to the platform, the available applications expand to meet emerging research needs. If you have ideas for applications that would benefit the astronomy community, the CANFAR team encourages you to reach out to support@canfar.net.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#configuring-your-session","title":"Configuring Your Session","text":""},{"location":"platform/guides/interactive-sessions/launch-contributed/#choosing-a-meaningful-name","title":"Choosing a Meaningful Name","text":"<p>When setting up your contributed application session, choose a name that reflects both the application you're using and the purpose of your work. Names like <code>pluto-galaxy-analysis-2024</code>, <code>marimo-pipeline-development</code>, or <code>vscode-survey-processing</code> help you quickly identify the session's purpose when you return to your work later.</p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#resource-planning","title":"Resource Planning","text":"<p>The resource requirements for contributed applications can vary significantly depending on the nature of your work and the specific application you're using. Web-based development environments like VSCode Browser typically run comfortably with 8-16GB of memory and 2-4 CPU cores, while reactive notebook environments processing large datasets might benefit from 16-32GB of memory.</p> <p>For memory allocation, consider starting with 16GB as a reasonable default for most contributed applications. If you're working with large datasets or computationally intensive visualizations, you might want to increase this to 32GB or more. The reactive nature of some applications means they might use more memory than traditional notebooks since they maintain the state of all computations simultaneously.</p> <p>CPU requirements are generally modest for most contributed applications, with 2-4 cores being sufficient for typical workflows. However, if your work involves parallel processing or you're running multiple computational tasks simultaneously, additional cores can significantly improve performance.</p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#launching-and-connecting","title":"Launching and Connecting","text":"<p>After configuring your session parameters, clicking \"Launch\" initiates the container deployment process. The session will appear on your portal dashboard, and you can monitor its startup progress. Contributed applications typically take 30-90 seconds to fully initialize, as they need to start the web service and establish connections to CANFAR's storage systems.</p> <p>Once the session is running, clicking the session icon will open the application in a new browser tab. The first connection might take a few additional moments as the application completes its startup sequence and presents its interface.</p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#working-effectively","title":"\ud83d\udd27 Working Effectively","text":""},{"location":"platform/guides/interactive-sessions/launch-contributed/#understanding-application-interfaces","title":"Understanding Application Interfaces","text":"<p>Most contributed applications follow modern web application conventions, but each has its own personality and workflow patterns. Rather than trying to force a one-size-fits-all approach, take some time to explore each application's unique features and interface paradigms.</p> <p>The typical structure you'll encounter includes a navigation or menu area that provides access to the application's main features, a central content area where your work happens, and various panels or sidebars for configuration, file management, and tool access. Status information and feedback usually appear in designated areas that don't interfere with your primary workflow.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#integrating-with-canfar-storage","title":"Integrating with CANFAR Storage","text":"<p>Persistence Reminder</p> <p>Save important results to <code>/arc/projects/</code> or <code>/arc/home/</code>. Temporary paths and in-app caches may not persist after the session ends.</p> <p>One of the most powerful aspects of contributed applications is their seamless integration with CANFAR's storage infrastructure. Your applications can directly access your project data through <code>/arc/projects/yourproject/</code>, your personal files via <code>/arc/home/yourusername/</code>, and temporary processing space in <code>/scratch/</code>. This integration means you can move fluidly between different types of sessions and applications while maintaining access to the same data.</p> <p>Understanding these storage patterns helps you organize your work effectively. You might use your personal home directory for notebooks and scripts under development, project directories for shared data and collaborative work, and scratch space for temporary files and intermediate processing results that don't need long-term storage.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#authentication-and-collaboration","title":"Authentication and Collaboration","text":"<p>The integration with CANFAR's authentication system means you don't need to manage separate credentials for each application. Your existing CANFAR account provides access to all contributed applications, and the same group-based permissions that govern your access to shared storage also apply within applications.</p> <p>This seamless authentication enables powerful collaboration patterns. You can share session URLs with collaborators who have appropriate permissions, work together in real-time on analysis projects, and maintain consistent access controls across different tools and workflows.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#real-world-examples","title":"\ud83d\udee0\ufe0f Real-World Examples","text":""},{"location":"platform/guides/interactive-sessions/launch-contributed/#interactive-data-exploration-with-plutojl","title":"Interactive Data Exploration with Pluto.jl","text":"<p>Imagine you're working with a large astronomical catalog and want to explore correlations between different measured parameters. Traditional notebook approaches require you to re-run cells manually as you adjust parameters or add new visualizations. With Pluto.jl, you can create sliders and interactive controls that automatically update all dependent visualizations and calculations in real-time.</p> <p>You might start by loading your catalog data and creating a basic scatter plot. As you add interactive controls for magnitude cuts, color selections, or coordinate ranges, the plot updates automatically. When you decide to add a histogram showing the distribution of selected objects, it immediately reflects your current filter settings. This reactive approach makes exploratory data analysis feel more like an interactive conversation with your data.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#collaborative-development-with-vscode-browser","title":"Collaborative Development with VSCode Browser","text":"<p>Consider a scenario where you're collaborating with colleagues on a complex data processing pipeline that involves multiple Python modules, configuration files, and documentation. Using VSCode Browser, you can work in a full-featured development environment that includes syntax highlighting, debugging capabilities, integrated terminal access, and extension support for specialized astronomical tools.</p> <p>The browser-based nature means your collaborators can access the same development environment without worrying about local software installation or version compatibility issues. Everyone works with the same tools, the same Python environment, and the same file system, eliminating the \"works on my machine\" problem that often plagues collaborative software development.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#modern-python-notebooks-with-marimo","title":"Modern Python Notebooks with Marimo","text":"<p>If you've experienced frustration with traditional Jupyter notebooks becoming inconsistent as you develop and modify your analysis, Marimo offers a refreshing alternative. Because Marimo notebooks are reactive and stored as standard Python files, you can develop complex analyses that remain consistent and reproducible.</p> <p>The reactive execution model means that when you modify a function or variable definition, all dependent computations automatically update. This eliminates the common notebook problem where cells are executed out of order, leaving you with inconsistent results. The fact that notebooks are stored as Python files also makes them easy to version control with Git and share with colleagues who might not be using notebook environments.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#best-practices","title":"\ud83d\udd12 Best Practices","text":""},{"location":"platform/guides/interactive-sessions/launch-contributed/#understanding-application-permissions","title":"Understanding Application Permissions","text":"<p>Contributed applications operate within the same security framework as other CANFAR services, inheriting your account permissions and access controls. This means they can access your home directory, project directories where you're a member, and VOSpace areas where you have appropriate permissions. However, they cannot access other users' private data or perform system administration functions.</p> <p>When working with contributed applications, it's important to understand what data the application might access and how it processes that information. Most applications are designed to work locally with your data and don't transmit information to external services, but it's always good practice to review the documentation for any application you're using for the first time.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#data-security-considerations","title":"Data Security Considerations","text":"<p>The web-based nature of contributed applications introduces some unique security considerations. Always use HTTPS connections when accessing applications, be cautious about uploading sensitive credentials or tokens to applications unless specifically required, and use CANFAR's group-based permissions to manage access to shared data appropriately.</p> <p>For researchers working with sensitive or proprietary data, it's worth understanding how each application handles data processing and whether any information might be cached or logged. Most contributed applications are designed with these concerns in mind, but understanding the data flow helps you make informed decisions about which tools to use for different types of work.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#contributing-your-apps","title":"\ud83e\uddd1\u200d\ud83d\udcbb Contributing Your Apps","text":""},{"location":"platform/guides/interactive-sessions/launch-contributed/#understanding-the-technical-framework","title":"Understanding the Technical Framework","text":"<p>If you're interested in developing your own contributed application, the technical requirements are designed to be straightforward while ensuring security and compatibility with CANFAR's infrastructure. Your application needs to be containerized and provide a web interface that runs on port 5000.</p> <p>The key technical requirement is including a startup script at <code>/skaha/startup.sh</code> in your container image. This script serves as the entry point for your application and should handle the initialization and startup of your web service. Here's how this typically works in practice:</p> <pre><code># Expose the required port\nEXPOSE 5000\n\n# Create the skaha directory and copy your startup script\nRUN mkdir /skaha\nCOPY your_startup_script.sh /skaha/startup.sh\nRUN chmod gou+x /skaha/startup.sh\n\n# Set the startup script as the entrypoint\nENTRYPOINT [ \"/skaha/startup.sh\" ]\n</code></pre> <p>Your startup script needs to launch your web application in a way that handles signals properly and listens on the correct interface. Here's the pattern used by the Marimo application:</p> <pre><code>#!/bin/bash -e\nset -e\necho \"[Application Startup] Starting application server...\"\n\n# Use 'exec' to replace the script process with your application process.\n# This is important for signal handling (e.g., SIGTERM from Kubernetes).\nexec your_application \\\n  --port 5000 \\\n  --host 0.0.0.0 \\\n  --other-required-options\n</code></pre> <p>The <code>exec</code> command is crucial because it ensures proper signal handling when the container needs to shut down. The <code>--host 0.0.0.0</code> flag makes your application accessible from outside the container, and <code>--port 5000</code> uses the standard port that CANFAR expects.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#development-and-testing-workflow","title":"Development and Testing Workflow","text":"<p>Developing a contributed application typically follows an iterative process. You'll start by building and testing your application locally using Docker, ensuring it works correctly in a containerized environment. Once you have a working container, you can test it on CANFAR by pushing it to a container registry and launching it as a contributed application session.</p> <p>During development, pay particular attention to how your application integrates with CANFAR's storage systems and authentication. Test that your application can access the expected file system paths and that it behaves correctly when running under the CANFAR user account rather than as root.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#community-integration","title":"Community Integration","text":"<p>The most successful contributed applications solve real problems that multiple researchers face and provide capabilities that aren't easily replicated with existing tools. Before beginning development, consider reaching out to the CANFAR community through the support channels to discuss your ideas and gather feedback on potential use cases.</p> <p>When you're ready to contribute your application, contact support@canfar.net with information about your application's purpose, target user community, and key features. The CANFAR team will work with you to integrate your application into the platform and ensure it meets the technical and security requirements.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#troubleshooting","title":"\ud83c\udd98 Troubleshooting","text":""},{"location":"platform/guides/interactive-sessions/launch-contributed/#application-startup-problems","title":"Application Startup Problems","text":"<p>If your contributed application doesn't load or seems to hang during startup, the most common cause is that the application is still initializing. Web applications can take 60-90 seconds to fully start up, especially if they need to install packages or perform initial configuration. Try waiting a bit longer before concluding there's a problem.</p> <p>Browser caching can sometimes cause issues with contributed applications, especially if you've used the same application before and it has been updated. Try refreshing the page or opening the application in a private/incognito browser window to bypass potential caching issues.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#data-access-challenges","title":"Data Access Challenges","text":"<p>Problems accessing data usually stem from incorrect file paths or permission issues. Verify that the files you're trying to access exist in the expected locations and that you have the necessary permissions. Remember that contributed applications access the same file system as other CANFAR sessions, so paths that work in Jupyter notebooks should work in contributed applications as well.</p> <p>If you're having trouble accessing shared project data, check your group membership and ensure that the project directory has the correct permissions. Sometimes data access issues are actually authentication problems in disguise.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#performance-and-resource-issues","title":"Performance and Resource Issues","text":"<p>If your contributed application feels slow or unresponsive, consider whether you've allocated sufficient resources for your workload. Web-based applications can be more memory-intensive than you might expect, especially reactive notebook environments that maintain state for all computations.</p> <p>You can often improve performance by closing unnecessary browser tabs, ensuring you have sufficient memory allocated to your session, and monitoring your resource usage through the browser's developer tools if available.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#evolving-ecosystem","title":"\ud83d\udcda Evolving Ecosystem","text":""},{"location":"platform/guides/interactive-sessions/launch-contributed/#current-applications-and-their-strengths","title":"Current Applications and Their Strengths","text":"<p>The current catalog of contributed applications represents different approaches to interactive scientific computing, each with its own strengths and ideal use cases. Pluto.jl excels in situations where you want immediate feedback on how changes affect your analysis and is particularly powerful for teaching and exploration. Marimo brings similar reactive capabilities to Python while maintaining compatibility with standard Python tooling and version control systems.</p> <p>VSCode Browser provides the most comprehensive development environment, making it ideal for complex software projects, multi-file analyses, and situations where you need advanced editing capabilities or specific extensions. It's particularly valuable when you're developing tools that others will use or when you need the debugging and profiling capabilities that come with a full IDE.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#looking-forward","title":"Looking Forward","text":"<p>The contributed applications ecosystem continues to evolve as the community identifies new needs and develops innovative solutions. Future applications might focus on specialized domains like time-series analysis, interactive 3D visualization, or collaborative annotation tools. The flexibility of the platform means that if you can envision a web-based tool that would benefit astronomical research, it can likely be implemented as a contributed application.</p> <p>The key to this ecosystem's success is community engagement. As more researchers use these tools and provide feedback, applications improve and new ideas emerge. If you have suggestions for improvements to existing applications or ideas for entirely new tools, the CANFAR team encourages you to share them.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#research-integration","title":"\ud83d\udd17 Research Integration","text":""},{"location":"platform/guides/interactive-sessions/launch-contributed/#building-comprehensive-workflows","title":"Building Comprehensive Workflows","text":"<p>Contributed applications work best when integrated into comprehensive research workflows that leverage CANFAR's full capabilities. You might begin analysis in a Jupyter notebook session, move to a contributed application for specialized processing or visualization, and then return to notebooks for final analysis and documentation.</p> <p>The seamless access to shared storage means you can hand off work between different session types and applications without worrying about data transfer or synchronization. This flexibility allows you to choose the best tool for each phase of your research rather than being constrained by the limitations of any single environment.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#collaboration-and-knowledge-sharing","title":"Collaboration and Knowledge Sharing","text":"<p>The web-based nature of contributed applications makes them particularly powerful for collaboration and knowledge sharing. You can share session URLs with colleagues for real-time collaboration, use the same applications for training workshops and educational activities, and ensure that everyone on your team has access to the same tools and capabilities regardless of their local computing environment.</p> <p>This democratization of access to specialized tools can significantly impact how research teams work together and how knowledge is transferred between experienced and novice researchers.</p>"},{"location":"platform/guides/interactive-sessions/launch-contributed/#whats-next","title":"\ud83d\udd17 What's Next?","text":"<p>Contributed applications represent just one facet of CANFAR's comprehensive research platform. To make the most of these tools, consider exploring how they integrate with CANFAR's other capabilities. The Storage Guide will help you effectively manage data for use with contributed applications. Understanding Notebook Sessions can help you prepare data for specialized processing in contributed applications. For automated workflows, Batch Processing can complement the interactive analysis you do in contributed applications. And if you're interested in developing your own tools, the Container Development guide provides the technical foundation for creating contributed applications.</p> <p>Making the Most of Contributed Applications</p> <p>The key to success with contributed applications lies in matching the right tool to your specific workflow needs. Take time to explore each application's unique capabilities, don't hesitate to experiment with different approaches to your analysis challenges, and remember that the most powerful workflows often combine multiple tools and session types. The CANFAR community is always eager to help you find the best approaches for your research, so don't hesitate to reach out with questions or suggestions.</p>"},{"location":"platform/guides/interactive-sessions/launch-desktop/","title":"Launching Desktop Sessions","text":"<p>Desktop sessions provide a full Linux graphical environment in your browser, giving you access to traditional astronomy software with familiar desktop interfaces. This guide walks you through launching and using desktop sessions on the CANFAR Science Platform.</p> <p>\ud83c\udfaf What You'll Learn</p> <ul> <li>How to launch, connect, and size desktop sessions</li> <li>What software is available and how to launch it</li> <li>How to manage files and storage within desktop sessions</li> <li>Tips for collaboration, performance, and troubleshooting</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#overview","title":"Overview","text":"<p>Desktop sessions offer:</p> <ul> <li>Full Linux desktop: Complete graphical environment in your browser</li> <li>Multi-application workflow: Run multiple programs simultaneously</li> <li>Traditional interfaces: Use software with graphical user interfaces</li> <li>File management: Visual file browser and management tools</li> <li>Session persistence: Resume work exactly where you left off</li> </ul> <p>Common use cases include:</p> <ul> <li>Running GUI-based astronomy software (DS9, SAOImage, IRAF)</li> <li>Complex multi-step workflows requiring multiple applications</li> <li>Teaching and demonstration scenarios</li> <li>Legacy software that requires a desktop environment</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#creating-a-desktop-session","title":"Creating a Desktop Session","text":""},{"location":"platform/guides/interactive-sessions/launch-desktop/#step-1-select-session-type","title":"Step 1: Select Session Type","text":"<p>From the Science Portal dashboard, click the plus sign (+) to create a new session,  then select desktop as your session type.</p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#step-2-choose-container","title":"Step 2: Choose Container","text":"<p>The desktop container includes a comprehensive set of astronomy software. Currently,  there's one main desktop environment available with pre-installed tools.</p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#step-3-configure-session","title":"Step 3: Configure Session","text":""},{"location":"platform/guides/interactive-sessions/launch-desktop/#session-name","title":"Session Name","text":"<p>Give your session a descriptive name that will help you identify it later  (e.g., \"data-reduction\", \"teaching-session\", \"multi-instrument-analysis\").</p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#resource-configuration","title":"Resource Configuration","text":"<p>Desktop sessions typically require more resources than other session types:</p> <p>Memory (RAM): - 16GB (recommended): Standard desktop usage - 32GB: Multiple applications or large datasets - 64GB: Intensive workflows with large data</p> <p>CPU Cores: - 2 cores: Basic desktop operations - 4 cores (recommended): Multiple applications - 8+ cores: Compute-intensive desktop workflows</p>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#step-4-launch-session","title":"Step 4: Launch Session","text":"<p>Click the Launch button and wait for your session to initialize. Desktop sessions  may take slightly longer to start than other session types.</p> <p></p> <p>Your session will appear on the Science Portal dashboard. Click the desktop icon to access your session.</p> <p></p> <p>Connection Timing</p> <p>Sometimes it takes a few seconds for the session link to work properly. If you see a \"Bad gateway\" error, wait a moment and try again.</p>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#connecting-to-your-desktop","title":"Connecting to Your Desktop","text":""},{"location":"platform/guides/interactive-sessions/launch-desktop/#initial-connection","title":"Initial Connection","text":"<p>Click the Connect button to access your desktop environment.</p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#desktop-environment","title":"Desktop Environment","text":"<p>You'll see a full Linux desktop environment with:</p> <ul> <li>Taskbar: Application launcher and system controls</li> <li>File manager: Browse your storage and files</li> <li>Terminal: Command-line access</li> <li>Pre-installed software: Astronomy applications ready to use</li> </ul> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#session-persistence","title":"Session Persistence","text":"<p>When your session becomes inactive, you'll be returned to the connection page.  Click Connect again to resume exactly where you left off - all your applications  and work remain open.</p>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#available-software","title":"Available Software","text":""},{"location":"platform/guides/interactive-sessions/launch-desktop/#astronomy-applications","title":"Astronomy Applications","text":"<p>Your desktop session includes:</p> <ul> <li>DS9: Advanced FITS image viewer and analyzer</li> <li>SAOImage: Astronomical image display</li> <li>CASA: Complete radio astronomy suite with GUI</li> <li>Python environments: With Jupyter, AstroPy, and other libraries</li> <li>IRAF/PyRAF: Legacy optical astronomy reduction (if needed)</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#development-tools","title":"Development Tools","text":"<ul> <li>Text editors: gedit, vim, emacs</li> <li>IDEs: Available through package installation</li> <li>Version control: Git and other VCS tools</li> <li>Compilers: GCC, Python, and other development tools</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#system-tools","title":"System Tools","text":"<ul> <li>File manager: Graphical file operations</li> <li>Terminal: Full shell access</li> <li>System monitor: Resource usage monitoring</li> <li>Network tools: File transfer and connectivity utilities</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#working-with-applications","title":"Working with Applications","text":""},{"location":"platform/guides/interactive-sessions/launch-desktop/#launching-applications","title":"Launching Applications","text":"<p>Method 1: Application Menu 1. Click the applications menu in the taskbar 2. Browse categories (Graphics, Science, Development) 3. Click to launch your chosen application</p> <p>Method 2: Terminal <pre><code># Launch DS9\nds9 &amp;\n\n# Launch CASA with GUI\ncasa --gui &amp;\n\n# Launch Python with astronomy libraries\nipython --pylab\n</code></pre></p> <p>Method 3: File Association - Double-click FITS files to open in DS9 - Right-click files for \"Open with\" options</p>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#example-multi-application-workflow","title":"Example: Multi-Application Workflow","text":"<p>Here's a typical desktop workflow for optical astronomy:</p> <ol> <li>File Management: Use file manager to organize data</li> <li>Image Display: Open FITS files in DS9 for inspection</li> <li>Analysis: Launch Python/Jupyter for data analysis</li> <li>Documentation: Use text editor for notes and scripts</li> <li>Results: Save plots and analysis outputs</li> </ol>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#casa-desktop-usage","title":"CASA Desktop Usage","text":"<p>To use CASA with its graphical interface:</p> <pre><code># Launch CASA with GUI\ncasa --gui\n\n# Or use the interactive shell\ncasa\n</code></pre> <p>The desktop environment allows you to use CASA's plotting and visualization  tools that aren't available in command-line mode.</p>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#desktop-session-tips","title":"Desktop Session Tips","text":""},{"location":"platform/guides/interactive-sessions/launch-desktop/#copy-paste-between-containers","title":"Copy &amp; Paste Between Containers","text":"<p>Since different containers (e.g., CASA and terminal windows) in a desktop session may run on different remote computers, copying and pasting text between containers requires using the Clipboard application.</p>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#accessing-the-clipboard","title":"Accessing the Clipboard","text":"<ol> <li>Open the Clipboard: Click the arrow at the far left of the desktop to open the application menu</li> <li>Find Clipboard: Look for \"Clipboard\" in the middle of the application list and click it</li> </ol>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#using-the-clipboard","title":"Using the Clipboard","text":"<p>The Clipboard serves as an intermediary for transferring text between containers:</p> <ol> <li>Copy text: Highlight text in the source container and use <code>Ctrl+Shift+C</code></li> <li>Transfer via Clipboard: The text should appear in the Clipboard application</li> <li>Select in Clipboard: Highlight the text in the Clipboard and press <code>Ctrl+Shift+C</code></li> <li>Paste to target: Click in the destination container and use <code>Ctrl+Shift+V</code></li> </ol> <p></p> <p>Keyboard Shortcuts</p> <ul> <li>Copy: <code>Ctrl+Shift+C</code></li> <li>Paste: <code>Ctrl+Shift+V</code></li> <li>These shortcuts work consistently across all desktop containers</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#adjusting-font-size","title":"Adjusting Font Size","text":"<p>Desktop containers support adjustable font sizes for better readability:</p>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#changing-terminal-font-size","title":"Changing Terminal Font Size","text":"<ol> <li>Access font menu: Hold <code>Ctrl</code> and right-click anywhere in a terminal window</li> <li>Select size: Choose from the available font size options (Small, Medium, Large)</li> <li>Apply immediately: Font changes take effect instantly</li> </ol> <p>This feature works in: - Terminal windows - CASA command-line interface - Text-based applications</p> <p>Font Persistence</p> <p>Font size changes apply only to the current session. You'll need to readjust when starting new sessions.</p>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#file-management","title":"File Management","text":""},{"location":"platform/guides/interactive-sessions/launch-desktop/#storage-access","title":"Storage Access","text":"<p>Your desktop session provides access to:</p> <ul> <li><code>/arc/projects/[group]/</code>: Shared project storage</li> <li><code>/arc/home/[username]/</code>: Personal persistent storage</li> <li><code>/home/[username]/</code>: Session-local home directory</li> <li><code>/tmp/</code>: Temporary scratch space</li> </ul> <p>Persistence Reminder</p> <p>Use <code>/arc/projects/</code> or <code>/arc/home/</code> for important files. The session-local home and <code>/tmp/</code> are not guaranteed to persist after the session ends.</p>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#file-operations","title":"File Operations","text":"<p>Use the graphical file manager for:</p> <ul> <li>Drag-and-drop: Move files between directories</li> <li>Visual browsing: Preview images and data files</li> <li>Batch operations: Select multiple files for operations</li> <li>Permissions: Set file and directory permissions</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#data-transfer","title":"Data Transfer","text":"<p>Transfer files to/from your desktop session:</p> <ul> <li>Browser upload: Use Science Portal file manager</li> <li>Command line: <code>scp</code>, <code>rsync</code>, <code>wget</code></li> <li>Cloud storage: Mount external storage systems</li> <li>USB/local: Upload through browser interface</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#session-management","title":"Session Management","text":""},{"location":"platform/guides/interactive-sessions/launch-desktop/#resource-monitoring","title":"Resource Monitoring","text":"<p>Monitor your session resources:</p> <pre><code># Check memory usage\nfree -h\n\n# Monitor CPU usage\nhtop\n\n# Check disk space\ndf -h /arc /tmp\n</code></pre>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Close unused applications: Free up memory and CPU</li> <li>Use virtual desktops: Organize workflows across multiple desktops</li> <li>Monitor network: Large file transfers can affect responsiveness</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#ending-sessions","title":"Ending Sessions","text":"<p>To properly end your desktop session:</p> <ol> <li>Save all work: Ensure data is saved to persistent storage</li> <li>Close applications: Exit programs cleanly</li> <li>Disconnect: Close the browser tab</li> <li>Delete session: Use Science Portal to free resources</li> </ol>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#collaboration-features","title":"Collaboration Features","text":""},{"location":"platform/guides/interactive-sessions/launch-desktop/#screen-sharing","title":"Screen Sharing","text":"<p>Share your desktop session with collaborators:</p> <ol> <li>Access session sharing from Science Portal</li> <li>Add collaborator usernames</li> <li>Set permissions (view-only or full control)</li> <li>Collaborators see your full desktop in real-time</li> </ol>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#teaching-and-demonstration","title":"Teaching and Demonstration","text":"<p>Desktop sessions are ideal for:</p> <ul> <li>Live demonstrations: Show software usage to groups</li> <li>Hands-on training: Students can follow along</li> <li>Collaborative debugging: Work together on problems</li> <li>Code review: Visual examination of code and results</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#troubleshooting","title":"Troubleshooting","text":""},{"location":"platform/guides/interactive-sessions/launch-desktop/#common-issues","title":"Common Issues","text":"<p>Session won't connect - Wait 30 seconds and try again - Check browser compatibility (Chrome/Firefox recommended) - Disable browser extensions that might interfere</p> <p>Poor performance - Check resource usage with <code>htop</code> - Close unnecessary applications - Consider increasing session memory</p> <p>Applications won't start - Check for error messages in terminal - Verify sufficient memory available - Try launching from command line for error details</p> <p>File access problems - Verify paths to <code>/arc/projects/[group]/</code> - Check group permissions - Ensure files aren't locked by other processes</p>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#browser-optimization","title":"Browser Optimization","text":"<p>For best performance:</p> <ul> <li>Use Chrome or Firefox: Best compatibility and performance</li> <li>Close other tabs: Free up browser memory</li> <li>Stable connection: Ensure reliable internet connection</li> <li>Disable browser extensions: Remove potential conflicts</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#getting-help","title":"Getting Help","text":"<ul> <li>Support: Email support@canfar.net</li> <li>Community: Join our Discord for desktop tips</li> <li>Documentation: Check software-specific guides for DS9, CASA, etc.</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#best-practices","title":"Best Practices","text":""},{"location":"platform/guides/interactive-sessions/launch-desktop/#resource-management","title":"Resource Management","text":"<ul> <li>Plan resource needs: Estimate memory for your workflow</li> <li>Monitor usage: Check system resources regularly</li> <li>Clean up: Remove temporary files when finished</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#workflow-organization","title":"Workflow Organization","text":"<ul> <li>Organize windows: Use multiple desktops for complex workflows</li> <li>Save frequently: Desktop sessions can be terminated for maintenance</li> <li>Document work: Keep notes on your analysis steps</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#data-management","title":"Data Management","text":"<ul> <li>Use persistent storage: Save important work to <code>/arc/</code></li> <li>Organize files: Create clear directory structures</li> <li>Backup results: Important outputs should be backed up</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-desktop/#next-steps","title":"Next Steps","text":"<ul> <li>CASA Desktop Workflows: Advanced CASA usage</li> <li>Radio Astronomy Guide: Desktop-based radio analysis workflows</li> <li>Batch Processing: Scale up desktop workflows</li> <li>Notebook Sessions: Alternative analysis environment</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-firefly/","title":"Launching a Firefly Session","text":"<p>The LSST table and image visualizer for astronomical data exploration</p> <p>\ud83c\udfaf What You'll Learn</p> <ul> <li>How to launch a Firefly session and choose the right version</li> <li>How to load images, tables, and access CANFAR storage</li> <li>How to perform catalog overlays, plotting, and cutouts</li> <li>Performance tips for large surveys and troubleshooting guidance</li> </ul> <p>Firefly is a powerful web-based visualization tool originally developed for the Rubin Observatory LSST. It provides advanced capabilities for viewing images, overlaying catalogs, and analyzing tabular data - making it perfect for survey data analysis and multi-wavelength astronomy.</p>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#what-is-firefly","title":"\ud83c\udfaf What is Firefly?","text":"<p>Firefly offers specialized tools for:</p> <ul> <li>Image visualization with advanced stretch and color controls</li> <li>Catalog overlay and source analysis tools  </li> <li>Table viewer with filtering, plotting, and statistical tools</li> <li>Multi-wavelength data comparison and analysis</li> <li>Large survey datasets like LSST, HSC, and WISE</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#key-features","title":"Key Features","text":"Feature Capability Image Display FITS images with WCS support, multiple panels Catalog Overlay Plot sources on images, interactive selection Table Analysis Sort, filter, plot columns, statistical analysis Multi-band RGB color composites, band switching Cutout Services Extract subimages from large surveys Coordinate Systems Support for all standard astronomical coordinates"},{"location":"platform/guides/interactive-sessions/launch-firefly/#launching-firefly","title":"\ud83d\ude80 Launching Firefly","text":""},{"location":"platform/guides/interactive-sessions/launch-firefly/#step-1-create-new-session","title":"Step 1: Create New Session","text":"<ol> <li>Login to the CANFAR Science Portal</li> <li>Click the plus sign (+) to create a new session</li> <li>Select <code>firefly</code> as your session type</li> </ol>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#step-2-choose-container","title":"Step 2: Choose Container","text":"<p>The container selection updates automatically after choosing the session type. Select the Firefly container version you need:</p> <ul> <li>firefly:latest - Most recent stable version (recommended)</li> <li>firefly:X.X - Specific version for reproducible analysis</li> </ul> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#step-3-configure-session","title":"Step 3: Configure Session","text":""},{"location":"platform/guides/interactive-sessions/launch-firefly/#session-name","title":"Session Name","text":"<p>Choose a descriptive name that helps identify your work: - <code>lsst-photometry</code> - <code>hsc-catalog-analysis</code>  - <code>multiband-survey</code></p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#memory-requirements","title":"Memory Requirements","text":"<p>Select RAM based on your data size:</p> <ul> <li>8GB: Small catalogs, single images</li> <li>16GB: Default, suitable for most work</li> <li>32GB: Large catalogs, multiple images</li> <li>64GB: Very large survey datasets</li> </ul> <p>Memory Planning</p> <p>Tables with millions of rows and multi-image layouts benefit from 32GB+ RAM. Start with 8GB and scale up if you hit browser or session limits.</p>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#cpu-cores","title":"CPU Cores","text":"<p>Most Firefly work is I/O bound rather than CPU intensive:</p> <ul> <li>2 cores: Default, sufficient for most visualization tasks</li> <li>4 cores: Large table operations, complex filtering</li> </ul> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#step-4-launch-session","title":"Step 4: Launch Session","text":"<ol> <li>Click \"Launch\" button</li> <li>Wait for container initialization (~30-60 seconds)</li> <li>Session appears on your portal dashboard</li> <li>Click the session icon to access Firefly</li> </ol>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#using-firefly","title":"\ud83d\udd25 Using Firefly","text":""},{"location":"platform/guides/interactive-sessions/launch-firefly/#interface-overview","title":"Interface Overview","text":"<p>Firefly's interface consists of several main areas:</p> <pre><code>graph TD\n    Interface[Firefly Interface]\n\n    Interface --&gt; Upload[\"\ud83d\udcc1 File Upload Area\"]\n    Interface --&gt; Images[\"\ud83d\uddbc\ufe0f Image Display\"]\n    Interface --&gt; Tables[\"\ud83d\udcca Table Viewer\"]\n    Interface --&gt; Tools[\"\ud83d\udd27 Analysis Tools\"]\n\n    Upload --&gt; Local[Local Files]\n    Upload --&gt; URLs[Remote URLs]\n    Upload --&gt; VOSpace[VOSpace Files]\n\n    Images --&gt; Display[Image Canvas]\n    Images --&gt; Controls[Display Controls]\n    Images --&gt; Overlays[Catalog Overlays]\n\n    Tables --&gt; Browse[Data Browser]\n    Tables --&gt; Filter[Filtering Tools]\n    Tables --&gt; Plot[Plotting Tools]</code></pre>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#loading-data","title":"Loading Data","text":""},{"location":"platform/guides/interactive-sessions/launch-firefly/#upload-local-files","title":"Upload Local Files","text":"<p>FITS Images: <pre><code>1. Click \"Images\" tab\n2. Select \"Upload\" \n3. Choose FITS file from your computer\n4. Image loads automatically with WCS if available\n</code></pre></p> <p>Catalog Tables: <pre><code>1. Click \"Tables\" tab\n2. Select \"Upload\"\n3. Choose CSV, FITS table, or VOTable\n4. Table opens in browser interface\n</code></pre></p>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#access-canfar-storage","title":"Access CANFAR Storage","text":"<p>From <code>arc</code> Projects: <pre><code># Files in your project directory are accessible via:\n# /arc/projects/yourproject/data/image.fits\n# /arc/projects/yourproject/catalogs/sources.csv\n</code></pre></p> <p>From VOSpace: <pre><code>1. In Firefly, use \"File\" \u2192 \"Open\"\n2. Navigate to VOSpace URLs\n3. Access: vos://cadc.nrc.ca~vault/yourproject/\n</code></pre></p>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#remote-data-access","title":"Remote Data Access","text":"<p>Survey Archives: <pre><code># Example URLs for Firefly\nhttps://archive.stsci.edu/hlsp/data.fits\nhttps://irsa.ipac.caltech.edu/data/WISE/cutouts/\n</code></pre></p>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#image-analysis","title":"Image Analysis","text":""},{"location":"platform/guides/interactive-sessions/launch-firefly/#basic-image-display","title":"Basic Image Display","text":"<pre><code>1. Load FITS image\n2. Adjust stretch (log, linear, sqrt)\n3. Set scale limits (min/max values)\n4. Choose color table (heat, cool, rainbow)\n</code></pre>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#multi-band-rgb","title":"Multi-band RGB","text":"<pre><code>1. Load three images (e.g., g, r, i bands)\n2. Select \"RGB\" mode\n3. Assign each image to R, G, or B channel\n4. Adjust relative scaling\n</code></pre>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#coordinate-systems","title":"Coordinate Systems","text":"<pre><code># Firefly supports standard coordinate systems:\n- Equatorial (RA/Dec) - J2000, B1950\n- Galactic coordinates\n- Ecliptic coordinates  \n- Pixel coordinates\n</code></pre>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#catalog-analysis","title":"Catalog Analysis","text":""},{"location":"platform/guides/interactive-sessions/launch-firefly/#table-operations","title":"Table Operations","text":"<p>Basic Navigation: <pre><code>- Sort columns by clicking headers\n- Filter rows using search box\n- Select multiple rows with Ctrl+click\n- Pan/zoom table with mouse wheel\n</code></pre></p> <p>Advanced Filtering: <pre><code>// Example filters (use in filter box):\nmagnitude &lt; 20.5                    // Bright sources\ncolor_g_r &gt; 0.5 &amp;&amp; color_g_r &lt; 1.5  // Color selection\ndistance &lt; 100                      // Distance constraint\n</code></pre></p>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#plotting-tools","title":"Plotting Tools","text":"<p>Column Plots: <pre><code>1. Select table columns for X and Y axes\n2. Choose plot type (scatter, histogram, line)\n3. Apply color coding by third column\n4. Add error bars if available\n</code></pre></p> <p>Image-Catalog Overlay: <pre><code>1. Load image and catalog table\n2. Match coordinate columns (RA, Dec)\n3. Select overlay symbol (circle, cross, diamond)\n4. Adjust symbol size and color\n5. Sources appear overlaid on image\n</code></pre></p>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#advanced-features","title":"Advanced Features","text":""},{"location":"platform/guides/interactive-sessions/launch-firefly/#cutout-services","title":"Cutout Services","text":"<p>Extract subimages from large surveys:</p> <pre><code># Using Firefly's cutout interface\n1. Right-click on image location\n2. Select \"Create Cutout\"\n3. Specify size (arcmin)\n4. Choose format (FITS, JPEG, PNG)\n5. Download or save to VOSpace\n</code></pre>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#multi-wavelength-analysis","title":"Multi-wavelength Analysis","text":"<pre><code>1. Load images in different bands\n2. Use \"Blink\" mode to compare\n3. Create RGB composite\n4. Overlay catalog with color-magnitude selection\n5. Identify sources across wavelengths\n</code></pre>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#data-export","title":"Data Export","text":"<p>Save Results: <pre><code>- Modified tables \u2192 CSV, FITS, VOTable formats\n- Image displays \u2192 PNG, PDF for publications  \n- Analysis plots \u2192 Vector formats for papers\n- Session state \u2192 Save/restore workspace\n</code></pre></p>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#common-workflows","title":"\ud83d\udee0\ufe0f Common Workflows","text":""},{"location":"platform/guides/interactive-sessions/launch-firefly/#survey-photometry","title":"Survey Photometry","text":"<pre><code>1. Load survey image (HSC, LSST, etc.)\n2. Upload photometric catalog\n3. Overlay sources on image\n4. Filter by magnitude and color\n5. Create color-magnitude diagram\n6. Export selected sources\n</code></pre>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#multi-object-analysis","title":"Multi-object Analysis","text":"<pre><code>1. Load target list (CSV with coordinates)\n2. Create cutouts around each target\n3. Measure properties in each cutout\n4. Compile results in table\n5. Plot trends and correlations\n6. Save analysis products\n</code></pre>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#time-series-visualization","title":"Time Series Visualization","text":"<pre><code>1. Load time-series table (time, magnitude, error)\n2. Create light curve plot\n3. Apply period folding if needed\n4. Identify outliers and trends\n5. Export cleaned data\n</code></pre>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#integration-with-canfar","title":"\ud83d\udd27 Integration with CANFAR","text":""},{"location":"platform/guides/interactive-sessions/launch-firefly/#storage-access","title":"Storage Access","text":"<p>ARC Projects: <pre><code># Your project data appears in Firefly file browser\n/arc/projects/yourproject/\n\u251c\u2500\u2500 images/           # FITS images\n\u251c\u2500\u2500 catalogs/         # Source tables  \n\u251c\u2500\u2500 results/          # Analysis products\n\u2514\u2500\u2500 plots/            # Exported figures\n</code></pre></p> <p>VOSpace Integration: <pre><code># Access archived data\nvos://cadc.nrc.ca~vault/yourproject/\n\u251c\u2500\u2500 published_data/   # Public datasets\n\u251c\u2500\u2500 working_data/     # Analysis in progress\n\u2514\u2500\u2500 final_products/   # Paper-ready results\n</code></pre></p>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#collaborative-features","title":"Collaborative Features","text":"<p>Session Sharing: <pre><code>1. Copy Firefly session URL\n2. Share with team members (same CANFAR group)\n3. Collaborate on analysis in real-time\n4. Each user sees same data and visualizations\n</code></pre></p> <p>Data Sharing: <pre><code>1. Save analysis results to shared project space\n2. Export publication-quality figures\n3. Share VOSpace links for external collaborators\n4. Version control important datasets\n</code></pre></p>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#performance-tips","title":"\ud83d\udcca Performance Tips","text":""},{"location":"platform/guides/interactive-sessions/launch-firefly/#large-dataset-handling","title":"Large Dataset Handling","text":"<p>Memory Management: <pre><code>- Load subsets of large catalogs first\n- Use server-side filtering when possible\n- Close unused tables and images\n- Monitor memory usage in browser\n</code></pre></p> <p>Network Optimization: <pre><code>- Use compressed file formats (gzip FITS)\n- Access local files when possible (/arc/projects)\n- Cache frequently used data locally\n- Use cutout services for large images\n</code></pre></p>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#visualization-performance","title":"Visualization Performance","text":"<p>Image Display: <pre><code>- Use appropriate image size for screen resolution\n- Apply reasonable stretch limits\n- Close unused image panels\n- Use PNG format for screenshots\n</code></pre></p> <p>Table Operations: <pre><code>- Filter large tables before plotting\n- Use sampling for very large datasets\n- Index frequently used columns\n- Batch operations when possible\n</code></pre></p>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#troubleshooting","title":"\ud83c\udd98 Troubleshooting","text":""},{"location":"platform/guides/interactive-sessions/launch-firefly/#common-issues","title":"Common Issues","text":"<p>Firefly Won't Load: <pre><code>- Check browser compatibility (Chrome, Firefox recommended)\n- Clear browser cache and cookies\n- Disable browser extensions that might interfere\n- Try incognito/private browsing mode\n</code></pre></p> <p>Images Not Displaying: <pre><code>- Verify FITS file format and WCS headers\n- Check file permissions and accessibility\n- Try loading smaller test image first\n- Ensure sufficient memory allocation\n</code></pre></p> <p>Tables Not Loading: <pre><code>- Verify file format (CSV, FITS table, VOTable)\n- Check column headers and data types\n- Ensure proper delimiter in CSV files\n- Try loading subset of data first\n</code></pre></p> <p>Performance Issues: <pre><code>- Reduce number of overlay sources\n- Close unused browser tabs\n- Increase session memory allocation\n- Use more efficient file formats\n</code></pre></p>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#external-resources","title":"\ud83d\udd17 External Resources","text":""},{"location":"platform/guides/interactive-sessions/launch-firefly/#documentation","title":"Documentation","text":"<ul> <li>Firefly User Guide - Comprehensive documentation</li> <li>LSST Science Pipelines - Integration with LSST tools</li> <li>IRSA Tutorials - Survey data tutorials</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#data-archives","title":"Data Archives","text":"<ul> <li>LSST Data Portal - LSST survey data</li> <li>HSC Archive - Hyper Suprime-Cam data</li> <li>IRSA - Infrared survey data</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-firefly/#whats-next","title":"\ud83d\udd17 What's Next?","text":"<p>Firefly works great with other CANFAR tools:</p> <ul> <li>Table Analysis \u2192 - Advanced catalog management</li> <li>Desktop Sessions \u2192 - Use Firefly with other GUI tools</li> <li>Batch Processing \u2192 - Automate large survey analysis</li> <li>Container Guide \u2192 - Customize Firefly environment</li> </ul> <p>Firefly Best Practices</p> <ol> <li>Start with small datasets to learn the interface before tackling large surveys</li> <li>Use appropriate memory - large catalogs need more RAM than single images  </li> <li>Save your work frequently - export important results to <code>/arc/projects/</code></li> <li>Collaborate effectively - share session URLs for real-time teamwork</li> <li>Optimize performance - close unused data and use efficient file formats</li> </ol>"},{"location":"platform/guides/interactive-sessions/launch-notebook/","title":"Launching Jupyter Notebook Sessions","text":"<p>Interactive Jupyter Lab sessions provide a powerful environment for data analysis, visualization, and computational astronomy. This guide walks you through launching and using notebook sessions on the CANFAR Science Platform.</p> <p>\ud83c\udfaf What You'll Learn</p> <ul> <li>How to launch a Jupyter notebook session on CANFAR</li> <li>How to choose the right container and resources</li> <li>How storage works inside notebooks and what persists</li> <li>Tips for performance, collaboration, and troubleshooting</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#overview","title":"Overview","text":"<p>Jupyter notebooks combine code execution, rich text documentation, and inline visualizations in a single interface. CANFAR's notebook sessions include:</p> <ul> <li>Jupyter Lab: Full-featured development environment</li> <li>Pre-configured containers: Astronomy-specific software stacks</li> <li>Persistent storage: Access to your ARC and VOSpace data</li> <li>Collaborative sharing: Share sessions with team members</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#creating-a-new-session","title":"Creating a New Session","text":""},{"location":"platform/guides/interactive-sessions/launch-notebook/#step-1-access-session-creation","title":"Step 1: Access Session Creation","text":"<p>From the Science Portal dashboard, click the plus sign (+) to create a new session,  then select notebook as your session type.</p> <p></p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#step-2-choose-your-container","title":"Step 2: Choose Your Container","text":"<p>Select a container image that includes the software you need. Each container comes pre-configured with specific tools and libraries:</p> <ul> <li>astroml (recommended): Modern Python astronomy libraries (AstroPy, NumPy, SciPy, Matplotlib)</li> <li>CASA 6.5-notebook: Includes CASA (Common Astronomy Software Applications) for radio astronomy</li> <li>General purpose: Standard Python data science stack</li> </ul> <p></p> <p>Container Selection</p> <p>Start with astroml for most astronomy workflows. It includes the latest astronomy libraries and is actively maintained. Use CASA containers only when you specifically need CASA functionality.</p>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#step-3-configure-session-resources","title":"Step 3: Configure Session Resources","text":""},{"location":"platform/guides/interactive-sessions/launch-notebook/#session-name","title":"Session Name","text":"<p>Choose a descriptive name that helps you identify this session later (e.g., \"galaxy-photometry\", \"pulsar-analysis\").</p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#memory-ram-selection","title":"Memory (RAM) Selection","text":"<p>Select the maximum memory your analysis will require. Start conservatively\u2014you can always launch a new session with more memory if needed.</p> <ul> <li>8GB: Light data analysis, small datasets</li> <li>16GB (default): Suitable for most analyses, equivalent to a MacBook Pro</li> <li>32GB+: Large datasets, memory-intensive computations</li> </ul> <p>Resource sharing: Computing resources are shared among all users. Large memory requests may delay session startup if resources are unavailable.</p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#cpu-cores","title":"CPU Cores","text":"<p>Choose the number of processing cores based on your computational needs:</p> <ul> <li>1 core: Simple analysis, single-threaded code</li> <li>2 cores (default): Recommended for most tasks</li> <li>4+ cores: Parallel processing, intensive computations</li> </ul> <p>Most astronomy software uses only one core unless specifically configured for parallel processing.</p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#step-4-launch-your-session","title":"Step 4: Launch Your Session","text":"<p>Click the Launch button to create your Notebook session. The system will:</p> <ol> <li>Allocate computing resources</li> <li>Pull the container image (if not cached)</li> <li>Initialize your environment</li> <li>Start Jupyter Lab</li> </ol> <p>First Launch Timing</p> <p>The first launch of a specific container may take 2-3 minutes while the image is downloaded and cached. Subsequent launches are typically 30-60 seconds.</p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#step-5-connect-to-your-session","title":"Step 5: Connect to Your Session","text":"<p>Wait for the Notebook icon to appear on your dashboard, then click it to access your session. Initial startup may take 2-3 minutes if the container hasn't been used recently on this server.</p> <p></p>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#working-in-jupyter-lab","title":"Working in Jupyter Lab","text":""},{"location":"platform/guides/interactive-sessions/launch-notebook/#available-interfaces","title":"Available Interfaces","text":"<ul> <li>Python 3 (ipykernel): Standard Python with astronomy libraries</li> <li>Terminal: Command-line access for advanced operations</li> <li>File Browser: Navigate your <code>/arc</code> storage directories</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#storage-access","title":"Storage Access","text":"<p>Your notebook session automatically mounts:</p> <ul> <li><code>/arc/projects/[group]/</code>: Shared project storage</li> <li><code>/arc/home/[username]/</code>: Personal storage</li> <li><code>/tmp/</code>: Temporary space (cleared when session ends)</li> </ul> <p>Save to Persistent Storage</p> <p>Files in <code>/tmp/</code> do not persist when the session ends. Save important work to <code>/arc/projects/</code> or <code>/arc/home/</code>. For heavy I/O, use <code>/scratch/</code> if available and copy results to <code>/arc</code> when done.</p>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#example-astronomy-analysis","title":"Example: Astronomy Analysis","text":"<p>Here's a simple example using AstroPy to work with FITS data:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom astropy.io import fits\nfrom astropy.wcs import WCS\n\n# Load a FITS file\nhdul = fits.open(\"/arc/projects/myproject/data/image.fits\")\ndata = hdul[0].data\nheader = hdul[0].header\n\n# Display the image\nplt.figure(figsize=(10, 8))\nplt.imshow(data, origin=\"lower\", cmap=\"viridis\")\nplt.colorbar(label=\"Flux\")\nplt.title(\"Astronomical Image\")\nplt.show()\n</code></pre>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#using-casa-in-jupyter","title":"Using CASA in Jupyter","text":"<p>If you're using a CASA container, you can run CASA commands directly in Python notebooks:</p> <p></p> <pre><code># CASA example\nimport casa_tools as tools\nimport casa_tasks as tasks\n\n# Create measurement set summary\ntasks.listobs(vis=\"/arc/projects/myproject/data/observation.ms\")\n\n# Image the data\ntasks.tclean(\n    vis=\"/arc/projects/myproject/data/observation.ms\",\n    imagename=\"my_image\",\n    imsize=1024,\n    cell=\"0.1arcsec\",\n)\n</code></pre>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#session-management","title":"Session Management","text":""},{"location":"platform/guides/interactive-sessions/launch-notebook/#sharing-sessions","title":"Sharing Sessions","text":"<p>Share your session with collaborators:</p> <ol> <li>Click the session menu in Jupyter Lab</li> <li>Select \"Share Session\"</li> <li>Add collaborator usernames</li> <li>Set permissions (read-only or read-write)</li> </ol>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#saving-your-work","title":"Saving Your Work","text":"<ul> <li>Auto-save: Notebooks auto-save every 2 minutes</li> <li>Manual save: Use Ctrl+S or File \u2192 Save</li> <li>Version control: Consider using Git for code versioning</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#ending-sessions","title":"Ending Sessions","text":"<p>Always properly shut down your session to free resources:</p> <ol> <li>Save all work</li> <li>Close Jupyter Lab tab</li> <li>Return to Science Portal</li> <li>Click the session icon and select \"Delete\"</li> </ol>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#best-practices","title":"Best Practices","text":""},{"location":"platform/guides/interactive-sessions/launch-notebook/#resource-usage","title":"Resource Usage","text":"<ul> <li>Start small: Begin with minimal resources and scale up if needed</li> <li>Monitor usage: Use the terminal to check memory with <code>htop</code> or <code>free -h</code></li> <li>Clean up: Remove large temporary files when finished</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#data-management","title":"Data Management","text":"<ul> <li>Organize files: Create clear directory structures in your <code>/arc</code> space</li> <li>Document work: Use markdown cells to explain your analysis</li> <li>Backup results: Important results should be saved to persistent storage</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#collaboration","title":"Collaboration","text":"<ul> <li>Share sessions: Real-time collaboration for debugging and teaching</li> <li>Version control: Use Git for code sharing and version management</li> <li>Documentation: Well-documented notebooks help collaborators understand your work</li> </ul>"},{"location":"platform/guides/interactive-sessions/launch-notebook/#troubleshooting","title":"Troubleshooting","text":""},{"location":"platform/guides/interactive-sessions/launch-notebook/#common-issues","title":"Common Issues","text":"<p>Session won't start - Check resource availability - Try reducing memory/CPU requirements - Contact support if persistent</p> <p>Can't access files - Verify file paths in <code>/arc/projects/[group]/</code> - Check group permissions - Ensure files were uploaded correctly</p> <p>Notebook kernel crashes - Often due to memory overuse - Restart kernel and reduce data size - Consider using more memory</p> <p>Performance issues - Check if other users are sharing resources - Use <code>htop</code> to monitor system usage - Consider running during off-peak hours</p>"},{"location":"platform/guides/radio-astronomy/","title":"Radio Astronomy on CANFAR","text":"<p>The CANFAR Science Platform provides comprehensive support for radio astronomy data processing, with specialized containers and workflows optimized for radio interferometry, single-dish observations, and VLBI analysis.</p> <p>\ud83c\udfaf What You'll Learn</p> <ul> <li>The radio astronomy software stack available on CANFAR (CASA, CARTA, tools)</li> <li>How to choose session types and containers for common workflows</li> <li>Typical interferometry, single-dish, and VLBI workflows</li> <li>Strategies for large-scale processing, storage, and performance</li> <li>Troubleshooting tips and best practices for reliable pipelines</li> </ul>"},{"location":"platform/guides/radio-astronomy/#overview","title":"Overview","text":"<p>Radio astronomy on CANFAR includes:</p> <ul> <li>CASA (Common Astronomy Software Applications): Complete radio interferometry suite</li> <li>CARTA: Advanced radio data cube visualization</li> <li>Specialized containers: Optimized for radio astronomy workflows</li> <li>Large-scale processing: Handle multi-terabyte datasets efficiently</li> <li>Collaborative analysis: Share sessions and results with your team</li> </ul>"},{"location":"platform/guides/radio-astronomy/#radio-astronomy-software-stack","title":"Radio Astronomy Software Stack","text":""},{"location":"platform/guides/radio-astronomy/#casa-environments","title":"CASA Environments","text":"<p>CANFAR provides multiple CASA versions and configurations:</p> <ul> <li>CASA 6.5 (recommended): Latest stable release with Python 3 support</li> <li>CASA 6.4: Previous stable version for compatibility</li> <li>CASA Pipeline: Automated calibration and imaging pipelines</li> <li>Custom builds: Specialized versions for specific instruments</li> </ul>"},{"location":"platform/guides/radio-astronomy/#supporting-tools","title":"Supporting Tools","text":"<ul> <li>CARTA: Interactive visualization of radio data cubes</li> <li>Miriad: Legacy radio astronomy package</li> <li>AIPS: Classic NRAO package (limited support)</li> <li>Python libraries: RadioAstro tools, APLpy, spectral-cube</li> </ul>"},{"location":"platform/guides/radio-astronomy/#data-format-support","title":"Data Format Support","text":"<ul> <li>Measurement Sets (MS): CASA native format</li> <li>FITS: Single-dish and processed data</li> <li>UVFITS: Legacy interferometry format</li> <li>HDF5: Large dataset storage</li> <li>MIRIAD: Legacy format support</li> </ul> <p>Choosing a Container</p> <p>For interactive analysis, start with <code>casa:6.5-notebook</code> or <code>casa:6.5-desktop</code>. Use <code>carta</code> for visualization. For pipelines, use a headless CASA container via batch jobs.</p>"},{"location":"platform/guides/radio-astronomy/#common-radio-astronomy-workflows","title":"Common Radio Astronomy Workflows","text":""},{"location":"platform/guides/radio-astronomy/#interferometry-data-reduction","title":"Interferometry Data Reduction","text":"<pre><code>graph TD\n    A[Raw Visibility Data] --&gt; B[Data Import]\n    B --&gt; C[Flagging]\n    C --&gt; D[Calibration]\n    D --&gt; E[Imaging]\n    E --&gt; F[Self-calibration]\n    F --&gt; G[Final Images]\n\n    D --&gt; H[Bandpass]\n    D --&gt; I[Gain]\n    D --&gt; J[Flux Scale]</code></pre> <p>Typical steps:</p> <ol> <li>Data import: Convert raw data to Measurement Set format</li> <li>Inspection: Check data quality and structure</li> <li>Flagging: Remove RFI and bad data</li> <li>Calibration: Bandpass, gain, and flux scale calibration</li> <li>Imaging: Create maps with CLEAN algorithm</li> <li>Self-calibration: Iterative improvement</li> <li>Analysis: Extract scientific results</li> </ol>"},{"location":"platform/guides/radio-astronomy/#single-dish-processing","title":"Single-dish Processing","text":"<p>Workflow elements:</p> <ul> <li>Data import: Load single-dish observations</li> <li>Baseline subtraction: Remove instrumental effects</li> <li>Calibration: Temperature and flux calibration</li> <li>Gridding: Combine multiple observations</li> <li>Imaging: Create final maps</li> </ul>"},{"location":"platform/guides/radio-astronomy/#vlbi-processing","title":"VLBI Processing","text":"<p>Specialized tools:</p> <ul> <li>CASA VLBI pipeline: Automated correlation and calibration</li> <li>AIPS integration: For legacy VLBI workflows</li> <li>Custom scripts: Institution-specific pipelines</li> </ul> <p>Large Datasets</p> <p>Radio data can be very large. Use <code>/scratch/</code> for temporary processing and save results promptly to <code>/arc/projects/</code>. Plan resource allocations (RAM/CPU) accordingly.</p>"},{"location":"platform/guides/radio-astronomy/#getting-started-with-radio-astronomy","title":"Getting Started with Radio Astronomy","text":""},{"location":"platform/guides/radio-astronomy/#choosing-the-right-session-type","title":"Choosing the Right Session Type","text":"<p>For interactive analysis: - Notebook sessions: Python-based analysis with CASA - Desktop sessions: Full CASA GUI access - CARTA sessions: Data cube visualization</p> <p>For large-scale processing: - Batch jobs: Automated pipeline execution - API submissions: Programmatic job control</p> <p>Resource Sizing</p> <p>Start with 16GB RAM and 2-4 cores for interactive work; increase for imaging and large cubes. Batch jobs can request more resources but may queue longer.</p>"},{"location":"platform/guides/radio-astronomy/#container-selection","title":"Container Selection","text":"<p>Choose containers based on your needs:</p> <pre><code># Latest CASA with Jupyter\nimages.canfar.net/skaha/casa:6.5-notebook\n\n# CASA desktop environment\nimages.canfar.net/skaha/casa:6.5-desktop\n\n# CARTA for visualization\nimages.canfar.net/skaha/carta:3.0\n\n# Custom radio astronomy stack\nimages.canfar.net/skaha/radio-submm:latest\n</code></pre>"},{"location":"platform/guides/radio-astronomy/#casa-usage-examples","title":"CASA Usage Examples","text":""},{"location":"platform/guides/radio-astronomy/#basic-data-inspection","title":"Basic Data Inspection","text":"<pre><code># In a CASA-enabled Python environment\nimport casa_tools as tools\nimport casa_tasks as tasks\n\n# List observation details\ntasks.listobs(vis=\"observation.ms\", verbose=True)\n\n# Plot UV coverage\ntasks.plotuv(vis=\"observation.ms\")\n\n# Check data quality\ntasks.plotms(vis=\"observation.ms\", xaxis=\"time\", yaxis=\"amp\", coloraxis=\"antenna1\")\n</code></pre>"},{"location":"platform/guides/radio-astronomy/#calibration-pipeline","title":"Calibration Pipeline","text":"<pre><code># Typical CASA calibration sequence\nimport os\nfrom casa_tasks import *\n\n# Set up paths\nmsname = \"observation.ms\"\ncaldir = \"calibration/\"\nos.makedirs(caldir, exist_ok=True)\n\n# 1. Flagging\nflagdata(vis=msname, mode=\"manual\", antenna=\"ant5\")  # Bad antenna\nflagdata(vis=msname, mode=\"tfcrop\", datacolumn=\"data\")  # RFI\n\n# 2. Set flux scale for calibrator\nsetjy(vis=msname, field=\"3C273\", standard=\"Perley-Butler-2017\")\n\n# 3. Bandpass calibration\nbandpass(\n    vis=msname,\n    caltable=caldir + \"bandpass.bcal\",\n    field=\"3C273\",\n    refant=\"ant1\",\n    solint=\"inf\",\n)\n\n# 4. Gain calibration\ngaincal(\n    vis=msname,\n    caltable=caldir + \"phase.gcal\",\n    field=\"3C273\",\n    calmode=\"p\",\n    refant=\"ant1\",\n    solint=\"int\",\n)\n\ngaincal(\n    vis=msname,\n    caltable=caldir + \"amp.gcal\",\n    field=\"3C273\",\n    calmode=\"ap\",\n    refant=\"ant1\",\n    solint=\"10min\",\n    gaintable=[caldir + \"bandpass.bcal\", caldir + \"phase.gcal\"],\n)\n\n# 5. Apply calibration\napplycal(\n    vis=msname,\n    field=\"target\",\n    gaintable=[caldir + \"bandpass.bcal\", caldir + \"phase.gcal\", caldir + \"amp.gcal\"],\n)\n</code></pre>"},{"location":"platform/guides/radio-astronomy/#imaging","title":"Imaging","text":"<pre><code># Create images with tclean\ntclean(\n    vis=\"observation.ms\",\n    imagename=\"target_image\",\n    field=\"target\",\n    imsize=1024,\n    cell=\"0.1arcsec\",\n    weighting=\"briggs\",\n    robust=0.0,\n    niter=1000,\n    threshold=\"0.1mJy\",\n    interactive=False,\n)\n\n# Create moment maps for spectral line data\nimmoments(imagename=\"target_image.image\", moments=[0, 1, 2], outfile=\"target_moments\")\n</code></pre>"},{"location":"platform/guides/radio-astronomy/#spectral-line-analysis","title":"Spectral Line Analysis","text":"<pre><code># Extract spectral profiles\nimval(imagename=\"datacube.image\", region=\"circle[[12h30m45s, -30d15m30s], 5arcsec]\")\n\n# Create position-velocity diagrams\nimpv(\n    imagename=\"datacube.image\",\n    outfile=\"pv_diagram.image\",\n    mode=\"coords\",\n    start=\"12h30m40s -30d15m30s\",\n    end=\"12h30m50s -30d15m30s\",\n    width=\"2arcsec\",\n)\n</code></pre>"},{"location":"platform/guides/radio-astronomy/#carta-for-radio-data","title":"CARTA for Radio Data","text":""},{"location":"platform/guides/radio-astronomy/#loading-data-cubes","title":"Loading Data Cubes","text":"<p>CARTA excels at visualizing radio data cubes:</p> <ol> <li>Launch CARTA session: Use CARTA 3.0 for best performance</li> <li>Load cube: Open your CASA image or FITS cube</li> <li>Navigate channels: Use channel slider for frequency/velocity</li> <li>Create animations: Generate movies through the cube</li> </ol>"},{"location":"platform/guides/radio-astronomy/#advanced-carta-features","title":"Advanced CARTA Features","text":"<p>Moment map generation: - Calculate moment 0, 1, and 2 maps interactively - Define custom velocity ranges - Export results for further analysis</p> <p>Spectral analysis: - Click on pixels to see spectra - Fit Gaussian profiles to lines - Measure line properties (velocity, width, flux)</p> <p>Region analysis: - Define analysis regions - Extract statistical properties - Generate publication-quality plots</p>"},{"location":"platform/guides/radio-astronomy/#large-scale-radio-processing","title":"Large-Scale Radio Processing","text":""},{"location":"platform/guides/radio-astronomy/#batch-processing-strategies","title":"Batch Processing Strategies","text":"<p>For large surveys or multi-epoch observations:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nBatch process multiple observations\n\"\"\"\n\nimport os\nimport glob\nfrom casa_tasks import *\n\n\ndef process_observation(msname):\n    \"\"\"Process a single measurement set\"\"\"\n\n    print(f\"Processing {msname}\")\n\n    # Create output directories\n    caldir = f\"{msname}.cal/\"\n    imgdir = f\"{msname}.img/\"\n    os.makedirs(caldir, exist_ok=True)\n    os.makedirs(imgdir, exist_ok=True)\n\n    try:\n        # Calibration\n        calibrate_data(msname, caldir)\n\n        # Imaging\n        image_data(msname, imgdir)\n\n        print(f\"Completed {msname}\")\n\n    except Exception as e:\n        print(f\"Error processing {msname}: {e}\")\n\n\ndef calibrate_data(msname, caldir):\n    \"\"\"Standard calibration pipeline\"\"\"\n\n    # Flagging\n    flagdata(vis=msname, mode=\"tfcrop\")\n\n    # Calibration steps (simplified)\n    bandpass(vis=msname, caltable=f\"{caldir}/bandpass.bcal\")\n    gaincal(vis=msname, caltable=f\"{caldir}/phase.gcal\")\n    applycal(vis=msname, gaintable=[f\"{caldir}/bandpass.bcal\"])\n\n\ndef image_data(msname, imgdir):\n    \"\"\"Create standard images\"\"\"\n\n    # Continuum image\n    tclean(vis=msname, imagename=f\"{imgdir}/continuum\", niter=1000, threshold=\"0.1mJy\")\n\n    # Spectral cube (if line data)\n    tclean(vis=msname, imagename=f\"{imgdir}/cube\", specmode=\"cube\", niter=500)\n\n\n# Main processing loop\nif __name__ == \"__main__\":\n\n    # Find all measurement sets\n    ms_files = glob.glob(\"/arc/projects/survey/data/*.ms\")\n\n    for msname in ms_files:\n        process_observation(msname)\n\n    print(\"Batch processing complete\")\n</code></pre>"},{"location":"platform/guides/radio-astronomy/#parallel-processing","title":"Parallel Processing","text":"<p>For very large datasets, use parallel processing:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nParallel radio data processing\n\"\"\"\n\nfrom multiprocessing import Pool\nimport functools\n\n\ndef process_with_casa(msname):\n    \"\"\"Process single MS with CASA\"\"\"\n\n    # Import CASA tools in subprocess\n    import casa_tasks as tasks\n\n    # Your processing code here\n    tasks.flagdata(vis=msname, mode=\"tfcrop\")\n    # ... rest of processing\n\n    return f\"Processed {msname}\"\n\n\ndef main():\n    # List of measurement sets\n    ms_files = [\n        \"/arc/projects/survey/data/obs1.ms\",\n        \"/arc/projects/survey/data/obs2.ms\",\n        \"/arc/projects/survey/data/obs3.ms\",\n    ]\n\n    # Process in parallel\n    with Pool(processes=4) as pool:\n        results = pool.map(process_with_casa, ms_files)\n\n    for result in results:\n        print(result)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Batch vs Interactive</p> <p>Use interactive sessions for development and QA of pipelines, then switch to batch jobs for production processing of many datasets.</p>"},{"location":"platform/guides/radio-astronomy/#data-management-for-radio-astronomy","title":"Data Management for Radio Astronomy","text":""},{"location":"platform/guides/radio-astronomy/#storage-strategy","title":"Storage Strategy","text":"<p>Radio astronomy data requires careful storage planning:</p> <p>Raw data: Store in <code>/arc/projects/[group]/raw/</code> - Original measurement sets - Backup copies of critical observations - Metadata and observation logs</p> <p>Processed data: Organize in <code>/arc/projects/[group]/processed/</code> - Calibrated measurement sets - Final images and cubes - Derived products (catalogs, moment maps)</p> <p>Results: Save to <code>/arc/projects/[group]/results/</code> - Publication-ready images - Scientific catalogs - Analysis scripts and notebooks</p>"},{"location":"platform/guides/radio-astronomy/#file-organization","title":"File Organization","text":"<pre><code>/arc/projects/radio-survey/\n\u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 2024/\n\u2502   \u2502   \u251c\u2500\u2500 jan/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 obs_20240115.ms\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 obs_20240116.ms\n\u2502   \u2502   \u2514\u2500\u2500 feb/\n\u2502   \u2514\u2500\u2500 2023/\n\u251c\u2500\u2500 processed/\n\u2502   \u251c\u2500\u2500 calibrated/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2514\u2500\u2500 cubes/\n\u251c\u2500\u2500 results/\n\u2502   \u251c\u2500\u2500 catalogs/\n\u2502   \u251c\u2500\u2500 plots/\n\u2502   \u2514\u2500\u2500 papers/\n\u2514\u2500\u2500 scripts/\n    \u251c\u2500\u2500 calibration/\n    \u251c\u2500\u2500 imaging/\n    \u2514\u2500\u2500 analysis/\n</code></pre> <p>Data Hygiene</p> <p>Maintain clear directory structures, version your scripts, and document parameters for reproducibility.</p>"},{"location":"platform/guides/radio-astronomy/#data-transfer","title":"Data Transfer","text":"<p>For large radio datasets:</p> <p>From observatories: - Use <code>rsync</code> for efficient transfer - Transfer during off-peak hours - Verify data integrity with checksums</p> <p>Between CANFAR and external systems: - Use VOSpace for large files - Consider data compression - Plan transfer schedules</p>"},{"location":"platform/guides/radio-astronomy/#performance-optimization","title":"Performance Optimization","text":""},{"location":"platform/guides/radio-astronomy/#memory-management","title":"Memory Management","text":"<p>Radio data processing is memory-intensive:</p> <pre><code># Monitor memory usage in CASA\nimport psutil\nimport os\n\n\ndef check_memory():\n    \"\"\"Check current memory usage\"\"\"\n    process = psutil.Process(os.getpid())\n    memory_gb = process.memory_info().rss / 1024**3\n    print(f\"Memory usage: {memory_gb:.1f} GB\")\n\n\n# Use memory-efficient processing\ncheck_memory()\ntclean(\n    vis=\"large_dataset.ms\",\n    imagename=\"output\",\n    # Use smaller chunk sizes for large data\n    parallel=True,\n    pbcor=False,\n)  # Skip if not needed\ncheck_memory()\n</code></pre>"},{"location":"platform/guides/radio-astronomy/#disk-io-optimization","title":"Disk I/O Optimization","text":"<pre><code># Use scratch space for temporary files\nimport tempfile\nimport shutil\n\n# Create temporary directory in scratch space\nwith tempfile.TemporaryDirectory(dir=\"/tmp\") as tmpdir:\n\n    # Copy data to scratch for processing\n    scratch_ms = f\"{tmpdir}/working.ms\"\n    shutil.copytree(\"original.ms\", scratch_ms)\n\n    # Process on fast scratch storage\n    tclean(vis=scratch_ms, imagename=f\"{tmpdir}/temp_image\")\n\n    # Copy results back to persistent storage\n    shutil.copy(f\"{tmpdir}/temp_image.image\", \"/arc/projects/myproject/results/\")\n</code></pre>"},{"location":"platform/guides/radio-astronomy/#troubleshooting-radio-workflows","title":"Troubleshooting Radio Workflows","text":""},{"location":"platform/guides/radio-astronomy/#common-casa-issues","title":"Common CASA Issues","text":"<p>Memory errors: - Reduce image size or increase session memory - Use chunked processing for large datasets - Clear CASA cache regularly</p> <p>Calibration failures: - Check data quality with <code>plotms</code> - Verify calibrator flux scales - Inspect antenna flagging</p> <p>Imaging problems: - Adjust <code>tclean</code> parameters - Check UV coverage and weighting - Verify coordinate systems</p>"},{"location":"platform/guides/radio-astronomy/#performance-issues","title":"Performance Issues","text":"<p>Slow processing: - Use appropriate number of CPU cores - Enable parallel processing where available - Monitor system resources</p> <p>Disk space problems: - Clean up temporary files regularly - Use scratch space for intermediate products - Compress completed datasets</p>"},{"location":"platform/guides/radio-astronomy/#getting-help","title":"Getting Help","text":"<p>CASA Documentation: - CASA Guides - CASA Reference Manual</p> <p>CANFAR Support: - Email support@canfar.net - Discord community for peer support - Check existing tutorials and examples</p>"},{"location":"platform/guides/radio-astronomy/#best-practices","title":"Best Practices","text":""},{"location":"platform/guides/radio-astronomy/#workflow-documentation","title":"Workflow Documentation","text":"<ul> <li>Script versioning: Use Git for analysis scripts</li> <li>Parameter logging: Record all processing parameters</li> <li>Provenance tracking: Document data lineage</li> <li>Reproducibility: Ensure workflows can be repeated</li> </ul>"},{"location":"platform/guides/radio-astronomy/#collaboration","title":"Collaboration","text":"<ul> <li>Shared scripts: Develop reusable pipeline components</li> <li>Session sharing: Collaborate on interactive analysis</li> <li>Result sharing: Use consistent output formats</li> <li>Knowledge sharing: Document lessons learned</li> </ul>"},{"location":"platform/guides/radio-astronomy/#quality-control","title":"Quality Control","text":"<ul> <li>Validation steps: Include quality checks in pipelines</li> <li>Sanity checks: Verify results at each stage</li> <li>Backup strategies: Protect critical intermediate products</li> <li>Error handling: Robust error recovery in automated pipelines</li> </ul>"},{"location":"platform/guides/radio-astronomy/casa-workflows/","title":"CASA Desktop Workflows","text":"<p>Advanced CASA workflows and desktop integration for radio astronomy data analysis on the CANFAR Science Platform.</p> <p>\ud83c\udfaf What You'll Learn</p> <ul> <li>How to launch and work efficiently with CASA in desktop sessions</li> <li>Advanced calibration, imaging, and self-calibration workflows</li> <li>Integration patterns with DS9 and CARTA</li> <li>Parallel strategies, memory management, and automation</li> </ul>"},{"location":"platform/guides/radio-astronomy/casa-workflows/#overview","title":"Overview","text":"<p>This guide covers sophisticated CASA workflows that leverage the desktop environment for complex analysis, visualization, and data processing tasks.</p>"},{"location":"platform/guides/radio-astronomy/casa-workflows/#desktop-casa-setup","title":"Desktop CASA Setup","text":""},{"location":"platform/guides/radio-astronomy/casa-workflows/#launching-casa-desktop","title":"Launching CASA Desktop","text":"<ol> <li>Start Desktop Session:</li> <li>Go to CANFAR Portal</li> <li>Click \"Desktop\" session</li> <li>Choose <code>radio-astronomy/casa</code> container</li> <li> <p>Set resources: 4+ cores, 8+ GB RAM for large datasets</p> </li> <li> <p>Open CASA:</p> </li> <li>Click desktop \"Applications\" menu</li> <li>Navigate to \"Science\" \u2192 \"CASA\"</li> <li>Or open terminal and type <code>casa</code></li> </ol> <p>Persistence Reminder</p> <p>Save important results to <code>/arc/projects/</code> or <code>/arc/home/</code>. Temporary locations or session-local directories may not persist after session end.</p>"},{"location":"platform/guides/radio-astronomy/casa-workflows/#desktop-environment-advantages","title":"Desktop Environment Advantages","text":"<ul> <li>Multiple CASA sessions: Run parallel analysis tasks</li> <li>GUI tools: Access plotms, casaviewer, casabrowser</li> <li>File management: Visual file browser with FITS preview</li> <li>External tools: Use ds9, CARTA, Python IDEs alongside CASA</li> <li>Session persistence: Keep analysis state across disconnections</li> </ul>"},{"location":"platform/guides/radio-astronomy/casa-workflows/#advanced-analysis-workflows","title":"Advanced Analysis Workflows","text":""},{"location":"platform/guides/radio-astronomy/casa-workflows/#multi-dataset-calibration-pipeline","title":"Multi-Dataset Calibration Pipeline","text":"<pre><code># casa_calibration_pipeline.py\nimport os\nimport glob\n\n\ndef full_calibration_pipeline(raw_data_dir, output_dir):\n    \"\"\"Complete calibration pipeline for multiple datasets\"\"\"\n\n    # Setup directories\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Find all measurement sets\n    ms_list = glob.glob(f\"{raw_data_dir}/*.ms\")\n\n    for ms in ms_list:\n        print(f\"Processing {ms}\")\n\n        # 1. Initial flagging\n        flagdata(vis=ms, mode=\"manual\", autocorr=True, flagbackup=True)\n\n        # 2. Set flux scale\n        setjy(vis=ms, field=\"3C48\", standard=\"Perley-Butler-2017\")  # Calibrator\n\n        # 3. Bandpass calibration\n        bp_table = f\"{output_dir}/{os.path.basename(ms)}.B1\"\n        bandpass(vis=ms, caltable=bp_table, field=\"3C48\", solint=\"inf\", combine=\"scan\")\n\n        # 4. Phase calibration\n        phase_table = f\"{output_dir}/{os.path.basename(ms)}.G1\"\n        gaincal(\n            vis=ms,\n            caltable=phase_table,\n            field=\"3C48\",\n            calmode=\"p\",\n            solint=\"int\",\n            gaintable=[bp_table],\n        )\n\n        # 5. Amplitude calibration\n        amp_table = f\"{output_dir}/{os.path.basename(ms)}.G2\"\n        gaincal(\n            vis=ms,\n            caltable=amp_table,\n            field=\"3C48\",\n            calmode=\"ap\",\n            solint=\"inf\",\n            gaintable=[bp_table, phase_table],\n        )\n\n        # 6. Apply calibrations\n        applycal(\n            vis=ms,\n            field=\"\",  # All fields\n            gaintable=[bp_table, phase_table, amp_table],\n            gainfield=[\"3C48\", \"3C48\", \"3C48\"],\n        )\n\n        # 7. Split calibrated data\n        output_ms = f\"{output_dir}/{os.path.basename(ms)}_calibrated.ms\"\n        split(vis=ms, outputvis=output_ms, datacolumn=\"corrected\")\n\n        print(f\"Calibration complete: {output_ms}\")\n\n\n# Run pipeline\nfull_calibration_pipeline(\n    \"/arc/projects/survey/raw/\", \"/arc/projects/survey/calibrated/\"\n)\n</code></pre>"},{"location":"platform/guides/radio-astronomy/casa-workflows/#imaging-workflow-with-quality-assessment","title":"Imaging Workflow with Quality Assessment","text":"<pre><code># casa_imaging_workflow.py\n\n\ndef imaging_workflow_with_qa(ms_file, target_field, output_dir):\n    \"\"\"Comprehensive imaging with quality assessment\"\"\"\n\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    base_name = f\"{output_dir}/{target_field}\"\n\n    # 1. Initial dirty image for assessment\n    tclean(\n        vis=ms_file,\n        imagename=f\"{base_name}_dirty\",\n        field=target_field,\n        imsize=[1024, 1024],\n        cell=[\"2arcsec\"],\n        niter=0,  # Dirty image\n        deconvolver=\"hogbom\",\n    )\n\n    # 2. Analyze dirty image for parameters\n    ia.open(f\"{base_name}_dirty.image\")\n    stats = ia.statistics()\n    peak = stats[\"max\"][0]\n    rms = stats[\"rms\"][0]\n    dynamic_range = peak / rms\n    ia.close()\n\n    print(f\"Dirty image stats:\")\n    print(f\"  Peak: {peak:.3e} Jy/beam\")\n    print(f\"  RMS: {rms:.3e} Jy/beam\")\n    print(f\"  Dynamic range: {dynamic_range:.1f}\")\n\n    # 3. Determine cleaning parameters\n    threshold = 3 * rms  # 3-sigma threshold\n    niter = min(10000, int(dynamic_range * 100))\n\n    # 4. Clean image\n    tclean(\n        vis=ms_file,\n        imagename=f\"{base_name}_clean\",\n        field=target_field,\n        imsize=[1024, 1024],\n        cell=[\"2arcsec\"],\n        niter=niter,\n        threshold=f\"{threshold}Jy\",\n        deconvolver=\"multiscale\",\n        scales=[0, 6, 18],\n        gain=0.1,\n        cycleniter=1000,\n        usemask=\"auto-multithresh\",\n        interactive=False,\n    )\n\n    # 5. Primary beam correction\n    impbcor(\n        imagename=f\"{base_name}_clean.image\",\n        pbimage=f\"{base_name}_clean.pb\",\n        outfile=f\"{base_name}_clean.pbcor\",\n        overwrite=True,\n    )\n\n    # 6. Generate diagnostic plots\n    create_qa_plots(f\"{base_name}_clean\", target_field)\n\n    print(f\"Imaging complete: {base_name}_clean.pbcor\")\n\n\ndef create_qa_plots(image_base, field_name):\n    \"\"\"Create quality assessment plots\"\"\"\n\n    # Import CASA plotting tools\n    from casaplotms import plotms\n\n    # 1. uv-coverage plot\n    plotms(\n        vis=f\"{image_base}.ms\",\n        xaxis=\"uvdist\",\n        yaxis=\"amp\",\n        avgtime=\"60s\",\n        plotfile=f\"{image_base}_uvcoverage.png\",\n        showgui=False,\n    )\n\n    # 2. Image histogram\n    ia.open(f\"{image_base}.image\")\n    pixels = ia.getchunk()\n    ia.close()\n\n    plt.figure(figsize=(10, 6))\n    plt.hist(pixels.flatten(), bins=100, alpha=0.7)\n    plt.xlabel(\"Intensity (Jy/beam)\")\n    plt.ylabel(\"Number of pixels\")\n    plt.title(f\"Intensity Distribution - {field_name}\")\n    plt.yscale(\"log\")\n    plt.savefig(f\"{image_base}_histogram.png\")\n    plt.close()\n\n    print(f\"QA plots saved: {image_base}_*.png\")\n\n\n# Example usage\nimaging_workflow_with_qa(\"calibrated_data.ms\", \"M31\", \"/arc/projects/m31/images/\")\n</code></pre>"},{"location":"platform/guides/radio-astronomy/casa-workflows/#self-calibration-loop","title":"Self-Calibration Loop","text":"<pre><code># casa_selfcal.py\n\n\ndef self_calibration_loop(\n    ms_file, source_field, output_dir, nloops=3, gain_solint=[\"inf\", \"30s\", \"10s\"]\n):\n    \"\"\"Iterative self-calibration\"\"\"\n\n    base_name = f\"{output_dir}/{source_field}\"\n\n    # Initial image\n    current_ms = ms_file\n\n    for loop in range(nloops):\n        print(f\"\\n=== Self-cal loop {loop + 1} ===\")\n\n        # 1. Image current data\n        img_name = f\"{base_name}_selfcal{loop}\"\n        tclean(\n            vis=current_ms,\n            imagename=img_name,\n            field=source_field,\n            imsize=[512, 512],\n            cell=[\"1arcsec\"],\n            niter=5000,\n            threshold=\"0.1mJy\",\n            deconvolver=\"hogbom\",\n            interactive=False,\n        )\n\n        # 2. Calculate gain solutions\n        cal_table = f\"{base_name}_selfcal{loop}.gcal\"\n        gaincal(\n            vis=current_ms,\n            caltable=cal_table,\n            field=source_field,\n            gaintype=\"G\",\n            calmode=\"p\",  # Phase-only first loops\n            solint=gain_solint[min(loop, len(gain_solint) - 1)],\n            refant=\"auto\",\n        )\n\n        # 3. Apply calibration\n        applycal(vis=current_ms, field=source_field, gaintable=[cal_table])\n\n        # 4. Split corrected data for next loop\n        if loop &lt; nloops - 1:\n            next_ms = f\"{base_name}_selfcal{loop+1}.ms\"\n            split(\n                vis=current_ms,\n                outputvis=next_ms,\n                field=source_field,\n                datacolumn=\"corrected\",\n            )\n            current_ms = next_ms\n\n        # 5. Assess improvement\n        assess_selfcal_improvement(img_name, loop)\n\n\ndef assess_selfcal_improvement(image_name, loop):\n    \"\"\"Assess self-calibration improvement\"\"\"\n\n    ia.open(f\"{image_name}.image\")\n    stats = ia.statistics()\n    peak = stats[\"max\"][0]\n    rms = stats[\"rms\"][0]\n    ia.close()\n\n    print(f\"Loop {loop + 1} results:\")\n    print(f\"  Peak: {peak*1000:.2f} mJy/beam\")\n    print(f\"  RMS: {rms*1000:.2f} mJy/beam\")\n    print(f\"  S/N: {peak/rms:.1f}\")\n\n\n# Run self-calibration\nself_calibration_loop(\"target_data.ms\", \"NGC1068\", \"/arc/projects/agn/selfcal/\")\n</code></pre>"},{"location":"platform/guides/radio-astronomy/casa-workflows/#integration-with-external-tools","title":"Integration with External Tools","text":""},{"location":"platform/guides/radio-astronomy/casa-workflows/#casa-ds9-workflow","title":"CASA + DS9 Workflow","text":"<pre><code># casa_ds9_integration.py\n\n\ndef casa_to_ds9_analysis(casa_image, region_file=None):\n    \"\"\"Export CASA image to DS9 for detailed analysis\"\"\"\n\n    # 1. Export to FITS\n    fits_name = casa_image.replace(\".image\", \".fits\")\n    exportfits(imagename=casa_image, fitsimage=fits_name, overwrite=True)\n\n    # 2. Launch DS9 with image\n    os.system(f\"ds9 {fits_name} &amp;\")\n\n    # 3. If region file provided, load it\n    if region_file:\n        print(f\"Load region file {region_file} in DS9\")\n        print(\"File \u2192 Region \u2192 Load\")\n\n    # 4. Return to CASA for further analysis\n    print(f\"Image {fits_name} loaded in DS9\")\n    print(\"Create regions in DS9, save as .reg file\")\n    print(\"Use importuvfits() to load back to CASA if needed\")\n\n\n# Usage\ncasa_to_ds9_analysis(\"M31_clean.image\", \"spiral_arms.reg\")\n</code></pre>"},{"location":"platform/guides/radio-astronomy/casa-workflows/#casa-carta-integration","title":"CASA + CARTA Integration","text":"<pre><code># casa_carta_workflow.py\n\n\ndef casa_carta_cube_analysis(cube_ms, output_cube):\n    \"\"\"Create spectral cube for CARTA analysis\"\"\"\n\n    # 1. Create spectral cube\n    tclean(\n        vis=cube_ms,\n        imagename=output_cube,\n        imsize=[256, 256, 1, 100],  # spatial + spectral\n        cell=[\"2arcsec\"],\n        niter=1000,\n        deconvolver=\"hogbom\",\n        specmode=\"cube\",\n        start=\"1.4GHz\",\n        width=\"1MHz\",\n        nchan=100,\n    )\n\n    # 2. Export for CARTA\n    fits_cube = f\"{output_cube}.fits\"\n    exportfits(imagename=f\"{output_cube}.image\", fitsimage=fits_cube, overwrite=True)\n\n    # 3. Launch CARTA\n    print(f\"Load {fits_cube} in CARTA for:\")\n    print(\"- Spectral profile analysis\")\n    print(\"- Moment map generation\")\n    print(\"- Region statistics\")\n    print(\"- 3D visualization\")\n\n    return fits_cube\n\n\n# Create cube for analysis\ncube_fits = casa_carta_cube_analysis(\"line_data.ms\", \"HI_cube\")\n</code></pre>"},{"location":"platform/guides/radio-astronomy/casa-workflows/#parallel-processing-strategies","title":"Parallel Processing Strategies","text":""},{"location":"platform/guides/radio-astronomy/casa-workflows/#multi-core-casa-operations","title":"Multi-Core CASA Operations","text":"<pre><code># casa_parallel.py\nimport multiprocessing as mp\nimport os\n\n\ndef parallel_imaging(ms_list, output_dir, ncores=None):\n    \"\"\"Image multiple datasets in parallel\"\"\"\n\n    if ncores is None:\n        ncores = mp.cpu_count() - 1\n\n    def image_single_ms(ms_file):\n        \"\"\"Image a single measurement set\"\"\"\n        base_name = os.path.basename(ms_file).replace(\".ms\", \"\")\n\n        tclean(\n            vis=ms_file,\n            imagename=f\"{output_dir}/{base_name}\",\n            imsize=[512, 512],\n            cell=[\"2arcsec\"],\n            niter=1000,\n            deconvolver=\"hogbom\",\n        )\n\n        return f\"Completed: {base_name}\"\n\n    # Use multiprocessing\n    with mp.Pool(ncores) as pool:\n        results = pool.map(image_single_ms, ms_list)\n\n    for result in results:\n        print(result)\n\n\n# Process multiple observations\nms_files = glob.glob(\"/arc/projects/survey/*.ms\")\nparallel_imaging(ms_files, \"/arc/projects/survey/images/\", ncores=4)\n</code></pre>"},{"location":"platform/guides/radio-astronomy/casa-workflows/#memory-efficient-large-dataset-processing","title":"Memory-Efficient Large Dataset Processing","text":"<pre><code># casa_memory_efficient.py\n\n\ndef process_large_cube(input_cube, chunk_size=10):\n    \"\"\"Process large spectral cubes in chunks\"\"\"\n\n    # Get cube dimensions\n    ia.open(input_cube)\n    shape = ia.shape()\n    nchans = shape[3]\n    ia.close()\n\n    # Process in chunks\n    for start_chan in range(0, nchans, chunk_size):\n        end_chan = min(start_chan + chunk_size, nchans)\n\n        print(f\"Processing channels {start_chan}-{end_chan}\")\n\n        # Extract channel subset\n        chunk_name = f\"temp_chunk_{start_chan}_{end_chan}\"\n        imsubimage(\n            imagename=input_cube,\n            outfile=chunk_name,\n            box=\"\",\n            chans=f\"{start_chan}~{end_chan}\",\n        )\n\n        # Process chunk (example: smooth)\n        imsmooth(\n            imagename=chunk_name,\n            outfile=f\"{chunk_name}_smooth\",\n            kernel=\"gauss\",\n            major=\"3arcsec\",\n            minor=\"3arcsec\",\n        )\n\n        # Clean up temporary files\n        os.system(f\"rm -rf {chunk_name}\")\n\n        print(f\"Chunk {start_chan}-{end_chan} complete\")\n\n\n# Process 1000-channel cube in 50-channel chunks\nprocess_large_cube(\"large_HI_cube.image\", chunk_size=50)\n</code></pre> <p>Performance Planning</p> <p>Imaging and spectral cubes can be resource-intensive. Plan for sufficient RAM and use chunking or parallelization where possible.</p>"},{"location":"platform/guides/radio-astronomy/casa-workflows/#automation-and-scripting","title":"Automation and Scripting","text":""},{"location":"platform/guides/radio-astronomy/casa-workflows/#batch-processing-script","title":"Batch Processing Script","text":"<pre><code>#!/bin/bash\n# casa_batch_processing.sh\n\n# Set up environment\nexport OMP_NUM_THREADS=4\nexport CASA_ENGINE_LOG_LEVEL=WARN\n\n# Define directories\nRAW_DIR=\"/arc/projects/survey/raw\"\nCAL_DIR=\"/arc/projects/survey/calibrated\"  \nIMG_DIR=\"/arc/projects/survey/images\"\n\n# Create output directories\nmkdir -p $CAL_DIR $IMG_DIR\n\n# Process all measurement sets\nfor ms_file in $RAW_DIR/*.ms; do\n    echo \"Processing $(basename $ms_file)\"\n\n    # Run CASA calibration\n    casa --nogui -c \"execfile('calibration_pipeline.py'); \\\n                     full_calibration_pipeline('$ms_file', '$CAL_DIR')\"\n\n    # Run imaging\n    casa --nogui -c \"execfile('imaging_workflow.py'); \\\n                     imaging_workflow('$CAL_DIR/$(basename $ms_file .ms)_cal.ms', '$IMG_DIR')\"\ndone\n\necho \"Batch processing complete\"\n</code></pre>"},{"location":"platform/guides/radio-astronomy/casa-workflows/#quality-assessment-dashboard","title":"Quality Assessment Dashboard","text":"<pre><code># casa_qa_dashboard.py\nimport glob\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef generate_qa_dashboard(image_dir, output_html):\n    \"\"\"Generate HTML quality assessment dashboard\"\"\"\n\n    images = glob.glob(f\"{image_dir}/*.image\")\n\n    html_content = \"\"\"\n    &lt;html&gt;\n    &lt;head&gt;&lt;title&gt;CASA Processing QA Dashboard&lt;/title&gt;&lt;/head&gt;\n    &lt;body&gt;\n    &lt;h1&gt;Quality Assessment Dashboard&lt;/h1&gt;\n    &lt;table border=\"1\"&gt;\n    &lt;tr&gt;&lt;th&gt;Image&lt;/th&gt;&lt;th&gt;Peak (mJy)&lt;/th&gt;&lt;th&gt;RMS (mJy)&lt;/th&gt;&lt;th&gt;Dynamic Range&lt;/th&gt;&lt;/tr&gt;\n    \"\"\"\n\n    for image in images:\n        # Get statistics\n        ia.open(image)\n        stats = ia.statistics()\n        peak = stats[\"max\"][0] * 1000  # Convert to mJy\n        rms = stats[\"rms\"][0] * 1000\n        dynamic_range = peak / rms\n        ia.close()\n\n        # Add to HTML\n        base_name = os.path.basename(image)\n        html_content += f\"\"\"\n        &lt;tr&gt;\n        &lt;td&gt;{base_name}&lt;/td&gt;\n        &lt;td&gt;{peak:.2f}&lt;/td&gt;\n        &lt;td&gt;{rms:.2f}&lt;/td&gt;\n        &lt;td&gt;{dynamic_range:.1f}&lt;/td&gt;\n        &lt;/tr&gt;\n        \"\"\"\n\n    html_content += \"\"\"\n    &lt;/table&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\n\n    with open(output_html, \"w\") as f:\n        f.write(html_content)\n\n    print(f\"QA dashboard saved: {output_html}\")\n\n\n# Generate dashboard\ngenerate_qa_dashboard(\n    \"/arc/projects/survey/images/\", \"/arc/projects/survey/qa_dashboard.html\"\n)\n</code></pre>"},{"location":"platform/guides/radio-astronomy/casa-workflows/#best-practices","title":"Best Practices","text":""},{"location":"platform/guides/radio-astronomy/casa-workflows/#resource-management","title":"Resource Management","text":"<ul> <li>Memory allocation: Reserve 2GB per core for imaging</li> <li>Disk space: Allow 5x input data size for intermediate files</li> <li>Parallel processing: Use n-1 cores to maintain system responsiveness</li> <li>Cleanup: Remove temporary files regularly</li> </ul>"},{"location":"platform/guides/radio-astronomy/casa-workflows/#workflow-organization","title":"Workflow Organization","text":"<pre><code># Directory structure for complex projects\nproject_structure = {\n    \"raw/\": \"Original measurement sets\",\n    \"calibrated/\": \"Calibrated data\",\n    \"images/\": \"Final images\",\n    \"scripts/\": \"Processing scripts\",\n    \"logs/\": \"Processing logs\",\n    \"qa/\": \"Quality assessment outputs\",\n    \"docs/\": \"Analysis documentation\",\n}\n</code></pre>"},{"location":"platform/guides/radio-astronomy/casa-workflows/#error-handling","title":"Error Handling","text":"<pre><code>def robust_casa_operation(operation_func, *args, max_retries=3, **kwargs):\n    \"\"\"Execute CASA operation with error handling\"\"\"\n\n    for attempt in range(max_retries):\n        try:\n            result = operation_func(*args, **kwargs)\n            return result\n        except Exception as e:\n            print(f\"Attempt {attempt + 1} failed: {e}\")\n            if attempt &lt; max_retries - 1:\n                print(\"Retrying...\")\n                time.sleep(5)\n            else:\n                print(\"Operation failed after all retries\")\n                raise e\n\n\n# Usage\nrobust_casa_operation(tclean, vis=\"data.ms\", imagename=\"output\", niter=1000)\n</code></pre>"},{"location":"platform/guides/storage/","title":"CANFAR Storage Systems","text":"<p>Master CANFAR's storage systems for efficient data management</p> <p>\ud83c\udfaf What You'll Learn</p> <p>By the end of this guide, you'll understand: - The different storage systems available on CANFAR - When and how to use each storage type for your research - Best practices for data management, transfer, and backup - How to optimize your workflow for performance and data safety</p> <p>CANFAR provides multiple storage systems optimized for different stages of your research workflow. Understanding when and how to use each storage type is crucial for efficient data analysis and collaboration.</p>"},{"location":"platform/guides/storage/#types-comparison","title":"\ud83d\udcca Types Comparison","text":"Storage Mount Path Speed Persistence Backup Quota Best For ARC Projects <code>/arc/projects/group/</code> Fast SSD \u2705 Permanent \u2705 Daily snapshots Project-based Active research, shared data ARC Home <code>/arc/home/username/</code> Fast SSD \u2705 Permanent \u2705 Daily snapshots 10GB default Personal configs, keys Scratch <code>/scratch/</code> Fastest NVMe \u274c Wiped at session end \u274c No backup Unlimited Temporary processing VOSpace <code>vos:username/</code> Medium \u2705 Permanent \u2705 Geo-redundant User/project based Archives, public data"},{"location":"platform/guides/storage/#storage-lifecycle-overview","title":"\ud83d\uddfa\ufe0f Storage Lifecycle Overview","text":"<p>CANFAR's storage architecture is designed around the astronomy research lifecycle:</p> <pre><code>graph LR\n    Archive[\"\ud83d\udce6 External Archives&lt;br/&gt;ALMA, HST, etc.\"] \n    Download[\"\u2b07\ufe0f Download\"]\n    Scratch[\"\u26a1 Scratch Storage&lt;br/&gt;Fast processing\"]\n    Process[\"\ud83d\udd04 Data Processing\"]\n    ARC[\"\ud83d\udcc1 ARC Projects&lt;br/&gt;Shared results\"]\n    VOSpace[\"\u2601\ufe0f VOSpace&lt;br/&gt;Long-term archive\"]\n\n    Archive --&gt; Download\n    Download --&gt; Scratch\n    Scratch --&gt; Process\n    Process --&gt; ARC\n    ARC --&gt; VOSpace\n    ARC -.-&gt; |Backup| Process</code></pre>"},{"location":"platform/guides/storage/#arc-storage","title":"\ud83d\udcc1 ARC Storage","text":"<p>ARC (Advanced Research Computing) storage provides high-performance, persistent storage for active research.</p>"},{"location":"platform/guides/storage/#arcprojectsgroupname-shared-research-storage","title":"<code>/arc/projects/groupname/</code> - Shared Research Storage","text":"<p>When to Use ARC Projects</p> <ul> <li>Raw and processed datasets</li> <li>Analysis scripts and notebooks  </li> <li>Results and publications</li> <li>Shared team resources</li> <li>Collaborative workflows</li> </ul> <p>\ud83d\udd27 Features:</p> <ul> <li>Shared access - All group members can read/write</li> <li>Fast SSD storage - Optimized for data analysis</li> <li>Daily backups - 30-day retention policy</li> <li>ACL support - Fine-grained permission control</li> </ul> <p>\ud83d\udcc2 Recommended Structure:</p> <pre><code>/arc/projects/myproject/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/              # Original observational data\n\u2502   \u251c\u2500\u2500 processed/        # Calibrated/reduced data  \n\u2502   \u251c\u2500\u2500 catalogs/         # Reference catalogs\n\u2502   \u2514\u2500\u2500 simulations/      # Synthetic datasets\n\u251c\u2500\u2500 code/\n\u2502   \u251c\u2500\u2500 pipelines/        # Data processing workflows\n\u2502   \u251c\u2500\u2500 analysis/         # Analysis scripts\n\u2502   \u251c\u2500\u2500 notebooks/        # Jupyter notebooks\n\u2502   \u2514\u2500\u2500 tools/            # Custom utilities\n\u251c\u2500\u2500 results/\n\u2502   \u251c\u2500\u2500 plots/            # Figures and visualizations\n\u2502   \u251c\u2500\u2500 tables/           # Output measurements\n\u2502   \u251c\u2500\u2500 papers/           # Manuscripts and drafts\n\u2502   \u2514\u2500\u2500 presentations/    # Conference materials\n\u2514\u2500\u2500 docs/\n    \u251c\u2500\u2500 README.md         # Project documentation\n    \u251c\u2500\u2500 data_notes.md     # Dataset descriptions\n    \u2514\u2500\u2500 procedures.md     # Analysis procedures\n</code></pre>"},{"location":"platform/guides/storage/#archomeusername-personal-space","title":"<code>/arc/home/username/</code> - Personal Space","text":"<p>When to Use ARC Home</p> <ul> <li>Personal configuration files (<code>.bashrc</code>, <code>.jupyter/</code>)</li> <li>SSH keys and authentication credentials</li> <li>Personal scripts and utilities</li> <li>Small reference files</li> </ul> <p>\u26a0\ufe0f Limitations:</p> <ul> <li>10GB default quota (contact support for increases)</li> <li>Personal access only (not shared)</li> <li>Not suitable for large datasets</li> </ul>"},{"location":"platform/guides/storage/#storage-management","title":"Managing ARC Storage","text":""},{"location":"platform/guides/storage/#check-usage-and-quotas","title":"Check Usage and Quotas","text":"<pre><code># Check project storage usage\ndf -h /arc/projects/myproject\n\n# Detailed usage breakdown\ndu -sh /arc/projects/myproject/*\n\n# Check home directory usage\ndu -sh /arc/home/$USER/*\n\n# Check available space\ndf -h /arc\n</code></pre>"},{"location":"platform/guides/storage/#organizing-data","title":"Organizing Data","text":"<pre><code># Create organized directory structure\nmkdir -p /arc/projects/myproject/{data/{raw,processed,catalogs},code,results,docs}\n\n# Set group permissions for collaboration\nchmod -R g+rw /arc/projects/myproject/\nchmod g+s /arc/projects/myproject/  # Inherit group ownership\n</code></pre>"},{"location":"platform/guides/storage/#backup-and-recovery","title":"Backup and Recovery","text":"<p>ARC Backup</p> <p>ARC storage is automatically backed up daily with a 30-day retention policy. You can also restore files from snapshots if needed.</p> <pre><code># List available snapshots (if enabled)\nls /arc/projects/myproject/.snapshots/\n\n# Restore from snapshot\ncp /arc/projects/myproject/.snapshots/daily.2024-03-15/important_file.fits \\\n   /arc/projects/myproject/restored_file.fits\n</code></pre>"},{"location":"platform/guides/storage/#scratch-storage","title":"\u26a1 Scratch Storage","text":"<p>Scratch provides the fastest storage available on CANFAR, but files are temporary.</p> <p>Important: Scratch Storage Lifecycle</p> <p>Scratch storage is wiped at the end of each session, not nightly as some older documentation stated. When your interactive session ends or your batch job completes, all files in <code>/scratch/</code> are permanently deleted.</p>"},{"location":"platform/guides/storage/#when-to-use-scratch","title":"When to Use Scratch","text":"<p>\u2705 Excellent for: - Large intermediate files during processing - Temporary downloads before organizing in ARC - High I/O operations requiring maximum speed - Uncompressing large archives - Sorting and filtering large datasets</p> <p>\u274c Never use for: - Important results (will be lost!) - Files you need to keep between sessions - Shared data (only accessible within your session)</p>"},{"location":"platform/guides/storage/#scratch-lifecycle","title":"Scratch Lifecycle","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Session as Session/Job\n    participant Scratch as /scratch/\n\n    User-&gt;&gt;Session: Start session\n    Session-&gt;&gt;Scratch: Create empty /scratch/ directory\n    User-&gt;&gt;Scratch: Work with temporary files\n    Note over Scratch: Fast NVMe storage\n    User-&gt;&gt;Session: End session\n    Session-&gt;&gt;Scratch: DELETE ALL FILES\n    Note over Scratch: Directory wiped clean</code></pre>"},{"location":"platform/guides/storage/#scratch-best-practices","title":"Scratch Best Practices","text":"<pre><code># 1. Download large files to scratch first\ncd /scratch\nwget https://archive.alma.cl/large_dataset.tar.gz\n\n# 2. Process immediately\ntar -xzf large_dataset.tar.gz\ncasa --nologger -c \"process_data.py\"\n\n# 3. Save results to permanent storage\ncp processed_results.fits /arc/projects/myproject/data/processed/\n\n# 4. Clean up isn't necessary (done automatically)\n# but good practice during long sessions\nrm large_dataset.tar.gz intermediate_*.fits\n</code></pre>"},{"location":"platform/guides/storage/#multi-step-processing-workflow","title":"Multi-step Processing Workflow","text":"<pre><code>#!/bin/bash\n# Example: ALMA data reduction workflow using scratch\n\n# Step 1: Download to scratch\ncd /scratch\nalmaget 2019.1.00123.S\n\n# Step 2: Process with CASA\ncasa --nologger --agg -c \"\"\"\n# CASA script here\nexecfile('/arc/projects/myproject/code/reduction_script.py')\n\"\"\"\n\n# Step 3: Save important results\nmkdir -p /arc/projects/myproject/data/2019.1.00123.S/\ncp *.image.fits /arc/projects/myproject/data/2019.1.00123.S/\ncp *.uvfits /arc/projects/myproject/data/2019.1.00123.S/\n\n# Step 4: Create processing log\necho \"Processed $(date): 2019.1.00123.S\" &gt;&gt; /arc/projects/myproject/processing_log.txt\n</code></pre>"},{"location":"platform/guides/storage/#vospace","title":"\u2601\ufe0f VOSpace","text":"<p>VOSpace provides web-accessible, long-term archive storage based on IVOA standards.</p> <p>When to Use VOSpace</p> <ul> <li>Archives and public data</li> <li>Long-term preservation</li> <li>Sharing data with external collaborators</li> <li>Metadata-rich datasets</li> </ul> <p>\ud83d\udd27 Features:</p> <ul> <li>Web-based access - Upload/download via browser or command line</li> <li>Metadata support - Store astronomical metadata with files</li> <li>Version control - Track changes to datasets</li> <li>Sharing controls - Fine-grained access permissions</li> <li>Geographic redundancy - Multiple backup locations</li> </ul> <p>\u26a0\ufe0f Considerations:</p> <ul> <li>Slower access than ARC storage (network-based)</li> <li>Better for archives than active analysis</li> <li>Command-line tools required for advanced features</li> </ul>"},{"location":"platform/guides/storage/#vospace-vs-arc-comparison","title":"VOSpace vs ARC Comparison","text":"Use Case VOSpace ARC Projects Active analysis \u274c Too slow \u2705 Optimized Data sharing \u2705 Web interface \u26a0\ufe0f Requires group membership Public releases \u2705 Public URLs \u274c Access controlled Long-term preservation \u2705 Geo-redundant \u2705 Daily backups Large file processing \u274c Network overhead \u2705 Direct access"},{"location":"platform/guides/storage/#using-vospace","title":"Using VOSpace","text":""},{"location":"platform/guides/storage/#web-interface","title":"Web Interface","text":"<p>Access VOSpace through the CANFAR portal: \ud83d\udd17 VOSpace File Manager</p>"},{"location":"platform/guides/storage/#command-line-tools","title":"Command Line Tools","text":"<pre><code># Install VOSpace tools\npip install vostools\n\n# List VOSpace contents\nvls vos:myproject\n\n# Upload file\nvcp local_file.fits vos:myproject/\n\n# Download file  \nvcp vos:myproject/data.fits ./\n\n# Create directory\nvmkdir vos:myproject/results\n\n# Set permissions\nvchmod o+r vos:myproject/public_data.fits  # Make publicly readable\n</code></pre>"},{"location":"platform/guides/storage/#vospace-python-api","title":"VOSpace Python API","text":"<pre><code>import vos\n\n# Create client\nclient = vos.Client()\n\n# Upload file with metadata\nclient.copy(\"local_file.fits\", \"vos:myproject/survey_data.fits\")\n\n# Set metadata\nnode = client.get_node(\"vos:myproject/survey_data.fits\")\nnode.props[\"TELESCOPE\"] = \"ALMA\"\nnode.props[\"OBJECT\"] = \"NGC1365\"\nclient.update(node)\n\n# Download with progress\nclient.copy(\"vos:myproject/large_file.fits\", \"local_copy.fits\", send_md5=True)\n</code></pre>"},{"location":"platform/guides/storage/#data-transfer-strategies","title":"\ud83d\udd04 Data Transfers","text":"<p>Data Transfer Best Practice</p> <p>Always move important results from <code>/scratch/</code> to <code>/arc/projects/</code> or VOSpace before ending your session. Use the right tool for your file size and workflow.</p>"},{"location":"platform/guides/storage/#transfer-strategies-by-data-size","title":"Transfer Strategies by Data Size","text":""},{"location":"platform/guides/storage/#small-files-1gb","title":"Small Files (&lt;1GB)","text":"<pre><code># Direct copy (fastest for small files)\ncp /scratch/result.fits /arc/projects/myproject/results/\n\n# VOSpace upload\nvcp /arc/projects/myproject/final_catalog.fits vos:myproject/\n</code></pre>"},{"location":"platform/guides/storage/#medium-files-1-100gb","title":"Medium Files (1-100GB)","text":"<pre><code># Use rsync for reliability\nrsync -av --progress /scratch/large_dataset/ /arc/projects/myproject/data/\n\n# VOSpace with compression\nvcp --enable-compression /arc/projects/myproject/datacube.fits vos:myproject/\n</code></pre>"},{"location":"platform/guides/storage/#large-files-100gb","title":"Large Files (&gt;100GB)","text":"<pre><code># Process in chunks\nfor file in /scratch/survey_*.fits; do\n    # Process individual file\n    process_file.py \"$file\" \n    # Save results immediately\n    cp \"${file%.fits}_processed.fits\" /arc/projects/myproject/processed/\n    # Remove processed input to save space\n    rm \"$file\"\ndone\n</code></pre>"},{"location":"platform/guides/storage/#sshfs-setup-for-external-access","title":"SSHFS Setup for External Access","text":"<p>Mount CANFAR storage on your local computer:</p> <pre><code># Install SSHFS (macOS with Homebrew)\nbrew install --cask macfuse\nbrew install sshfs\n\n# Create mount point\nmkdir ~/canfar\n\n# Mount ARC storage\nsshfs username@ws-uv.canfar.net:/arc/projects/myproject ~/canfar\n\n# Work with files locally\nls ~/canfar\ncp ~/local_analysis.py ~/canfar/code/\n\n# Unmount when done\numount ~/canfar\n</code></pre>"},{"location":"platform/guides/storage/#sshfs-on-different-platforms","title":"SSHFS on Different Platforms","text":"macOSLinuxWindows <pre><code># Install dependencies\nbrew install --cask macfuse\nbrew install sshfs\n\n# Mount\nsshfs username@ws-uv.canfar.net:/arc/projects/myproject ~/canfar\n</code></pre> <pre><code># Install SSHFS\nsudo apt install sshfs  # Ubuntu/Debian\n# or\nsudo yum install sshfs  # CentOS/RHEL\n\n# Mount\nsshfs username@ws-uv.canfar.net:/arc/projects/myproject ~/canfar\n</code></pre> <pre><code># Install WinFsp and SSHFS-Win\n# Download from: https://github.com/billziss-gh/sshfs-win\n\n# Mount using SSHFS-Win GUI or command line\n</code></pre>"},{"location":"platform/guides/storage/#advanced-storage-operations","title":"\ud83d\udee0\ufe0f Advanced Storage Operations","text":""},{"location":"platform/guides/storage/#full-vospace-api-usage","title":"Full VOSpace API Usage","text":""},{"location":"platform/guides/storage/#metadata-management","title":"Metadata Management","text":"<pre><code># Set custom metadata\nvattr vos:myproject/observation.fits TELESCOPE ALMA\nvattr vos:myproject/observation.fits OBJECT \"NGC 1365\"\nvattr vos:myproject/observation.fits DATE-OBS \"2024-03-15\"\n\n# View metadata\nvattr vos:myproject/observation.fits\n</code></pre>"},{"location":"platform/guides/storage/#advanced-permissions","title":"Advanced Permissions","text":"<pre><code># Make file publicly readable\nvchmod o+r vos:myproject/public_catalog.fits\n\n# Grant read access to specific group\nvchmod g+r:external-collaborators vos:myproject/shared_data.fits\n\n# Set up public directory\nvmkdir vos:myproject/public\nvchmod o+r vos:myproject/public\n</code></pre>"},{"location":"platform/guides/storage/#cutout-services","title":"Cutout Services","text":"<p>Access subsections of large files without downloading the entire dataset:</p> <pre><code>import requests\n\n# Get cutout from FITS file in VOSpace\ncutout_url = \"https://ws-cadc.canfar.net/vospace/data/myproject/large_image.fits\"\nparams = {\"cutout\": \"[1:100,1:100]\", \"format\": \"fits\"}  # Section to extract\n\nresponse = requests.get(cutout_url, params=params)\nwith open(\"cutout.fits\", \"wb\") as f:\n    f.write(response.content)\n</code></pre>"},{"location":"platform/guides/storage/#automated-data-workflows","title":"Automated Data Workflows","text":"<pre><code>#!/usr/bin/env python\n\"\"\"\nAutomated data processing workflow using multiple storage systems\n\"\"\"\nimport os\nimport shutil\nimport vos\nfrom pathlib import Path\n\n\ndef process_dataset(dataset_id):\n    \"\"\"Process a dataset using optimal storage strategy\"\"\"\n\n    # 1. Download to scratch for fast processing\n    scratch_dir = Path(f\"/scratch/{dataset_id}\")\n    scratch_dir.mkdir(exist_ok=True)\n\n    # Download from VOSpace to scratch\n    client = vos.Client()\n    client.copy(f\"vos:archive/{dataset_id}.fits\", str(scratch_dir / \"raw_data.fits\"))\n\n    # 2. Process data (using scratch for speed)\n    os.chdir(scratch_dir)\n    # ... processing code here ...\n\n    # 3. Save results to ARC projects\n    results_dir = Path(f\"/arc/projects/myproject/results/{dataset_id}\")\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # Copy important results\n    shutil.copy(\"processed_image.fits\", results_dir)\n    shutil.copy(\"measurements.csv\", results_dir)\n    shutil.copy(\"processing.log\", results_dir)\n\n    # 4. Archive final products to VOSpace\n    client.copy(\n        str(results_dir / \"processed_image.fits\"),\n        f\"vos:myproject/processed/{dataset_id}_final.fits\",\n    )\n\n    # Scratch cleanup happens automatically at session end\n    print(f\"Processed {dataset_id} successfully\")\n\n\n# Process multiple datasets\nfor dataset in [\"obs001\", \"obs002\", \"obs003\"]:\n    process_dataset(dataset)\n</code></pre>"},{"location":"platform/guides/storage/#whats-next","title":"\ud83d\udd17 What's Next?","text":"<p>Now that you understand CANFAR's storage systems:</p> <ul> <li>VOSpace API Guide \u2192 - Advanced programmatic access and detailed transfer methods</li> <li>Interactive Sessions \u2192 - Access storage from sessions</li> <li>Batch Jobs \u2192 - Automated storage workflows</li> <li>Container Guide \u2192 - Storage access in containers</li> </ul> <p>Storage Strategy Summary</p> <p>Golden Rule: Use <code>/scratch/</code> for fast temporary work, save everything important to <code>/arc/projects/</code>, and archive final results in VOSpace. Plan your data workflow around these three storage tiers for optimal performance and data safety.</p>"},{"location":"platform/guides/storage/vospace-api/","title":"VOSpace API and Advanced Tools","text":"<p>Advanced data management capabilities using VOSpace APIs and command-line tools for automation and bulk operations.</p>"},{"location":"platform/guides/storage/vospace-api/#overview","title":"Overview","text":"<p>While the Storage Guide covers basic file operations, this guide focuses on programmatic access to your data using VOSpace APIs, command-line tools, and advanced workflows.</p>"},{"location":"platform/guides/storage/vospace-api/#command-line-tools","title":"Command-Line Tools","text":""},{"location":"platform/guides/storage/vospace-api/#installation","title":"Installation","text":"<p>VOSpace tools are pre-installed in all CANFAR containers. For local use:</p> <pre><code># In CANFAR notebook/desktop session\npip install vos\n\n# Verify installation\nvls --help\n</code></pre>"},{"location":"platform/guides/storage/vospace-api/#authentication","title":"Authentication","text":"<pre><code># Get a security certificate (valid for 24 hours)\ncadc-get-cert --cert ~/.ssl/cadcproxy.pem\n\n# Or use your username/password\nexport CADC_USERNAME=your_username\nexport CADC_PASSWORD=your_password\n</code></pre>"},{"location":"platform/guides/storage/vospace-api/#basic-operations","title":"Basic Operations","text":"<pre><code># List files and directories\nvls vos:CANFAR/your_username/\n\n# Copy files to VOSpace\nvcp mydata.fits vos:CANFAR/your_username/data/\n\n# Copy files from VOSpace  \nvcp vos:CANFAR/your_username/data/mydata.fits ./\n\n# Create directories\nvmkdir vos:CANFAR/your_username/projects/survey_analysis/\n\n# Move/rename files\nvmv vos:CANFAR/your_username/old.fits vos:CANFAR/your_username/new.fits\n\n# Remove files\nvrm vos:CANFAR/your_username/temp/old_data.fits\n</code></pre>"},{"location":"platform/guides/storage/vospace-api/#bulk-operations","title":"Bulk Operations","text":"<pre><code># Sync entire directories\nvsync --recursive ./local_data/ vos:CANFAR/your_username/backup/\n\n# Download project data\nvsync --recursive vos:CANFAR/shared_project/survey_data/ ./project_data/\n\n# Upload analysis results\nvsync --recursive ./results/ vos:CANFAR/your_username/analysis_outputs/\n</code></pre>"},{"location":"platform/guides/storage/vospace-api/#python-api","title":"Python API","text":""},{"location":"platform/guides/storage/vospace-api/#basic-usage","title":"Basic Usage","text":"<pre><code>import vos\n\n# Initialize client\nclient = vos.Client()\n\n# List directory contents\nfiles = client.listdir(\"vos:CANFAR/your_username/\")\nprint(files)\n\n# Check if file exists\nexists = client.isfile(\"vos:CANFAR/your_username/data.fits\")\n\n# Get file info\ninfo = client.get_info(\"vos:CANFAR/your_username/data.fits\")\nprint(f\"Size: {info['size']} bytes\")\nprint(f\"Modified: {info['date']}\")\n</code></pre>"},{"location":"platform/guides/storage/vospace-api/#file-operations","title":"File Operations","text":"<pre><code># Copy file to VOSpace\nclient.copy(\"mydata.fits\", \"vos:CANFAR/your_username/data/mydata.fits\")\n\n# Copy file from VOSpace\nclient.copy(\"vos:CANFAR/your_username/data/results.txt\", \"./results.txt\")\n\n# Create directory\nclient.mkdir(\"vos:CANFAR/your_username/new_project/\")\n\n# Delete file\nclient.delete(\"vos:CANFAR/your_username/temp/old_file.txt\")\n</code></pre>"},{"location":"platform/guides/storage/vospace-api/#advanced-operations","title":"Advanced Operations","text":"<pre><code>import os\nfrom astropy.io import fits\n\n\ndef process_fits_files(vospace_dir, output_dir):\n    \"\"\"Process all FITS files in a VOSpace directory\"\"\"\n\n    # List all FITS files\n    files = client.listdir(vospace_dir)\n    fits_files = [f for f in files if f.endswith(\".fits\")]\n\n    for fits_file in fits_files:\n        vospace_path = f\"{vospace_dir}/{fits_file}\"\n        local_path = f\"./temp_{fits_file}\"\n\n        # Download file\n        client.copy(vospace_path, local_path)\n\n        # Process with astropy\n        with fits.open(local_path) as hdul:\n            # Your processing here\n            processed_data = hdul[0].data * 2  # Example processing\n\n            # Save processed file\n            output_path = f\"{output_dir}/processed_{fits_file}\"\n            fits.writeto(output_path, processed_data, overwrite=True)\n\n            # Upload to VOSpace\n            client.copy(output_path, f\"vos:CANFAR/your_username/processed/{fits_file}\")\n\n        # Clean up temporary file\n        os.remove(local_path)\n\n\n# Usage\nprocess_fits_files(\"vos:CANFAR/your_username/raw_data\", \"./processed/\")\n</code></pre>"},{"location":"platform/guides/storage/vospace-api/#automation-workflows","title":"Automation Workflows","text":""},{"location":"platform/guides/storage/vospace-api/#batch-processing-script","title":"Batch Processing Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nAutomated data processing pipeline using VOSpace\n\"\"\"\nimport vos\nimport sys\nimport logging\nfrom pathlib import Path\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\ndef setup_vospace():\n    \"\"\"Initialize VOSpace client with authentication\"\"\"\n    try:\n        client = vos.Client()\n        # Test connection\n        client.listdir(\"vos:CANFAR/\")\n        return client\n    except Exception as e:\n        logger.error(f\"VOSpace authentication failed: {e}\")\n        sys.exit(1)\n\n\ndef sync_input_data(client, remote_dir, local_dir):\n    \"\"\"Download input data from VOSpace\"\"\"\n    logger.info(f\"Syncing {remote_dir} to {local_dir}\")\n\n    Path(local_dir).mkdir(parents=True, exist_ok=True)\n\n    # Get list of files\n    files = client.listdir(remote_dir)\n\n    for file in files:\n        if file.endswith((\".fits\", \".txt\", \".csv\")):\n            remote_path = f\"{remote_dir}/{file}\"\n            local_path = f\"{local_dir}/{file}\"\n\n            if not Path(local_path).exists():\n                logger.info(f\"Downloading {file}\")\n                client.copy(remote_path, local_path)\n\n\ndef upload_results(client, local_dir, remote_dir):\n    \"\"\"Upload processing results to VOSpace\"\"\"\n    logger.info(f\"Uploading results from {local_dir} to {remote_dir}\")\n\n    # Ensure remote directory exists\n    try:\n        client.mkdir(remote_dir)\n    except:\n        pass  # Directory might already exist\n\n    for file_path in Path(local_dir).glob(\"*\"):\n        if file_path.is_file():\n            remote_path = f\"{remote_dir}/{file_path.name}\"\n            logger.info(f\"Uploading {file_path.name}\")\n            client.copy(str(file_path), remote_path)\n\n\ndef main():\n    \"\"\"Main processing pipeline\"\"\"\n    client = setup_vospace()\n\n    # Configuration\n    input_remote = \"vos:CANFAR/shared_project/raw_data\"\n    output_remote = \"vos:CANFAR/your_username/processed_results\"\n    local_input = \"./input_data\"\n    local_output = \"./output_data\"\n\n    # Download input data\n    sync_input_data(client, input_remote, local_input)\n\n    # Your processing code here\n    logger.info(\"Processing data...\")\n    # ... processing logic ...\n\n    # Upload results\n    upload_results(client, local_output, output_remote)\n\n    logger.info(\"Pipeline completed successfully\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"platform/guides/storage/vospace-api/#monitoring-and-logging","title":"Monitoring and Logging","text":""},{"location":"platform/guides/storage/vospace-api/#transfer-progress","title":"Transfer Progress","text":"<pre><code>def copy_with_progress(client, source, destination):\n    \"\"\"Copy file with progress monitoring\"\"\"\n    import time\n\n    # Start transfer\n    start_time = time.time()\n    client.copy(source, destination)\n    end_time = time.time()\n\n    # Get file size for speed calculation\n    if source.startswith(\"vos:\"):\n        info = client.get_info(source)\n        size_mb = info[\"size\"] / (1024 * 1024)\n    else:\n        size_mb = os.path.getsize(source) / (1024 * 1024)\n\n    duration = end_time - start_time\n    speed = size_mb / duration if duration &gt; 0 else 0\n\n    print(f\"Transfer completed: {size_mb:.1f} MB in {duration:.1f}s ({speed:.1f} MB/s)\")\n</code></pre>"},{"location":"platform/guides/storage/vospace-api/#error-handling","title":"Error Handling","text":"<pre><code>def robust_copy(client, source, destination, max_retries=3):\n    \"\"\"Copy with retry logic\"\"\"\n    import time\n\n    for attempt in range(max_retries):\n        try:\n            client.copy(source, destination)\n            return True\n        except Exception as e:\n            logger.warning(f\"Copy attempt {attempt + 1} failed: {e}\")\n            if attempt &lt; max_retries - 1:\n                time.sleep(2**attempt)  # Exponential backoff\n            else:\n                logger.error(f\"Copy failed after {max_retries} attempts\")\n                return False\n</code></pre>"},{"location":"platform/guides/storage/vospace-api/#performance-optimization","title":"Performance Optimization","text":""},{"location":"platform/guides/storage/vospace-api/#parallel-transfers","title":"Parallel Transfers","text":"<pre><code>import concurrent.futures\nimport threading\n\n\ndef parallel_upload(client, file_list, remote_dir, max_workers=4):\n    \"\"\"Upload multiple files in parallel\"\"\"\n\n    def upload_file(file_path):\n        remote_path = f\"{remote_dir}/{file_path.name}\"\n        try:\n            client.copy(str(file_path), remote_path)\n            return f\"\u2713 {file_path.name}\"\n        except Exception as e:\n            return f\"\u2717 {file_path.name}: {e}\"\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = [executor.submit(upload_file, f) for f in file_list]\n\n        for future in concurrent.futures.as_completed(futures):\n            result = future.result()\n            print(result)\n</code></pre>"},{"location":"platform/guides/storage/vospace-api/#caching-strategy","title":"Caching Strategy","text":"<pre><code>import hashlib\nfrom pathlib import Path\n\n\ndef cached_download(client, vospace_path, local_path, force_refresh=False):\n    \"\"\"Download file only if it has changed\"\"\"\n\n    local_file = Path(local_path)\n    cache_file = Path(f\"{local_path}.cache_info\")\n\n    # Get remote file info\n    remote_info = client.get_info(vospace_path)\n    remote_hash = remote_info.get(\"MD5\", \"\")\n\n    # Check if we have cached info\n    if not force_refresh and local_file.exists() and cache_file.exists():\n        cached_hash = cache_file.read_text().strip()\n        if cached_hash == remote_hash:\n            print(f\"Using cached version of {local_file.name}\")\n            return local_path\n\n    # Download file\n    print(f\"Downloading {local_file.name}\")\n    client.copy(vospace_path, local_path)\n\n    # Save cache info\n    cache_file.write_text(remote_hash)\n\n    return local_path\n</code></pre>"},{"location":"platform/guides/storage/vospace-api/#integration-examples","title":"Integration Examples","text":""},{"location":"platform/guides/storage/vospace-api/#with-astropy","title":"With Astropy","text":"<pre><code>from astropy.io import fits\nfrom astropy.table import Table\n\n\ndef analyze_vospace_catalog(client, catalog_path):\n    \"\"\"Analyze a catalog stored in VOSpace\"\"\"\n\n    # Download catalog\n    local_path = \"./temp_catalog.fits\"\n    client.copy(catalog_path, local_path)\n\n    # Load and analyze\n    table = Table.read(local_path)\n\n    # Example analysis\n    bright_sources = table[table[\"magnitude\"] &lt; 15]\n    print(f\"Found {len(bright_sources)} bright sources\")\n\n    # Save filtered results\n    result_path = \"./bright_sources.fits\"\n    bright_sources.write(result_path, overwrite=True)\n\n    # Upload results\n    result_vospace = catalog_path.replace(\".fits\", \"_bright.fits\")\n    client.copy(result_path, result_vospace)\n\n    # Cleanup\n    os.remove(local_path)\n    os.remove(result_path)\n</code></pre>"},{"location":"platform/guides/storage/vospace-api/#with-batch-jobs","title":"With Batch Jobs","text":"<pre><code>#!/bin/bash\n# Batch job script using VOSpace\n\n# Authenticate\ncadc-get-cert --cert ~/.ssl/cadcproxy.pem\n\n# Download input data\nvcp vos:CANFAR/project/input/data.fits ./input.fits\n\n# Process data\npython analysis_script.py input.fits output.fits\n\n# Upload results\nvcp output.fits vos:CANFAR/project/results/processed_$(date +%Y%m%d).fits\n\n# Cleanup\nrm input.fits output.fits\n</code></pre>"},{"location":"platform/guides/storage/vospace-api/#troubleshooting","title":"Troubleshooting","text":""},{"location":"platform/guides/storage/vospace-api/#common-issues","title":"Common Issues","text":"<p>Authentication Problems: <pre><code># Refresh certificate\ncadc-get-cert --cert ~/.ssl/cadcproxy.pem\n\n# Check certificate validity\ncadc-get-cert --cert ~/.ssl/cadcproxy.pem --days-valid\n</code></pre></p> <p>Network Timeouts: <pre><code># Increase timeout for large files\nimport vos\n\nclient = vos.Client()\nclient.timeout = 300  # 5 minutes\n</code></pre></p> <p>Permission Errors: <pre><code># Check file permissions\nvls -l vos:CANFAR/your_username/file.fits\n\n# Check directory access\nvls vos:CANFAR/project_name/\n</code></pre></p>"},{"location":"platform/guides/storage/vospace-api/#next-steps","title":"Next Steps","text":"<ul> <li>Batch Jobs \u2192 - Automate VOSpace workflows</li> <li>Containers \u2192 - Include VOSpace tools in custom containers</li> <li>Radio Astronomy \u2192 - Specialized data workflows</li> </ul>"},{"location":"platform/legacy/cloud-services/","title":"CANFAR Cloud Services","text":"<p>\ud83c\udfaf What You'll Learn</p> <ul> <li>What cloud services power CANFAR (via Digital Research Alliance Canada OpenStack)  </li> <li>Differences between DRAC OpenStack cloud and CANFAR cloud access  </li> <li>How to log in and manage credentials (CADC accounts vs DRAC accounts)  </li> <li>Where to find the correct cloud portals (Arbutus, Arbutus-CANFAR)  </li> <li>Best practices for using shared cloud allocation resources  </li> </ul> <ul> <li>All CANFAR cloud services are hosted on the Digital Research Alliance Canada (DRAC) OpenStack Infrastructure.  </li> <li>Access is integrated with CADC credentials and customized portals, ensuring consistency and security across CANFAR workflows.  </li> <li>When you request access, the CANFAR team will review it and arrange your registration with the DRAC.</li> </ul> <p>For a typical workflow with OpenStack Cloud Services and batch processing on CANFAR, see the tutorial below.</p>"},{"location":"platform/legacy/cloud-services/#key-differences","title":"Key differences from DRAC defaults","text":"<ul> <li>Credentials: Sign in with CADC Username/Password (not a DRAC account).</li> <li>Portal:  Use the arbutus-canfar portal (instead of arbutus).</li> <li>Resource policy: interactive analysis gets reasonable quotas; batch processing can scale to large footprints.</li> </ul>"},{"location":"platform/legacy/cloud-services/#registration","title":"Registration &amp; Allocation","text":"<p>A CADC account is required to access cloud services.</p> <ol> <li>Register a CADC account.</li> <li>Email CANFAR support with:<ul> <li>Project Name</li> <li>CADC Account <code>Username</code></li> <li>Estimated resources (storage, compute; whether you need batch)</li> <li>A short description of your use case (2\u20133 sentences)</li> </ul> </li> </ol> <p>CANFAR will review and coordinate project/quotas on the DRAC side.</p>"},{"location":"platform/legacy/cloud-services/#batch-tutorial","title":"Batch Tutorial","text":"<p>This tutorial builds a basic source detection pipeline for CFHT MegaCam images on a CANFAR VM with fast access to the CADC archive and VOSpace, that runs it on the CANFAR batch system.</p> <p>You will learn to</p> <ul> <li>Create/manage VMs on DRAC OpenStack / CANFAR</li> <li>Access CADC VOSpace</li> <li>Submit batch jobs that run your pipeline</li> </ul> <p>Only need persistent (non-batch) VMs?</p> <ul> <li>If you just need persistent VMs, follow DRAC VM docs and skip the batch sections here.</li> <li>Throughout the guide, <code>&lt;USERNAME&gt;</code> refers to your CADC username; <code>&lt;VOSPACE&gt;</code> maps to your VOSpace; <code>&lt;PROJECT&gt;</code> is the OpenStack Project.</li> </ul>"},{"location":"platform/legacy/cloud-services/#create-and-connect-to-a-vm","title":"Create and Connect to a VM","text":""},{"location":"platform/legacy/cloud-services/#create-vm","title":"1. Create a VM","text":"<p>Use the DRAC web dashboard.</p> <ol> <li>Sign in to  Dashboard with CADC <code>&lt;USERNAME&gt;</code>/password.</li> <li>Each CANFAR allocation maps to an OpenStack <code>&lt;PROJECT&gt;</code>. Use the top-left project picker to switch if you belong to multiple.</li> </ol> <p>Follow DRAC\u2019s Creating a Linux VM. Summary below.</p>"},{"location":"platform/legacy/cloud-services/#2-import-an-ssh-public-key","title":"2. Import an SSH Public Key","text":"<p>OpenStack prefers SSH key pairs over passwords.</p> <ul> <li>If you do not have a key pair, run <code>ssh-keygen</code> locally or follow DRAC\u2019s [SSH Keys documentation].</li> <li>In Compute \u2192 Key Pairs, click Import Key Pair.</li> <li>Name the key and paste your public key (default path <code>~/.ssh/id_rsa.pub</code>).</li> </ul>"},{"location":"platform/legacy/cloud-services/#3-allocate-a-public-ip","title":"3. Allocate a Public IP","text":"<ul> <li>Go to Network \u2192 Floating IPs.</li> <li>If none is listed, click Allocate IP to Project.</li> <li>Typically, each project has one public IP; if exhausted you\u2019ll see Quota Exceeded.</li> </ul>"},{"location":"platform/legacy/cloud-services/#4-launch-an-instance","title":"4. Launch an Instance","text":"<ul> <li>In Compute \u2192 Instances \u2192 Launch Instance, choose:</li> <li>Source: canfar-ubuntu-20.04 (important for batch)</li> <li>Flavor: e.g., <code>c2-7.5gb-30</code> (2 vCPU / 7.5 GiB RAM / ~31 GiB ephemeral disk)</li> <li>Key Pair: select your SSH key</li> <li>Click Launch.</li> </ul>"},{"location":"platform/legacy/cloud-services/#5-connect-to-the-instance","title":"5. Connect to the Instance","text":"<p>After status becomes Running, first associate the floating IP (menu \u2192 Associate Floating IP), then SSH:</p> <pre><code>ssh ubuntu@&lt;FLOATING_IP&gt;\n</code></pre> <p>Create a local user matching your CADC account (for audit/minimal access):</p> <pre><code>sudo canfar_create_user &lt;USERNAME&gt;\nlogout\nssh &lt;USERNAME&gt;@&lt;FLOATING_IP&gt;\n</code></pre> <p>Default image users</p> <ul> <li>Ubuntu images: <code>ubuntu</code></li> <li>Rocky Linux images: <code>rocky</code></li> </ul>"},{"location":"platform/legacy/cloud-services/#install-software","title":"Install software","text":"<p>The base VM image comes with only a minimal set of packages. For this example, we need to install two additional tools:</p> <ul> <li>Source Extractor (source detection): software used to detect astronomical sources in FITS images, producing catalogues of stars and galaxies.</li> <li>funpack (FITS decompressor; Ubuntu package <code>libcfitsio-bin</code>)\uff1aa decompression utility for FITS images. Most FITS images provided by CADC are Rice-compressed and stored with an <code>.fz</code> extension. Since Source Extractor only accepts uncompressed images, we will use <code>funpack</code> to uncompress them. The <code>funpack</code> executable is distributed as part of the <code>libcfitsio-bin</code> package in Debian/Ubuntu.</li> </ul> <p>Because both tools are available from the Ubuntu software repository, we can install them system-wide after updating the package index: Install packages<pre><code>sudo apt update -y\nsudo apt install -y source-extractor libcfitsio-bin\n</code></pre></p>"},{"location":"platform/legacy/cloud-services/#test-vm","title":"Test on the VM","text":"<p>Use the ephemeral disk (mounted at <code>/mnt</code>) for scratch.</p> <pre><code>sudo canfar_setup_scratch  # create /mnt/scratch with proper permissions\ncd /mnt/scratch\ncp /usr/share/source-extractor/default* .\ncat &gt; default.param &lt;&lt;'EOF'\nNUMBER\nMAG_AUTO\nX_IMAGE\nY_IMAGE\nEOF\ncadcget cadc:CFHT/1056213p.fits.fz\nfunpack -D 1056213p.fits.fz\nsource-extractor 1056213p.fits -CATALOG_NAME 1056213p.cat\n</code></pre> <p>Scratch space</p> <ul> <li>Run <code>canfar_setup_scratch</code> each time you boot a new instance.</li> <li>In batch mode, each job gets its own scratch directory (not <code>/mnt/scratch</code>).</li> </ul>"},{"location":"platform/legacy/cloud-services/#persist-results","title":"Persist results to VOSpace","text":"<p>Ephemeral storage is wiped when the VM terminates. Upload the output <code>1056213p.cat</code> to VOSpace (the VM includes the <code>vos</code> client).</p> <p>Obtain a proxy certificate for automated access:</p> <pre><code>cadc_dotnetrc      # one-time helper to create ~/.netrc\ncadc-get-cert -n   # generate an X509 proxy (default 10 days)\n</code></pre> <pre><code>vcp 1056213p.cat vos:&lt;VOSPACE&gt;/\n</code></pre> <p>Credential hygiene</p> <p><code>.netrc</code> stores credentials in plaintext. Use only on controlled hosts and restrict permissions: <code>chmod 600 ~/.netrc</code>.</p>"},{"location":"platform/legacy/cloud-services/#snapshot","title":"Snapshot the instance","text":"<p>In the Instances view, click Create Snapshot (e.g., name it <code>image-reduction-2023-08-21</code>).</p> <p>Warning</p> <p>Avoid writes on the VM while a snapshot is being created.</p> <p>Without a snapshot, ephemeral data is lost when the instance is deleted. Volume-backed VMs persist data but are not suitable for batch.</p>"},{"location":"platform/legacy/cloud-services/#batch-script","title":"Automate as a batch script","text":"<p>CANFAR batch is powered by HTCondor; Cloud Scheduler launches worker VMs on demand.</p> <p>Create <code>~/do_catalogue.bash</code>:</p> <pre><code>#!/usr/bin/env bash\nset -euo pipefail\nid=\"$1\"\ncadcget \"cadc:CFHT/${id}.fits.fz\"\nfunpack -D \"${id}.fits.fz\"\ncp /usr/share/source-extractor/default* .\ncat &gt; default.param &lt;&lt;'EOF'\nNUMBER\nMAG_AUTO\nX_IMAGE\nY_IMAGE\nEOF\nsource-extractor \"${id}.fits\" -CATALOG_NAME \"${id}.cat\"\nvcp \"${id}.cat\" \"vos:&lt;VOSPACE&gt;/\"\n</code></pre>"},{"location":"platform/legacy/cloud-services/#submission","title":"Write a submission file","text":"<p>Submit four image IDs: <code>1056215p 1056216p 1056217p 1056218p</code>.</p> do_catalogue.sub<pre><code>executable = do_catalogue.bash\n\noutput = do_catalogue-$(arguments).out\nerror  = do_catalogue-$(arguments).err\nlog    = do_catalogue-$(arguments).log\n\nqueue arguments from (\n  1056215p\n  1056216p\n  1056217p\n  1056218p\n)\n</code></pre>"},{"location":"platform/legacy/cloud-services/#submit","title":"Submit jobs","text":"<p>Two authorizations are needed: - Access to snapshots in <code>&lt;PROJECT&gt;</code> - Write access to <code>&lt;VOSPACE&gt;</code></p> <p>On the batch login node <code>batch.canfar.net</code>:</p> <pre><code>ssh &lt;USERNAME&gt;@batch.canfar.net\n. &lt;PROJECT&gt;-openrc.sh     # set OpenStack env (once per session)\n</code></pre> <p>Submit:</p> <pre><code>canfar_submit do_catalogue.sub image-reduction-2023-08-21 c2-7.5gb-30\n</code></pre> <p>Where: - <code>do_catalogue.sub</code>: submission file - <code>image-reduction-2023-08-21</code>: snapshot image name - <code>c2-7.5gb-30</code>: VM flavor (list via <code>openstack flavor list</code>)</p> <p>Monitor:</p> <pre><code>condor_q\ncondor_q -all   # all users summary\n</code></pre> <p>When the interactive VM is no longer needed, delete it from the dashboard (Delete Instances).</p>"},{"location":"platform/legacy/cloud-services/#extras","title":"Extras: helpful commands &amp; VM maintenance","text":"<ul> <li>Keep the OS updated: <code>sudo apt update &amp;&amp; sudo apt dist-upgrade</code> (or <code>dnf</code> on Rocky).</li> <li>Prebuilt VM helpers (<code>canfar-ubuntu-20.04</code> / <code>canfar-rocky-8</code>):</li> <li><code>cadc_cert -u &lt;USERNAME&gt;</code>: obtain a CADC proxy (legacy helper)</li> <li><code>cadc_dotnetrc</code>: create/update <code>~/.netrc</code></li> <li><code>canfar_setup_scratch</code>: set up <code>/mnt/scratch</code></li> <li><code>canfar_create_user &lt;USERNAME&gt;</code>: create a local user and grant sudo</li> <li><code>canfar_update</code>: update CANFAR scripts and CADC clients ```</li> </ul>"},{"location":"platform/legacy/publication/","title":"Publications (DOIs)","text":""},{"location":"platform/legacy/publication/#purpose","title":"Purpose","text":"<p>The CANFAR Data Publication Service (DPS) links a paper to the data package used in the research. DPS provides storage and registers a DOI with DataCite; the DOI permanently resolves to your landing page and data directory.</p>"},{"location":"platform/legacy/publication/#access","title":"Access","text":"<ul> <li>CANFAR Science Portal \u2192 Data Publication</li> <li>Data Publication Service</li> </ul>"},{"location":"platform/legacy/publication/#account","title":"Account requirements","text":"<p>The first author needs a CADC account to access the DPS UI and, later, the user\u2011managed storage (VOSpace).</p> <p>What you'll learn</p> <ul> <li>Request a DOI</li> <li>Upload the data package</li> <li>(Optional) Provide referee access</li> <li>Publish via DataCite</li> </ul>"},{"location":"platform/legacy/publication/#doi-guide","title":"DOI guide","text":""},{"location":"platform/legacy/publication/#request","title":"1) Request a DOI","text":"<ul> <li>A DOI is reserved for your package (e.g., 10.11570/20.0006).</li> <li>A Data Directory (VOSpace) is created and accessible via the Web UI or <code>vos</code> tools.</li> <li>A landing page is generated.</li> </ul>"},{"location":"platform/legacy/publication/#upload","title":"2) Upload the data package","text":"<p>Choose a method based on size and file count:</p> <ul> <li>Few/small files \u2192 Use the Web Storage UI</li> <li>Large or many files \u2192 Use the <code>vos, vcp</code> CLI tools</li> </ul> <p>More details: DOI Data Package.</p>"},{"location":"platform/legacy/publication/#refereeing","title":"3) Refereeing","text":"<p>On request, CADC can create a read\u2011only account for the editor/referee to access the data directory. The account is disabled after review.</p>"},{"location":"platform/legacy/publication/#publish","title":"4) Publish with DataCite","text":"<p>From DPS, click Publish to mint the DOI with DataCite and lock the Data Directory. Later metadata changes (e.g., adding the publication DOI) require contacting CANFAR support.</p>"},{"location":"platform/legacy/publication/#using","title":"Using the DPS","text":""},{"location":"platform/legacy/publication/#list","title":"Listing current DOIs","text":"<p>DPS shows your DOIs (status, title, landing page, data directory). From here, you can request, view, edit, or publish depending on status.</p>"},{"location":"platform/legacy/publication/#new","title":"Requesting a new DOI","text":"<p>Use New from the list or go to the request page.</p> <p>Required</p> <ul> <li>First Author</li> <li>Title</li> </ul> <p>Optional (can be edited later)</p> <ul> <li>Journal reference (journal, volume, page)</li> <li>Additional Authors</li> </ul> <p>After submission, a DOI Reference number is assigned and displayed.</p>"},{"location":"platform/legacy/publication/#details","title":"DOI Details","text":"<p>On the details page (e.g., DOI.20.0016) you\u2019ll find:</p> <ul> <li>DOI number / Title</li> <li>Authors / Journal reference</li> <li>DOI status</li> <li>Landing page link</li> <li>Data Directory link (shows  when frozen)</li> </ul>"},{"location":"platform/legacy/publication/#edit","title":"Editing details","text":"<ul> <li>Unpublished DOIs can be edited by authenticated users; click Update.</li> <li>Published DOIs require a request to CANFAR support.</li> </ul>"},{"location":"platform/legacy/publication/#landing","title":"Viewing the landing page","text":"<ul> <li>DOI: 10.11570/20.0016</li> <li>Landing page: landing page</li> </ul> <p>Published landing pages are publicly accessible.</p>"},{"location":"platform/legacy/publication/#publish-action","title":"Publishing a DOI","text":"<p>If not yet published, a Publish button appears at the top right. Publishing: - Completes registration with DataCite - Locks the Data Directory</p> <p>Related publication info can be added later via support.</p>"},{"location":"platform/legacy/publication/#delete","title":"Deleting unpublished DOIs","text":"<p>Unpublished records can be deleted via Delete on the request page. Published DOIs cannot be deleted.</p>"},{"location":"platform/legacy/publication/#data-package","title":"DOI Data Package","text":"<p>DPS hosts a Data Directory in the Vault (VOSpace) implementation for each DOI. A folder named <code>data/</code> is created under the DOI root; you control the structure beneath it.</p> <p>Example: Data Directory</p> <p>Locked after publish</p> <p>After publishing, the directory is locked. To modify contents or metadata, contact CANFAR support.</p>"},{"location":"platform/legacy/publication/#contents","title":"Contents","text":"<p>You decide what to include: data, figures, software, etc. We recommend a top\u2011level <code>README</code> describing layout and usage.</p>"},{"location":"platform/legacy/publication/#uploading","title":"Uploading","text":"<ul> <li>Few/small files: Web Storage UI.</li> <li>Large/many files: Use <code>vcp</code>, <code>vos</code> CLI Tools.</li> </ul>"},{"location":"platform/legacy/publication/#ref-access","title":"Refereeing access","text":"<p>Contact support to obtain a read\u2011only account and share with the editor/referee. They may request changes prior to publication.</p>"},{"location":"platform/legacy/publication/#discover","title":"Publish &amp; discoverability","text":"<p>After acceptance, click Publish to mint the DOI. The directory and metadata freeze; minimal discovery metadata will appear in DataCite search.</p>"},{"location":"platform/legacy/publication/#final-link","title":"Final linking","text":"<p>Finally, link the data package DOI to the journal DOI (currently manual): - Email support with the publication DOI and updated reference details. - Provide the data package DOI to the journal so it appears in the paper.</p>"},{"location":"platform/legacy/storage/","title":"VOSpace: CANFAR Storage System","text":"<p>\ud83c\udfaf What You'll Learn</p> <ul> <li>What VOSpace is and why it matters for CANFAR users  </li> <li>When to use VOSpace for data staging, storage, and sharing  </li> <li>Requirements for accessing VOSpace (accounts and tools)  </li> <li>Available access methods: web, CLI, Python <code>vos</code>, and <code>vofs</code> filesystem  </li> <li>Best practices for storing and managing data in VOSpace  </li> </ul> <p>VOSpace is the CANFAR storage system, an implementation of the Virtual Observatory specification. It provides long-term, secure, and collaborative storage for astronomy data. VOSpace is designed to integrate with CANFAR\u2019s processing and workflows, making it the central hub for data preparation, analysis, and sharing.  </p>"},{"location":"platform/legacy/storage/#why-use-vospace","title":"Why use VOSpace","text":"<ul> <li>Reliable and secure: Files are mirrored across four physical locations, protecting against disk failures.  </li> <li>Long-term storage: Intended for permanent or semi-permanent storage of astronomical data.  </li> <li>Collaboration: Enables data sharing between members of a project or research team.  </li> </ul> <p>VOSpace is especially useful when working with large datasets that must remain accessible and safe throughout a project lifecycle. </p>"},{"location":"platform/legacy/storage/#when-to-use","title":"When to use","text":"<ul> <li>Staging input data: If your data are not already in a CADC archive, you can upload them to VOSpace for faster access during processing.  </li> <li>Storing results: Keep outputs from the CANFAR processing system in a central, backed-up location.  </li> <li>Sharing data: Exchange datasets with collaborators in a controlled and efficient way.  </li> </ul> <p>Think of VOSpace as your workspace \u201chub\u201d: data comes in, gets processed, and results can be distributed or archived. </p>"},{"location":"platform/legacy/storage/#requirements","title":"Requirements","text":"<ul> <li>CADC account: Access requires registration for a valid CADC account.  </li> <li>Access methods:  </li> <li>Web interface: Easy to use, interactive, and familiar for browsing or uploading small datasets.  </li> <li>Python tools / CLI: The <code>vos</code> module and command-line clients allow scripted and automated access.  </li> <li>Filesystem mount (vofs): FUSE-based view that works like a local folder. Convenient for browsing, but not recommended for heavy processing.  </li> </ul> <p>Choose the access method that best fits your workflow: web for simple interaction, CLI/<code>vos</code> for automated tasks, and <code>vofs</code> only for lightweight exploration. </p> <p>Open Web Storage </p> <p>Access requires a CADC account.</p> <p>Related</p> <ul> <li>Python client &amp; CLI examples are below.</li> <li>Publications/DOIs.</li> </ul>"},{"location":"platform/legacy/storage/#vos-cli","title":"The <code>vos</code> CLI","text":"<p>Use the <code>vos</code> command-line client for scripts and terminals.</p>"},{"location":"platform/legacy/storage/#install","title":"Installation","text":"<ul> <li>Python \u2265 3.7</li> </ul> Install vos<pre><code>pip install -U vos\n# or per-user\npip install --user -U vos\n# ensure PATH includes user base\nexport PATH=\"${HOME}/.local/bin:${PATH}\"\n</code></pre>"},{"location":"platform/legacy/storage/#common-commands","title":"Common commands (recommended)","text":"<p>Replace <code>VOSPACE</code> with your space name (often your username; projects may have project spaces).</p> <pre><code># list the root of the entire VOSpace\nvls vos:\n\n# copy a local file to the VOSPACE root\nvcp \"${HOME}/bar\" \"vos:[VOSPACE]\"\n\n# wildcards\nvcp \"vos:[VOSPACE]/foo/*.txt\" .\n\n# FITS cutout (pixels)\nvcp \"vos:[VOSPACE]/image.fits[1:100,1:100]\" .\n\n# or by coordinates (example)\nvcp \"vos:[VOSPACE]/image.fits(10.25,10.25,0.1)\" .\n\n# headers only\nvcp --head \"vos:[VOSPACE]/image.fits\" .\n\n# inspect headers\nvcat --head \"vos:[VOSPACE]/image.fits\"\n\n# remove an entry\nvrm \"vos:[VOSPACE]/foo\"\n\n# create a container (directory)\nvmkdir \"vos:[VOSPACE]/bar\"\n\n# move/rename\nvmv \"vos:[VOSPACE]/bar\" \"vos:[VOSPACE]/foo/\"\nvmv \"vos:[VOSPACE]/foo/bar\" \"vos:[VOSPACE]/foo/bar2\"\n\n# group write permission\nvchmod g+w \"vos:[VOSPACE]/foo/bar.txt\" 'GROUP1, GROUP2, GROUP3'\n</code></pre> <p>More help:</p> <pre><code>vls --help\npydoc vos.commands\n</code></pre>"},{"location":"platform/legacy/storage/#python-api","title":"Python API","text":"<pre><code>from vos import Client\n\nclient = Client()\nlisting = client.listdir('vos:MyVOSpace')\nclient.copy('vos:MyVOSpace/Filename', '/local/filename')\n</code></pre>"},{"location":"platform/legacy/storage/#vofs","title":"VOSpace FUSE filesystem (<code>vofs</code>)","text":"<p>Mount VOSpace as a remote filesystem via FUSE.</p> <p>Performance</p> <p><code>vofs</code> is not recommended for batch or IO-heavy pipelines; it\u2019s best for light interactive browsing.</p>"},{"location":"platform/legacy/storage/#vofs-install","title":"Install <code>vofs</code>","text":"<p>After installing <code>vos</code>, install <code>vofs</code>:</p> <pre><code>pip install -U vofs\n# or per-user\npip install --user -U vofs\nexport PATH=\"${HOME}/.local/bin:${PATH}\"\n</code></pre>"},{"location":"platform/legacy/storage/#fuse","title":"FUSE prerequisites","text":"<p>Linux</p> <pre><code># some distros require FUSE and devel packages\nsudo yum install -y fuse fuse-devel || true\n# add your user to the fuse group (re-login required)\nsudo /usr/sbin/usermod -a -G fuse \"$(whoami)\"\n</code></pre> <p>macOS</p> <ul> <li>Install macFUSE. If you hit a <code>libfuse.dylib</code> error with <code>mountvofs</code>, set:</li> </ul> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/usr/local/lib\n</code></pre>"},{"location":"platform/legacy/storage/#vofs-usage","title":"Usage","text":"<pre><code># mount all accessible VOSpaces\nmountvofs\n\n# inspect mount point\nls /tmp/vospace\n\n# unmount\nfusermount -u /tmp/vospace   # Linux\numount /tmp/vospace          # macOS\n\n# mount a specific VOSpace\nmountvofs --vospace vos:USER --mountpoint /path/to/a/directory\n</code></pre> <p><code>mountvofs</code> creates a local cache (default 50 GiB, default <code>${HOME}/vos:USER</code>). It refreshes when the remote is newer; on close, writes are pushed back. Many science tools write infrequently and perform well; typical editors write temp files often and may degrade performance.</p> <p>For options:</p> <pre><code>mountvofs --help\n</code></pre>"},{"location":"platform/legacy/storage/#certificates","title":"Certificates (X509)","text":"<p>The CLI requires a valid certificate. CADC issues a long-lived certificate on account creation; batch uses short-lived proxy certificates.</p> <pre><code>cadc-get-cert -u &lt;USERNAME&gt;\n</code></pre>"},{"location":"platform/legacy/storage/#cert-batch","title":"Using vos with batch VMs","text":"<p>Proxy certificates are injected automatically into batch workers at submission time. If not, alternatives:</p> <p>More secure (recommended)</p> <pre><code># on batch.canfar.net, obtain a proxy\ncadc-get-cert -u &lt;USERNAME&gt;\n# copy it to your submission directory\ncp ${HOME}/.ssl/cadcproxy.pem .\n</code></pre> <p>Add to your submission file:</p> <pre><code>should_transfer_files = YES\ntransfer_input_files = cadcproxy.pem\n</code></pre> <p>At the top of the job script:</p> <pre><code>mv cadcproxy.pem ${HOME}/.ssl/\n</code></pre> <p>Less secure (not recommended) Run <code>cadc-get-cert</code> at job start. To avoid password prompts, create a <code>~/.netrc</code>:</p> <pre><code>machine www.canfar.net login &lt;USERNAME&gt; password &lt;PASSWORD&gt;\n</code></pre> <pre><code>chmod 600 ~/.netrc\n</code></pre>"}]}