{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"CANFAR Science Platform"},{"location":"#built-on-ivoa-standards-fair-principles","title":"Built on IVOA standards &amp; F.A.I.R. principles","text":"<p> Canadian Advanced Network for Astronomical Research is a scalable, cloud-native workspace for astronomy research. <p>Spin up JupyterLab, submit batch jobs, and collaborate in shared project spaces.  The CANFAR Science Platform gives researchers the tools they need with minimal setup. </p> <p>Discover what the CANFAR Science Platform can do for you, your team, and your research group.</p> <ul> <li> Interactive Sessions e.g. JupyterLab</li> <li> Batch Processing for large-scale analysis</li> <li> Shared Storage for collaborative datasets</li> <li> Software Containers with astronomy tools</li> <li> Collaboration Tools with group permissions</li> <li> Help &amp; Support for research workflows</li> <li> Python API for access and automation</li> <li> CLI for terminal users</li> <li> Publications of DataCite DOIs</li> <li> Platform Operations Deployments and infrastructure</li> <li> Release Notes for the latest updates</li> <li> Try out CANFAR Science Platform</li> <li> and much more...</li> </ul> <p>"},{"location":"#if-you-use-canfar-add-an-acknowledgement-to-your-papers-theses-and-other-research-outputs","title":"If you use CANFAR, add an acknowledgement to your papers, theses, and other research outputs.","text":"<p> The authors acknowledge the use of the Canadian Advanced Network for Astronomy Research (CANFAR) Science Platform operated by the Canadian Astronomy Data Centre (CADC) and the Digital Research Alliance of Canada (DRAC), with support from the National Research Council of Canada (NRC), the Canadian Space Agency (CSA), CANARIE, and the Canadian Foundation for Innovation (CFI). </p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#111-2025-11-26","title":"1.1.1 (2025-11-26)","text":""},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>update release versionss (651fc47)</li> </ul>"},{"location":"changelog/#110-2025-11-25","title":"1.1.0 (2025-11-25)","text":""},{"location":"changelog/#features","title":"Features","text":"<ul> <li>cli: enhanced resilience when handling session data from skaha api (2662f89)</li> </ul>"},{"location":"changelog/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>auth: better reporting for x509 cert date validity errors (47e800b)</li> <li>build: updated dependencies to support python 3.14 (664011c)</li> <li>cli: properly print cert errors (7cfe834)</li> <li>remove instances expectation from stats (aad2656)</li> <li>sessions: added debug logging to all async session requests (bd4b5a4)</li> </ul>"},{"location":"changelog/#documentation_1","title":"Documentation","text":"<ul> <li>add deployments documentation link (2a4cc49)</li> <li>best-practices: for containerizing (5d05f80)</li> <li>improve deployments link naming (76c89ef)</li> <li>mkdocs: added best-practices to the edge docs (6be501c)</li> <li>platform: added best practices for end users (0c747ea)</li> <li>release-notes: change release notes structure, added 2025.2 (a876880)</li> <li>releases: 2025.2 release note updates (21cd759)</li> <li>releases: Release process, roadmap, and contributions (ec7e46c)</li> <li>releases: typo (2224c02)</li> <li>releases: Update of general release information (4763715)</li> </ul>"},{"location":"changelog/#104-2025-09-26","title":"1.0.4 (2025-09-26)","text":""},{"location":"changelog/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>docs: updated colors for better accessibility in dark mode (a169037)</li> </ul>"},{"location":"changelog/#documentation_2","title":"Documentation","text":"<ul> <li>conduct: updated links (03576d1)</li> <li>fixes: updates to landing page (112c5ca)</li> <li>support: updated support help page to remove repition (092aae5)</li> </ul>"},{"location":"changelog/#103-2025-09-23","title":"1.0.3 (2025-09-23)","text":""},{"location":"changelog/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>docs: updated docs with link to the platform (39aead3)</li> <li>docs: updates for community docs (57d0d6f)</li> </ul>"},{"location":"changelog/#102-2025-09-16","title":"1.0.2 (2025-09-16)","text":""},{"location":"changelog/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>defaults: updated default config to use v1 server api (447b17e)</li> <li>session: default destroy_with not targets <code>Completed</code> jobs (08581be)</li> <li>sessions: image parameter now is not locked down to images.canfar.net to support local variations in all SRCnet deployments (2b69c52)</li> <li>sessions: removed no longer needed view=event param during http get (6afb91c)</li> </ul>"},{"location":"changelog/#documentation_3","title":"Documentation","text":"<ul> <li>overhaul and clarify platform guides, containers, sessions, storage, permissions, and mkdocs config (db52b02)</li> <li>release-notes: consolidation (7cbebae)</li> <li>updates: for breaking changes between v0 and v1 api (d58bd6c)</li> </ul>"},{"location":"changelog/#101-2025-09-11","title":"1.0.1 (2025-09-11)","text":""},{"location":"changelog/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>correct typos in release notes (def5f3c)</li> </ul>"},{"location":"changelog/#documentation_4","title":"Documentation","text":"<ul> <li>about-page: removed outedated governance info (16ff6cc)</li> <li>alma: fix image paths to sessions/images where needed (a8524e7)</li> <li>batch: fix CLI and API syntax errors (160b801)</li> <li>cleanup: fixes for relatives paths, removed redundant toc in platform docs, fixes docstrings (390d10b)</li> <li>release-notes: add CanfarSP 2025.1 announcement and version info (30c61d1)</li> <li>release-notes: fixes (4aaf24b)</li> <li>release-notes: update release notes (9f73c77)</li> <li>release-notes: v1.0.1 (d4d4d0d)</li> </ul>"},{"location":"changelog/#100-2025-09-09","title":"1.0.0 (2025-09-09)","text":""},{"location":"changelog/#breaking-changes","title":"\u26a0 BREAKING CHANGES","text":"<ul> <li>project: - The configuration directory has moved from <code>~/.skaha</code> to <code>~/.canfar</code>.</li> </ul>"},{"location":"changelog/#features_1","title":"Features","text":"<ul> <li>api: added support for flexible user sessions (c9b5b98)</li> <li>auth: add HTTPx authentication hooks for token refresh (489066d)</li> <li>auth: add OIDC authentication hooks for automatic token refresh (bc4596f)</li> <li>auth: added oidc device flow logic (1013cf5)</li> <li>auth: added x509 cert get logic (657a97d)</li> <li>auth: implement async and sync OIDC token refresh functions (b43b05f)</li> <li>auth: server info is now saved for each auth config (1788c1e)</li> <li>build: added edge container build and attestation (d07e008)</li> <li>cli: added <code>skaha open</code> command to open sessions in a web browser. Made skaha info more readable (c932187)</li> <li>cli: added auth cmds: list, switch, rm &amp; purge (6ab0bd8)</li> <li>cli: added capability to discover vosi capabilities of a server (da70dde)</li> <li>cli: added entrypoing cf (cf3fce8)</li> <li>cli: added feat to auto discover skaha servers (46eb565)</li> <li>cli: added list, ls to skaha ps as aliases (c88eb6a)</li> <li>cli: added support for using the CLI as canfar (5051af1)</li> <li>client: add expiry property and enhance SSL context handling for authentication (cb6261a)</li> <li>client: added logic to catch AuthExpiryError when the auth context is expired (49c106e)</li> <li>client: added loglevel to configure the python logging levels (431f46d)</li> <li>client: added token support to skaha client (6c7c748)</li> <li>client: enhance SkahaClient documentation and improve authentication handling (1e6b770)</li> <li>client: significant improvements to the base skaha client to support context managers (a197553)</li> <li>client: updated skaha client to use httpx instead of requests (4c10de8)</li> <li>cli: work-in-process (2e646ab)</li> <li>codecov: added badge (373412d)</li> <li>config: updated auth config to provide valid &amp; expired checks for all auth types and added tests (4dc6929)</li> <li>context: updates to context.resources api (4d08876)</li> <li>discover: added functionality to discover skaha servers from registries and also added tests (5857772)</li> <li>dockerfile: added base dockerfile for the project (28f7e51)</li> <li>docs: added docs about canfar (297156c)</li> <li>docs: added docs about canfar (76b69b1)</li> <li>docs: added realease notes (18a0d30)</li> <li>docs: importing science container docs with history (f388602)</li> <li>docs: reorg from scicon docs (85ca36c)</li> <li>docs: significant updates for client docs to provide sync/async variations of most python calls (7eacf8f)</li> <li>docs: significant updates for client docs to provide sync/async variations of most python calls (c86ae6b)</li> <li>docs: updated README, and the cleaned up the main landing page significantly (4618ed2)</li> <li>docs: updated README, and the cleaned up the main landing page significantly (2745b60)</li> <li>docs: updates to documentation theme and features (5289571)</li> <li>exceptions: added skaha exception classes (0651191)</li> <li>garble: added fernet and rot13 ciphers to encrypt/obsfucate sensitive info (7204879)</li> <li>github-actions: added pypi release action and updated client payload (b0b3593)</li> <li>helpers: added code to to split tasks for large scale processing (6ffe5cb)</li> <li>hooks: added cli tool typer hook for multiple command aliases (6c5bc15)</li> <li>logging: added comprehensive rich based logger for the project (a48c520)</li> <li>logs: added stdout for printing logs in terminal (0c34cad)</li> <li>models: added http models for the client (0679e8f)</li> <li>models: added supported auth tracking to upstream servers model (7098327)</li> <li>models: all client models have been moved to skaha.models (a07c676)</li> <li>oidc: added tests and made the oidc auth flow async (0059e12)</li> <li>project: This commit renames the project from <code>skaha</code> to <code>canfar</code> to align with the official support from CADC and create a unified naming scheme for the science platform (ecf55fb)</li> <li>session: added new feature to delete sessions with name prefix, kind and status (056254b), closes #37</li> <li>session: added session.events to show deployment events on the cluster, e.g. loading container image etc (58e9bca)</li> <li>sessions: added skaha AsyncSession (2693272)</li> <li>sessions: added support for firefly (87d16c1)</li> <li>tests: add comprehensive tests for OIDC authentication and HTTPx hooks (be7fb50)</li> <li>types: added mode, which reflects the auth type of the client (5781e4a)</li> <li>types: updated to the auth data model (7f293c4)</li> <li>update release-please configuration and versioning for CANFAR (b71a8d1)</li> </ul>"},{"location":"changelog/#bug-fixes_5","title":"Bug Fixes","text":"<ul> <li>api: httpx error catching hook now re-raises the errors rather than execessive printing in stdout (6b7aa71)</li> <li>api: httpx error catching hook now re-raises the errors rather than execessive printing in stdout (827205f)</li> <li>api: updated context, images, overview with httpx (391e857)</li> <li>attestation: added attestation for dockerhub container image (0ff4ba2)</li> <li>auth, config, http: refactor imports and clean up default handling in models (6095a09)</li> <li>auth, config, models: refactor authentication handling and improve configuration properties (aaa8c61)</li> <li>auth: added comprehensive login support (bd11fe4)</li> <li>auth: default x509 cert expiry is now 30 days, similar to cadc-get-cert (f0bd2a7)</li> <li>auth: improve login flow and server selection logic (18b8d6d)</li> <li>auth: oidc auth now properly displays qr code and manual links before trying to open webbrowser (1c756f9)</li> <li>auth: x509 expired now return True when there is no cert to be found (2ab7141)</li> <li>auth: x509 now has better exception handling (a4469b3)</li> <li>badge: update to codeql bagde url (c95b6e0)</li> <li>ci: fix to edge container build (59924bd)</li> <li>ci: improved secret cleanup (990c5a1)</li> <li>cleanup: comments (8a1a078)</li> <li>cli: added a new aliases section to cleanup the l&amp;f of the cli (6165575)</li> <li>cli: added new aliases for list and create, fixed added <code>Terminating</code> state for Status models (a7e946d), closes #103</li> <li>cli: auth now search for dev registries only when running with --dev flag (c490b46)</li> <li>cli: config - added path (30f2617)</li> <li>cli: create options cpus changed to cpu (05081ed)</li> <li>cli: deprecated skaha &amp; cf as entrypoints (6ea71de)</li> <li>cli: desktop-app info problems (6b37351)</li> <li>client, overview: refactor base URL handling to use 'url' attribute instead of 'server' (b429820)</li> <li>client: added fix for better printing of erorr msgs (72dd666)</li> <li>client: changed to the default and max values of parallel conns (ce552bf)</li> <li>client: deprecated client.verify since it no longer affects any logic (9eed6e9)</li> <li>client: fixed annotation issues for client, asyncClient (889a6a8)</li> <li>client: http client now properly adds the refresh hook before the expiry hook (b340308)</li> <li>client: the skaha client on performs x509 checks when creating the clients (b6c3b80)</li> <li>cli: fetch response model, utilization calc (266b8eb)</li> <li>cli: fetch response, now only inits startTime &amp; expiry time to now, if it cannot parse the input timestamps (79ea7f4)</li> <li>cli: fetchResponse model relaxed for expireTime (b082632)</li> <li>cli: fix for stats output print statement (856df95)</li> <li>cli: fixed the max request sizes in stat output (1f55f1e)</li> <li>cli: fixed version selection issue on login (3349c5b)</li> <li>cli: fixed version selection issue on login (79f3ca8)</li> <li>cli: numerous fixes for general look and feel, updated usage and help to be more descriptive (ea59085)</li> <li>cli: ps,list,ls are now sorted by startTime (fdf34ed)</li> <li>cli: relaxed fetch response requirements further (1ea157c)</li> <li>cli: relaxed FetchResponse model requirements (3801578)</li> <li>cli: relaxed requirements for fetchResponse (0856e76)</li> <li>cli: remove canfar list, and canfar rm as aliases due to conflicts with linux commands (e1a4742)</li> <li>cli: stats now report usage instead of requested, and the output is much more aligned with the platform ui (71662ed)</li> <li>cli: updated the discover logic to be cleaner (ebff177)</li> <li>contribution: updated guidelines (bc5400e)</li> <li>correct typos and misleading statements in release notes (d89cdca)</li> <li>dockerfile: fix to stage names (f46b081)</li> <li>docker: fix for docker build due to uv path install changes (00fd5de)</li> <li>docs: added community landing page (03236f7)</li> <li>docs: added ico, logo and fixed docker link to harbor (d73b7c6)</li> <li>docs: changed the location of legacy docs (0ef7e43)</li> <li>docs: cli (a24287d)</li> <li>docs: formatting (62ca659)</li> <li>docs: improve docstrings for clarity and consistency across authentication modules (48599d3)</li> <li>docs: improve formatting of docstring for gather function parameters (9807194)</li> <li>docs: remove slow tests details and clarify slow test marking criteria (18999b7)</li> <li>docs: updated doc/status/badge links (6efed00)</li> <li>docs: updated legacy docs with link fixes and moved around the toc (662b9bd)</li> <li>docs: updated main page based on feedback (96faabf)</li> <li>docs: updated release notes and community reorg (46b819a)</li> <li>docs: updated test architecture docs (4c32ef8)</li> <li>gha: fix for gh-pages push (27aa8f8)</li> <li>github-actions: added fixes for release deployments (dc1b03d)</li> <li>github-actions: added path restrictions for workflow triggers, reduced permissions for build flows (1220765)</li> <li>github-actions: fix for disabling pypi release (078176a)</li> <li>github-actions: migrated worklows to be under opencadc/canfar (0f267f5)</li> <li>github-actions: possible fix for deployment action (41e1886)</li> <li>github-actions: release actions now checkout tag_name ref for code (ebffafe)</li> <li>httpx: updated httpx hook and tests (f0f24e5)</li> <li>httpx: updated httpx hook and tests (7a3621d)</li> <li>Implement httpx error logging hooks and client integration (fee71c2)</li> <li>import: fix for ruff TypeChecking import error (6b5aa6f)</li> <li>init: added CERT_PATH as module global (bb4642d)</li> <li>lint: major improvements to standards (264b9b5)</li> <li>log: fixed a missed f-string (bf31015)</li> <li>logging: moved all modules to use the new logging facility (e05399f)</li> <li>models: added <code>Failed</code> as a status (ef2c34e)</li> <li>models: added desktop-app and contributed to allowed kinds (2b8514f)</li> <li>models: added logging (514fda2)</li> <li>models: create.spec model used in session.create now expects env to be None by default (c34b110)</li> <li>models: CreateSpec (f4cd04f)</li> <li>models: createSpec model now outputs kind with alias type (3184d5c)</li> <li>models: fixes model import errors for pydantic (fd264d4)</li> <li>models: models no longer search for SKAHA_REGISTRY_[USERNAME|SECRET] from environ, this will be supported in future releases with a comprehensive environment variable support accross all configurable variables (a1702c9)</li> <li>models: typo (4c7dbe6)</li> <li>models: update server fields to allow None values and adjust default handling (84b9c5d)</li> <li>models: updated checks for session kind (a8317f4)</li> <li>models: updated client auth and reg models (b1c3f2b)</li> <li>models: updated client config model for better backwards compatibility (e10f007)</li> <li>models: x509 now does a lazy, rather than an eager check for cert path (eb16f76)</li> <li>model: updates to default config to add auth tracking offered by the upstream server (52e291e)</li> <li>mypy: setting extra checks to true (0a29305)</li> <li>oidc: streamline OIDC imports and update token expiry handling (ff0b62d)</li> <li>overview: baseurl fix (4bcb8f4)</li> <li>pre-commit: add autofix argument to pretty-format-json hook (fddeb54)</li> <li>pre-commit: added jwt to mypy (f787507)</li> <li>pre-commit: cleanup (abb0839)</li> <li>pre-commit: updates (fa1b1c4)</li> <li>pyproject: fixed toml file with bad license key (ac1ffb6)</li> <li>readme: codeql bagde url (197a6eb)</li> <li>registry: added keel-dev to discoverable registeries (78ccbe6)</li> <li>registry: added swedish skaha server (cfad0ef)</li> <li>registry: fix for testing to include keel-dev (dcf3cda)</li> <li>release-please: reorder package-name and release-type fields for clarity, and autofix for json formatting (fddeb54)</li> <li>remove image link (a5c9766)</li> <li>security: fixing logging of sensitive info (07ff1d0)</li> <li>security: improvements to assertion checks (2ea1108)</li> <li>security: private init (0fada02)</li> <li>security: removed secret info (dccb1c2)</li> <li>security: restricting ssl context to use TLSv1.2 at a minimum (09654d0)</li> <li>security: X509 certs are checked if valid before first conn. to server (08327a9)</li> <li>session: events now returns None, when verbose=True (1b01dd6)</li> <li>session: fixed kind translation for session.create, increased max replicas limit to 512 (9bfe357)</li> <li>session: fixed set env in session.create (00b67ac)</li> <li>session: fixed sync log output when verbose is True (6ac63f9)</li> <li>session: improved docs, better testing logic to await async sleep (e54b9af)</li> <li>session: solidified the skaha async session api, moved common query building logic to utils.build (3f11aee)</li> <li>skaha-client: deprecated verify field (a70c4b4)</li> <li>style: lint (248565f)</li> <li>tests: consolidated registry tests (80ad530)</li> <li>tests: ensure newline at end of file in test_servers_mixed_status (b50349e)</li> <li>tests: fix for comparing float values (f391833)</li> <li>tests: fix for stdout (6c25d3e)</li> <li>tests: fixed issue with session tests (d004fde)</li> <li>tests: fixed issues with codecov tokens (07f87d9)</li> <li>tests: for bad filenames (f0f4831)</li> <li>tests: import (54ed5fc)</li> <li>tests: refactor authentication tests to use updated model attributes and improve structure (8cfa73f)</li> <li>tests: removed not needed tests (d5be89c)</li> <li>tests: removed now redundant test (651793f)</li> <li>tests: removed now redundant test (c6b519c)</li> <li>tests: stdout related errors (e262eb3)</li> <li>tests: tmp filename (1dd4477)</li> <li>tests: update mock path mkdir lambda to accept arbitrary arguments (2939796)</li> <li>tests: update URL validation tests to reject 'sftp' scheme (9117701)</li> <li>tests: updates (53a0a7e)</li> <li>utils: crypto fix for generating funny names :D (2eff101)</li> <li>utils: fixed logging x2 issue (7e218df)</li> <li>utils: vosi (c925e5a)</li> <li>wip: lint/style/type-hinting (ced042d)</li> <li>x509: fixes for handling unintialized expiry (76e7523)</li> <li>x509: updated x509 auth utils to be all colocated and updated tests (9e5e7c6)</li> </ul>"},{"location":"changelog/#documentation_5","title":"Documentation","text":"<ul> <li>add CanfarSP 2025.1 release notes to homepage (86b1a32)</li> <li>add detailed technical information and links to release notes (003b8a5)</li> <li>api: added better explanation for flexible mode resources (a153418)</li> <li>asyncSession: added docs (a8be7fc)</li> <li>auth: added auth and context docs (65d3c2e)</li> <li>bug-reports: added issue template and docs for reporting bugs (1e177ea)</li> <li>cleanup: links and repeated flexible session info (3080f11)</li> <li>cli: added docs (9a7f20a)</li> <li>client: updated class docstring (5531c6f)</li> <li>client: updated client documentation (74e64ec)</li> <li>client: updates (3db735b)</li> <li>Complete documentation cleanup and integration (c36a599)</li> <li>comprehensive review and updates across user guide sections (0e503c0)</li> <li>contributing: add note on alternative tooling for dependency management (f7683db)</li> <li>css: updated canfar logo (7bdbd75)</li> <li>doi: updated formatting of the doi details section (6133c1a)</li> <li>events: added session.events docs (7540062)</li> <li>flexible: added docs for flexible session support in cli, client and concepts (dfbdf1d)</li> <li>github-actions: changed the workflow name (868e114)</li> <li>helpers: updated docs for distributed.chunk and distributed.stripe usage (a8c15a0)</li> <li>helpers: updated docs to be better for advanced examples (4eb7558)</li> <li>hyperlinks: fixed after docs reorg (4301283)</li> <li>index: typo fixes (bc637c5)</li> <li>index: updated the landing page (e7dbac2)</li> <li>index: updates (a3c2e2e)</li> <li>landing-page: added publications and moved some things to footnotes (6f09786)</li> <li>landing-page: added publications and moved some things to footnotes (afc2336)</li> <li>landing-page: clean up (8ac0080)</li> <li>landing-page: cleanup and dyamic links (41f5aef)</li> <li>landing-page: cleanup and dyamic links (f37887b)</li> <li>pacakge: major updates for docs, merged in science container docs (3b9c12a)</li> <li>release-notes: CSP 2025.1 (d3ed86a)</li> <li>release-notes: fixes (627ee43)</li> <li>session: docstring (7189052)</li> <li>sessions: added docs for destroy_with fucntionality (afd0a11)</li> <li>sessions: consolidate and add session images for notebook, carta, firefly, desktop, transfer_file; fix image paths (a22d435)</li> <li>session: updated docs for session and async sessions (7e49fef)</li> <li>skaha: update (e40bad4)</li> <li>small text chagne (835dc6c)</li> <li>style: updates (e886d77)</li> <li>tests: updated info about slow tests (a4401a3)</li> <li>update for new registry authentication (683d6b6)</li> <li>updates: all over, work-in-progress (8ea1cce)</li> <li>updates: docs (c61fecd)</li> <li>updates: site config + token support (54d6ed9)</li> <li>user-guides----documentations: add legacy index and pages; update guides link; (0867c96)</li> </ul>"},{"location":"changelog/#080-2025-08-08","title":"0.8.0 (2025-08-08)","text":""},{"location":"changelog/#features_2","title":"Features","text":"<ul> <li>auth: add HTTPx authentication hooks for token refresh (489066d)</li> <li>auth: add OIDC authentication hooks for automatic token refresh (bc4596f)</li> <li>auth: added oidc device flow logic (1013cf5)</li> <li>auth: added x509 cert get logic (657a97d)</li> <li>auth: implement async and sync OIDC token refresh functions (b43b05f)</li> <li>auth: server info is now saved for each auth config (1788c1e)</li> <li>cli: added <code>skaha open</code> command to open sessions in a web browser. Made skaha info more readable (c932187)</li> <li>cli: added auth cmds: list, switch, rm &amp; purge (6ab0bd8)</li> <li>cli: added entrypoing cf (cf3fce8)</li> <li>cli: added feat to auto discover skaha servers (46eb565)</li> <li>cli: added list, ls to skaha ps as aliases (c88eb6a)</li> <li>cli: added support for using the CLI as canfar (5051af1)</li> <li>client: add expiry property and enhance SSL context handling for authentication (cb6261a)</li> <li>client: enhance SkahaClient documentation and improve authentication handling (1e6b770)</li> <li>client: significant improvements to the base skaha client to support context managers (a197553)</li> <li>cli: work-in-process (2e646ab)</li> <li>config: updated auth config to provide valid &amp; expired checks for all auth types and added tests (4dc6929)</li> <li>discover: added functionality to discover skaha servers from registries and also added tests (5857772)</li> <li>exceptions: added skaha exception classes (0651191)</li> <li>garble: added fernet and rot13 ciphers to encrypt/obsfucate sensitive info (7204879)</li> <li>helpers: added code to to split tasks for large scale processing (6ffe5cb)</li> <li>hooks: added cli tool typer hook for multiple command aliases (6c5bc15)</li> <li>logging: added comprehensive rich based logger for the project (a48c520)</li> <li>models: added http models for the client (0679e8f)</li> <li>models: all client models have been moved to skaha.models (a07c676)</li> <li>oidc: added tests and made the oidc auth flow async (0059e12)</li> <li>tests: add comprehensive tests for OIDC authentication and HTTPx hooks (be7fb50)</li> <li>types: added mode, which reflects the auth type of the client (5781e4a)</li> <li>types: updated to the auth data model (7f293c4)</li> </ul>"},{"location":"changelog/#bug-fixes_6","title":"Bug Fixes","text":"<ul> <li>auth, config, http: refactor imports and clean up default handling in models (6095a09)</li> <li>auth, config, models: refactor authentication handling and improve configuration properties (aaa8c61)</li> <li>auth: added comprehensive login support (bd11fe4)</li> <li>auth: improve login flow and server selection logic (18b8d6d)</li> <li>auth: x509 expired now return True when there is no cert to be found (2ab7141)</li> <li>auth: x509 now has better exception handling (a4469b3)</li> <li>cli: added a new aliases section to cleanup the l&amp;f of the cli (6165575)</li> <li>cli: added new aliases for list and create, fixed added <code>Terminating</code> state for Status models (a7e946d), closes #103</li> <li>cli: config - added path (30f2617)</li> <li>cli: create options cpus changed to cpu (05081ed)</li> <li>client, overview: refactor base URL handling to use 'url' attribute instead of 'server' (b429820)</li> <li>client: the skaha client on performs x509 checks when creating the clients (b6c3b80)</li> <li>cli: fix for stats output print statement (856df95)</li> <li>cli: numerous fixes for general look and feel, updated usage and help to be more descriptive (ea59085)</li> <li>cli: ps,list,ls are now sorted by startTime (fdf34ed)</li> <li>cli: updated the discover logic to be cleaner (ebff177)</li> <li>docs: improve docstrings for clarity and consistency across authentication modules (48599d3)</li> <li>docs: improve formatting of docstring for gather function parameters (9807194)</li> <li>docs: remove slow tests details and clarify slow test marking criteria (18999b7)</li> <li>docs: updated test architecture docs (4c32ef8)</li> <li>github-actions: fix for disabling pypi release (078176a)</li> <li>github-actions: migrated worklows to be under opencadc/canfar (0f267f5)</li> <li>import: fix for ruff TypeChecking import error (6b5aa6f)</li> <li>init: added CERT_PATH as module global (bb4642d)</li> <li>lint: major improvements to standards (264b9b5)</li> <li>log: fixed a missed f-string (bf31015)</li> <li>logging: moved all modules to use the new logging facility (e05399f)</li> <li>models: added <code>Failed</code> as a status (ef2c34e)</li> <li>models: added desktop-app and contributed to allowed kinds (2b8514f)</li> <li>models: CreateSpec (f4cd04f)</li> <li>models: fixes model import errors for pydantic (fd264d4)</li> <li>models: typo (4c7dbe6)</li> <li>models: update server fields to allow None values and adjust default handling (84b9c5d)</li> <li>models: updated client auth and reg models (b1c3f2b)</li> <li>models: updated client config model for better backwards compatibility (e10f007)</li> <li>models: x509 now does a lazy, rather than an eager check for cert path (eb16f76)</li> <li>mypy: setting extra checks to true (0a29305)</li> <li>oidc: streamline OIDC imports and update token expiry handling (ff0b62d)</li> <li>overview: baseurl fix (4bcb8f4)</li> <li>pre-commit: added jwt to mypy (f787507)</li> <li>pre-commit: updates (fa1b1c4)</li> <li>registry: added swedish skaha server (cfad0ef)</li> <li>security: fixing logging of sensitive info (07ff1d0)</li> <li>security: private init (0fada02)</li> <li>security: removed secret info (dccb1c2)</li> <li>skaha-client: deprecated verify field (a70c4b4)</li> <li>style: lint (248565f)</li> <li>tests: consolidated registry tests (80ad530)</li> <li>tests: ensure newline at end of file in test_servers_mixed_status (b50349e)</li> <li>tests: fix for comparing float values (f391833)</li> <li>tests: fix for stdout (6c25d3e)</li> <li>tests: for bad filenames (f0f4831)</li> <li>tests: import (54ed5fc)</li> <li>tests: refactor authentication tests to use updated model attributes and improve structure (8cfa73f)</li> <li>tests: removed not needed tests (d5be89c)</li> <li>tests: stdout related errors (e262eb3)</li> <li>tests: tmp filename (1dd4477)</li> <li>tests: update mock path mkdir lambda to accept arbitrary arguments (2939796)</li> <li>tests: update URL validation tests to reject 'sftp' scheme (9117701)</li> <li>tests: updates (53a0a7e)</li> <li>utils: crypto fix for generating funny names :D (2eff101)</li> <li>wip: lint/style/type-hinting (ced042d)</li> <li>x509: fixes for handling unintialized expiry (76e7523)</li> <li>x509: updated x509 auth utils to be all colocated and updated tests (9e5e7c6)</li> </ul>"},{"location":"changelog/#documentation_6","title":"Documentation","text":"<ul> <li>auth: added auth and context docs (65d3c2e)</li> <li>bug-reports: added issue template and docs for reporting bugs (1e177ea)</li> <li>cli: added docs (9a7f20a)</li> <li>client: updates (3db735b)</li> <li>contributing: add note on alternative tooling for dependency management (f7683db)</li> <li>helpers: updated docs for distributed.chunk and distributed.stripe usage (a8c15a0)</li> <li>helpers: updated docs to be better for advanced examples (4eb7558)</li> <li>skaha: update (e40bad4)</li> <li>tests: updated info about slow tests (a4401a3)</li> <li>updates: all over, work-in-progress (8ea1cce)</li> </ul>"},{"location":"changelog/#070-2025-05-28","title":"0.7.0 (2025-05-28)","text":""},{"location":"changelog/#features_3","title":"Features","text":"<ul> <li>session: added session.events to show deployment events on the cluster, e.g. loading container image etc (58e9bca)</li> </ul>"},{"location":"changelog/#bug-fixes_7","title":"Bug Fixes","text":"<ul> <li>security: improvements to assertion checks (2ea1108)</li> <li>security: X509 certs are checked if valid before first conn. to server (08327a9)</li> <li>session: events now returns None, when verbose=True (1b01dd6)</li> <li>session: improved docs, better testing logic to await async sleep (e54b9af)</li> </ul>"},{"location":"changelog/#documentation_7","title":"Documentation","text":"<ul> <li>events: added session.events docs (7540062)</li> <li>session: docstring (7189052)</li> </ul>"},{"location":"changelog/#061-2025-05-22","title":"0.6.1 (2025-05-22)","text":""},{"location":"changelog/#bug-fixes_8","title":"Bug Fixes","text":"<ul> <li>cleanup: comments (8a1a078)</li> <li>gha: fix for gh-pages push (27aa8f8)</li> <li>Implement httpx error logging hooks and client integration (fee71c2)</li> <li>pre-commit: cleanup (abb0839)</li> <li>security: restricting ssl context to use TLSv1.2 at a minimum (09654d0)</li> </ul>"},{"location":"changelog/#060-2025-05-12","title":"0.6.0 (2025-05-12)","text":""},{"location":"changelog/#features_4","title":"Features","text":"<ul> <li>client: added loglevel to configure the python logging levels (431f46d)</li> <li>client: added token support to skaha client (6c7c748)</li> <li>client: updated skaha client to use httpx instead of requests (4c10de8)</li> <li>logs: added stdout for printing logs in terminal (0c34cad)</li> <li>sessions: added skaha AsyncSession (2693272)</li> <li>sessions: added support for firefly (87d16c1)</li> </ul>"},{"location":"changelog/#bug-fixes_9","title":"Bug Fixes","text":"<ul> <li>api: updated context, images, overview with httpx (391e857)</li> <li>client: changed to the default and max values of parallel conns (ce552bf)</li> <li>client: deprecated client.verify since it no longer affects any logic (9eed6e9)</li> <li>client: fixed annotation issues for client, asyncClient (889a6a8)</li> <li>models: models no longer search for SKAHA_REGISTRY_[USERNAME|SECRET] from environ, this will be supported in future releases with a comprehensive environment variable support accross all configurable variables (a1702c9)</li> <li>models: updated checks for session kind (a8317f4)</li> <li>session: fixed sync log output when verbose is True (6ac63f9)</li> <li>session: solidified the skaha async session api, moved common query building logic to utils.build (3f11aee)</li> </ul>"},{"location":"changelog/#documentation_8","title":"Documentation","text":"<ul> <li>asyncSession: added docs (a8be7fc)</li> <li>client: updated class docstring (5531c6f)</li> <li>index: typo fixes (bc637c5)</li> <li>index: updates (a3c2e2e)</li> <li>session: updated docs for session and async sessions (7e49fef)</li> <li>updates: docs (c61fecd)</li> <li>updates: site config + token support (54d6ed9)</li> </ul>"},{"location":"changelog/#052-2025-03-03","title":"0.5.2 (2025-03-03)","text":""},{"location":"changelog/#bug-fixes_10","title":"Bug Fixes","text":"<ul> <li>session: fixed kind translation for session.create, increased max replicas limit to 512 (9bfe357)</li> </ul>"},{"location":"changelog/#051-2025-02-26","title":"0.5.1 (2025-02-26)","text":""},{"location":"changelog/#bug-fixes_11","title":"Bug Fixes","text":"<ul> <li>models: createSpec model now outputs kind with alias type (3184d5c)</li> </ul>"},{"location":"changelog/#050-2024-11-22","title":"0.5.0 (2024-11-22)","text":""},{"location":"changelog/#features_5","title":"Features","text":"<ul> <li>context: updates to context.resources api (4d08876)</li> </ul>"},{"location":"changelog/#bug-fixes_12","title":"Bug Fixes","text":"<ul> <li>docker: fix for docker build due to uv path install changes (00fd5de)</li> </ul>"},{"location":"changelog/#documentation_9","title":"Documentation","text":"<ul> <li>style: updates (e886d77)</li> </ul>"},{"location":"changelog/#044-2024-11-22","title":"0.4.4 (2024-11-22)","text":""},{"location":"changelog/#bug-fixes_13","title":"Bug Fixes","text":"<ul> <li>session: fixed set env in session.create (00b67ac)</li> </ul>"},{"location":"changelog/#043-2024-11-08","title":"0.4.3 (2024-11-08)","text":""},{"location":"changelog/#bug-fixes_14","title":"Bug Fixes","text":"<ul> <li>models: create.spec model used in session.create now expects env to be None by default (c34b110)</li> </ul>"},{"location":"changelog/#042-2024-10-30","title":"0.4.2 (2024-10-30)","text":""},{"location":"changelog/#bug-fixes_15","title":"Bug Fixes","text":"<ul> <li>models: added logging (514fda2)</li> </ul>"},{"location":"changelog/#documentation_10","title":"Documentation","text":"<ul> <li>index: updated the landing page (e7dbac2)</li> </ul>"},{"location":"changelog/#040-2024-10-25","title":"0.4.0 (2024-10-25)","text":""},{"location":"changelog/#features_6","title":"Features","text":"<ul> <li>build: added edge container build and attestation (d07e008)</li> <li>codecov: added badge (373412d)</li> <li>conduct: added a code of conduct for skaha community (f37046e)</li> <li>contributions: added a guideline (271a6df)</li> <li>dockerfile: added base dockerfile for the project (28f7e51)</li> <li>docs: added conduct,contributing,license and security sections to docs (5cac3c0)</li> <li>github-actions: added pypi release action and updated client payload (b0b3593)</li> <li>license: project now uses the AGPLv3 license (706f6f8)</li> <li>module: added support for private container registries (3b47c5c)</li> <li>packaging: moved skaha from poetry backend to uv (3b7b89f)</li> <li>security: added a security policy for the project (1338e7f)</li> <li>security: ossf scorecard (719cdfc)</li> <li>session: added new feature to delete sessions with name prefix, kind and status (056254b), closes #37</li> <li>templates: added bug report and feature requests templates (8a8dd20)</li> <li>client: updated client to include skaha version in prep for v1 release (e6360c0)</li> <li>overview: added new overview module (4a6336f)</li> <li>docs: added build (9049b92)</li> <li>session: create session now embeds two env variables into the container, REPLICA_COUNT and REPLICA_ID (ecbf48a)</li> <li>session: added support for multiple session management (219b74c)</li> <li>session: skaha.sessions api deprecated (e184663)</li> <li>release-please: implemented (2ac9728)</li> </ul>"},{"location":"changelog/#bug-fixes_16","title":"Bug Fixes","text":"<ul> <li>attestation: added attestation for dockerhub container image (0ff4ba2)</li> <li>badge: update to codeql bagde url (c95b6e0)</li> <li>ci/cd: bugfixes (b4b153c)</li> <li>ci/cd: fix for docs build (98eea9b)</li> <li>ci/cd: fixes for action deprecations, and uv errors (6a5af8c)</li> <li>CI: change to pre-commit checks (6216b02)</li> <li>ci: ci indent fix (4e02f72)</li> <li>ci: fix to edge container build (59924bd)</li> <li>ci: improved secret cleanup (990c5a1)</li> <li>contribution: updated guidelines (bc5400e)</li> <li>dockerfile: fix to stage names (f46b081)</li> <li>docs/ci: small fixes (e92c9eb)</li> <li>docs: updated doc/status/badge links (6efed00)</li> <li>github-actions: added fixes for release deployments (dc1b03d)</li> <li>github-actions: possible fix for deployment action (41e1886)</li> <li>github-actions: release actions now checkout tag_name ref for code (ebffafe)</li> <li>readme: codeql bagde url (197a6eb)</li> <li>tests: debugging ci/cd and common errors (7d6b3a9)</li> <li>tests: fixed issue with session tests (d004fde)</li> <li>tests: fixed issues with codecov tokens (07f87d9)</li> <li>tests: fixed session tests to be more consistent and run ~60s (19f0a6e)</li> <li>tests: fixed threading issue caused when one of the futures timesout (ba55a38)</li> <li>tests: fixes for session tests (b3f3e48)</li> <li>typing: multiple type hint fixes throughout the project (a533481)</li> <li>utils: fixed logging x2 issue (7e218df)</li> <li>docs: updated docs to include changelog, added reference for calling gpus in session.create (e58f9be)</li> <li>deps: updates (5644e15)</li> <li>session: fix for spawning sessions with gpus (961f766)</li> <li>tests: fixed session tests, which now default spawn with name-{replica-id} format (7e48031)</li> <li>env: fixed multiple tests and added support for multiple env parameters (c0500bf)</li> <li>client: updated session header to have the correct content-type (3146e41)</li> <li>images: images api now always prunes (a436e21)</li> <li>pre-commit: fixed broken pre-commit config (baedb82)</li> <li>type-hints: fixed broken hints (9f4e9db)</li> <li>type-hints: fixed broken type hints (c1d1356)</li> <li>gha: fix to release action (cc7b61a)</li> </ul>"},{"location":"changelog/#documentation_11","title":"Documentation","text":"<ul> <li>github-actions: changed the workflow name (868e114)</li> <li>README: updated with CI status (175ffce)</li> <li>sessions: added docs for destroy_with fucntionality (afd0a11)</li> <li>skaha: updated all docs (04551c9)</li> <li>docs: updates with a new ability to edit docs via PR (aa2314d)</li> <li>readme: update (1b975b6)</li> <li>docs: build command issue (becbc60)</li> <li>docs: fixed build issue (98b0543)</li> <li>docs: created documentation for the project (e0f5483)</li> <li>API: changed where order of docs (569d34f)</li> </ul>"},{"location":"conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behaviour that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behaviour include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behaviour and will take appropriate and fair corrective action in response to any behaviour that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behaviour may be reported to the community leaders responsible for enforcement at shiny.brar@nrc-cnrc.gc.ca. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behaviour deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behaviour was inappropriate. A public apology may be requested.</p>"},{"location":"conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behaviour. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behaviour.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behaviour,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant v2.0</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ and Translations for your preferred language.</p>"},{"location":"contributing/","title":"Contributing to CANFAR","text":"<p>Thank you for considering contributing to the canfar project! We welcome contributions from everyone. Please follow the guidelines below to help us maintain a high-quality codebase.</p> <p>We follow the Contributor Convenant of Code of Conduct. If you wish to contribute to canfar, please make sure to familiarize yourself with it.</p> <p>Contributions are not limited to just code. You can help us by:</p> <ul> <li>Answering questions on the Discussions board</li> <li>Improving the Documentation</li> <li>Reporting bugs and suggesting features via GitHub Issues (see our Bug Reporting Guide for detailed instructions)</li> <li>Spreading the word about CANFAR</li> </ul>"},{"location":"contributing/#how-to-contribute-code","title":"How to Contribute Code","text":""},{"location":"contributing/#1-fork-the-repository","title":"1. Fork the Repository","text":"<p>Start by forking the repository on GitHub. This will create a copy of the project under your GitHub account.</p>"},{"location":"contributing/#2-clone-your-fork","title":"2. Clone Your Fork","text":"<p>Clone your forked repository to your local machine:</p> <pre><code>git clone https://github.com/your-username/canfar.git\ncd canfar\n</code></pre>"},{"location":"contributing/#3-set-up-your-development-environment","title":"3. Set Up Your Development Environment","text":"<ul> <li>CANFAR uses uv for package, project and dependency management. To install uv, please refer to the astral-uv documentation.</li> <li>You need a valid CANFAR account and access to the CANFAR Science Platform. To request access, please request an account with the Canadian Astronomy Data Centre (CADC).</li> </ul> <p>To setup the development environment, simply run:</p> <pre><code>uv python install 3.13\nuv venv --python 3.13\nuv sync --all-extras --dev\n</code></pre> <p>These commands will install the Python version, create a virtual environment, and install all dependencies required for development.</p> <p>Alternative Tooling</p> <p>While this project uses uv for dependency and virtual environment management, you are welcome to use other tools like pip, conda, or virtualenv. The <code>pyproject.toml</code> file contains all the necessary information for these tools to create a compatible environment.</p> <p>Skaha uses pre-commit to manage the development workflow. To install the pre-commit hooks, simply run:</p> <pre><code>uv run pre-commit install --hook-type commit-msg\n</code></pre>"},{"location":"contributing/#contributing-to-documentation","title":"Contributing to Documentation","text":"<p>To contribute to the documentation, you first need to set up the project as described in the \"Set Up Your Development Environment\" section. The command <code>uv sync --all-extras --dev</code> will install all necessary dependencies, including the ones for building the documentation.</p> <p>Once the dependencies are installed, you can build and serve the documentation locally by running:</p> <pre><code>uv run mkdocs build\nuv run mkdocs serve\n</code></pre> <p>This will start a local server, and you can view the documentation in your web browser at the address indicated in the output (usually <code>http://127.0.0.1:8000</code>). The documentation will automatically reload when you make changes to the files.</p>"},{"location":"contributing/#4-make-your-changes","title":"4. Make Your Changes","text":"<p>Make your changes. Please make sure to add tests for your changes if applicable.</p>"},{"location":"contributing/#5-run-the-tests","title":"5. Run the Tests","text":"<p>To run tests for Skaha, you need to have a valid CANFAR account and access to the CANFAR Science Platform. To generate a certificate, please refer to the get started section.  It is expected that the certificate is installed in the default location (<code>~/.ssl/cadcproxy.pem</code>).</p> <pre><code>uv run pytest\n</code></pre>"},{"location":"contributing/#running-tests-efficiently","title":"Running Tests Efficiently","text":"<p>Some tests in the Skaha test suite are marked as \"slow\" because they involve network operations, waiting for session states, or other time-consuming operations. These tests can take several minutes to complete.</p> <p>Run all tests (including slow ones): <pre><code>uv run pytest\n</code></pre></p> <p>Skip slow tests for faster development: <pre><code>uv run pytest -m \"not slow\"\n</code></pre></p> <p>Run only slow tests: <pre><code>uv run pytest -m \"slow\"\n</code></pre></p> <p>The slow tests are primarily integration tests that interact with the CANFAR Science Platform and include: - Session creation and management tests - Log retrieval tests - Authentication timeout tests - Session statistics tests</p> <p>For rapid development and testing, it's recommended to use <code>-m \"not slow\"</code> to skip these time-consuming tests during your development cycle, and run the full test suite before submitting your pull request.</p>"},{"location":"contributing/#6-commit-your-changes","title":"6. Commit Your Changes","text":"<p>Skaha uses the conventional commit messages standard to ensure the commit history human and machine readable. Skaha ships with a tool called <code>commitizen</code> that helps you craft commit messages in the correct format.</p> <pre><code>git add files/you/changed.py\nuv run cz commit\n</code></pre> <p>At this point, you will also see pre-commit hooks running to check your code for any issues and ensure that the code is linted and formatted correctly.</p>"},{"location":"contributing/#7-push-changes-to-your-fork","title":"7. Push Changes to Your Fork","text":"<p>Push your changes to your forked repository:</p> <pre><code>git push\n</code></pre>"},{"location":"contributing/#8-create-a-pull-request","title":"8. Create a Pull Request","text":"<p>Once your changes are pushed to your fork, you can create a pull request from your forked repository to the main Skaha repository. The maintainers will review your changes and merge them if everything is in order.</p>"},{"location":"contributing/#9-celebrate","title":"9. Celebrate","text":"<p>Congratulations! You've made it through the contribution process! Now it's time to celebrate your hard work. Here are a few fun ways to do so:</p> <ul> <li>Dance Party: Put on your favorite tunes and have a solo dance party in your living room. Bonus points for using a disco ball!</li> <li>Snack Attack: Treat yourself to your favorite snack. Whether it's pizza, ice cream, or a healthy smoothie, you deserve it!</li> <li>Virtual High-Five: Send a virtual high-five to your fellow contributors. You can even use a GIF for extra flair!</li> <li>Meme It Up: Create a meme about your contribution journey. Share it in the Discussions board for a good laugh!</li> <li>Celebrate with Code: Write a fun piece of code that does absolutely nothing but prints \"I did it!\" to the console. Because why not?</li> </ul> <p>Remember, every contribution counts, and you\u2019ve just made the Skaha project a little better. Now go forth and celebrate like the coding rockstar you are!</p>"},{"location":"license/","title":"License","text":"<pre><code>                GNU AFFERO GENERAL PUBLIC LICENSE\n                   Version 3, 19 November 2007\n</code></pre> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/  Everyone is permitted to copy and distribute verbatim copies  of this license document, but changing it is not allowed.</p> <pre><code>                        Preamble\n</code></pre> <p>The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works.  By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.</p> <p>When we speak of free software, we are referring to freedom, not price.  Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.</p> <p>A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate.  Many developers of free software are heartened and encouraged by the resulting cooperation.  However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.</p> <p>The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community.  It requires the operator of a network server to provide the source code of the modified version running there to the users of that server.  Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.</p> <p>An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals.  This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p> <pre><code>                   TERMS AND CONDITIONS\n</code></pre> <ol> <li>Definitions.</li> </ol> <p>\"This License\" refers to version 3 of the GNU Affero General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License.  Each licensee is addressed as \"you\".  \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy.  The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy.  Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies.  Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License.  If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p> <ol> <li>Source Code.</li> </ol> <p>The \"source code\" for a work means the preferred form of the work for making modifications to it.  \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form.  A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities.  However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work.  For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p> <ol> <li>Basic Permissions.</li> </ol> <p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met.  This License explicitly affirms your unlimited permission to run the unmodified Program.  The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work.  This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force.  You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright.  Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below.  Sublicensing is not allowed; section 10 makes it unnecessary.</p> <ol> <li>Protecting Users' Legal Rights From Anti-Circumvention Law.</li> </ol> <p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p> <ol> <li>Conveying Verbatim Copies.</li> </ol> <p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p> <ol> <li>Conveying Modified Source Versions.</li> </ol> <p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <pre><code>a) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\n</code></pre> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit.  Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p> <ol> <li>Conveying Non-Source Forms.</li> </ol> <p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <pre><code>a) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\n</code></pre> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling.  In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage.  For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product.  A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source.  The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information.  But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed.  Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p> <ol> <li>Additional Terms.</li> </ol> <p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law.  If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it.  (Additional permissions may be written to require their own removal in certain cases when you modify the work.)  You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <pre><code>a) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\n</code></pre> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10.  If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term.  If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p> <ol> <li>Termination.</li> </ol> <p>You may not propagate or modify a covered work except as expressly provided under this License.  Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License.  If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p> <ol> <li>Acceptance Not Required for Having Copies.</li> </ol> <p>You are not required to accept this License in order to receive or run a copy of the Program.  Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance.  However, nothing other than this License grants you permission to propagate or modify any covered work.  These actions infringe copyright if you do not accept this License.  Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p> <ol> <li>Automatic Licensing of Downstream Recipients.</li> </ol> <p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License.  You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations.  If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License.  For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p> <ol> <li>Patents.</li> </ol> <p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based.  The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version.  For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement).  To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients.  \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License.  You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p> <ol> <li>No Surrender of Others' Freedom.</li> </ol> <p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License.  If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all.  For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p> <ol> <li>Remote Network Interaction; Use with the GNU General Public License.</li> </ol> <p>Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software.  This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.</p> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work.  The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.</p> <ol> <li>Revised Versions of this License.</li> </ol> <p>The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time.  Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number.  If the Program specifies that a certain numbered version of the GNU Affero General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation.  If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions.  However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p> <ol> <li>Disclaimer of Warranty.</li> </ol> <p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p> <ol> <li>Limitation of Liability.</li> </ol> <p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p> <ol> <li>Interpretation of Sections 15 and 16.</li> </ol> <p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <pre><code>                 END OF TERMS AND CONDITIONS\n\n        How to Apply These Terms to Your New Programs\n</code></pre> <p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program.  It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published\nby the Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source.  For example, if your program is a web application, its interface could display a \"Source\" link that leads users to an archive of the code.  There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.</p>"},{"location":"security/","title":"Security Policy","text":""},{"location":"security/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>If you discover a security vulnerability in this project, please report it by sending an email to shiny.brar@nrc-cnrc.gc.ca.</p> <p>We will respond as quickly as possible to address the issue.</p>"},{"location":"security/#security-updates","title":"Security Updates","text":"<p>We will make security updates available as soon as they are ready. Please ensure you are using the latest version of the project to benefit from these updates.</p>"},{"location":"security/#acknowledgments","title":"Acknowledgments","text":"<p>We appreciate the efforts of the community in helping us improve the security of this project.</p>"},{"location":"about/acknowledgement/","title":"Acknowledgement","text":"<p>If you have used CANFAR facilities for your research, please include the following acknowledgement in your publications, theses, and other research outputs.</p> <p>This directly helps secure continued funding and support for the CANFAR project.</p> <p>Citation</p> <p>The authors acknowledge the use of the Canadian Advanced Network for Astronomy Research (CANFAR) Science Platform operated by the Canadian Astronomy Data Centre (CADC) and the Digital Research Alliance of Canada (DRAC), with support from the National Research Council of Canada (NRC), the Canadian Space Agency (CSA), CANARIE, and the Canada Foundation for Innovation (CFI).</p>"},{"location":"about/home/","title":"Organization","text":"<p>The day-to-day operation of the CANFAR platform is coordinated by the CADC teams. The team management consists of:</p> Name Role Sharon Goliath Operations Lead Brian Major Software Development Lead JJ Kavelaars CADC Lead"},{"location":"about/partners/","title":"Partners","text":"<p>The National Research Council of Canada (NRC) is Canada\u2019s premier science agency. It supports the development of cutting-edge technologies and research that drive innovation and economic growth.</p> <p>The NRC is home to the Canadian Astronomy Data Centre and the Canadian Advanced Network for Astronomy Research (CANFAR).</p> <ul> <li> <p></p> <p>CANARIE connects Canada to the world by providing an ultra high-speed network that links Canada\u2019s researchers and educators to each other and to global data, technology, and colleagues through collaboration with government, industry, and international partners.</p> </li> <li> <p></p> <p>Digital Research Alliance Canada drives the acceleration of research and innovation across Canada. By deploying advanced research computing (ARC) systems, storage, and software solutions\u2014delivered in partnership with regional organizations\u2014they provide essential infrastructure for Canadian researchers and their collaborators across academia and industry. Their resources form a cornerstone of CANFAR\u2019s capabilities.</p> </li> <li> <p></p> <p>The Canadian Space Agency supports Canada\u2019s leadership in space astronomy and the growth of the national space technology sector. Through its funding of the Canadian Astronomy Data Centre, it directly supports the operation and advancement of CANFAR.</p> </li> </ul>"},{"location":"about/terms/","title":"Terms of Reference","text":""},{"location":"about/terms/#purpose","title":"Purpose","text":"<p>The Canadian Advanced Network for Astronomy Research is a consortium that serves data-intensive storage, access, and processing needs of university groups and centres engaged in astronomy research.</p> <p>To this end, CANFAR develops and operates user-facing and integrated services such as:</p> <ul> <li>Research Data Management</li> <li>User-managed storage and cloud processing</li> <li>Specialized visualization and analytics services</li> <li>Authentication and Authorization</li> <li>Support to researchers in adapting the system to their needs</li> </ul>"},{"location":"about/terms/#mission","title":"Mission","text":"<ol> <li> <p>To maintain and develop services in data and computationally intensive research that enable Canadian researchers and their international collaborators to generate the greatest possible scientific return on Canada\u2019s investment in telescopes and computational resources.</p> </li> <li> <p>To coordinate efforts and resources in multi-use applications and services across Canadian groups, by generating integrated grant proposals and funding requests that enable the combination and integration of the science application domain (predominantly located at the universities) and the technical expertise (e.g. at CADC). Such grant proposals and funding requests would be directed to any of the Canadian agencies involved in Science, Innovation, Computing and Big Data, such as NSERC, CFI, CANARIE or NRC.</p> </li> <li> <p>Interface on behalf of the astronomy community with Digital Research Alliance Canada to ensure that services meet the astronomy domain specific needs.</p> </li> <li> <p>Reduce barriers and accelerate adoption of new emerging technologies in Big Data and Computing in the astronomy community.</p> </li> <li> <p>Share capabilities developed in the astronomy community with other domains.</p> </li> </ol>"},{"location":"cli/authentication-contexts/","title":"Authentication Guide","text":"<p>CANFAR Python Client and CLI are designed to connect to multiple Science Platform servers around the world. This guide covers everything you need to know about authentication, from basic set-up to advanced scenarios.</p>"},{"location":"cli/authentication-contexts/#authentication-overview","title":"Authentication Overview","text":"<p>CANFAR clients use an Authentication Context system to manage connections to different Science Platform servers. This system supports multiple authentication methods and makes it easy to switch between servers.</p>"},{"location":"cli/authentication-contexts/#what-is-an-authentication-context","title":"What is an Authentication Context?","text":"<p>Think of an authentication context (context for short) as a saved profile that contains:</p> <ul> <li>Server information (URL, capabilities)</li> <li>Authentication credentials (X.509 certificate, OIDC tokens, etc.)</li> <li>User preferences for that specific server</li> </ul> <p>When you use CANFAR, one context is always active, and all commands and API calls are directed to that server.</p>"},{"location":"cli/authentication-contexts/#authentication-methods","title":"Authentication Methods","text":"<p>CANFAR supports several authentication methods:</p> <p>Authentication Methods</p> <ul> <li>X.509 Certificates - Most common, uses <code>.pem</code> certificate files</li> <li>OIDC Tokens - OpenID Connect for modern authentication flows</li> <li>Bearer Tokens - Direct token authentication for API access</li> </ul> <p>Automatic Configuration</p> <p>CANFAR automatically configures the appropriate authentication method based on the server's capabilities and your configuration.</p>"},{"location":"cli/authentication-contexts/#cli-authentication-management","title":"CLI Authentication Management","text":"<p>The CANFAR CLI provides comprehensive commands for managing your authentication contexts.</p>"},{"location":"cli/authentication-contexts/#initial-login-canfar-auth-login","title":"Initial Login (<code>canfar auth login</code>)","text":"<p>The <code>login</code> command is your starting point for connecting to any Science Platform server:</p> <pre><code>canfar auth login\n</code></pre> <p>What happens during login:</p> <ol> <li>Server Discovery - Automatically finds available Science Platform servers worldwide</li> <li>Server Selection - Interactive prompt to choose your target server</li> <li>Authentication Flow - Guides you through the server's authentication method</li> <li>Context Creation - Saves your credentials and server configuration</li> <li>Activation - Sets the new context as active for immediate use</li> </ol> <p>Login Options</p> <pre><code># Basic login with server discovery\ncanfar auth login\n\n# Include development/testing servers\ncanfar auth login --dev\n\n# Include non-responsive servers in discovery\ncanfar auth login --dead\n\n# Show detailed server information during selection\ncanfar auth login --details\n\n# Force re-authentication for existing context\ncanfar auth login --force\n</code></pre>"},{"location":"cli/authentication-contexts/#managing-multiple-contexts","title":"Managing Multiple Contexts","text":"<p>Once you have one or more authentication contexts, you can easily manage them:</p>"},{"location":"cli/authentication-contexts/#listing-contexts-canfar-auth-list","title":"Listing Contexts (<code>canfar auth list</code>)","text":"<p>View all your saved authentication contexts:</p> <pre><code>canfar auth list\n</code></pre> <p>Example Output: <pre><code>                  Available Authentication Contexts                  \n\n  Active   Name          Auth Mode   Server URL                      \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n    \u2705     CADC-CANFAR     x509      https://ws-uv.canfar.net/skaha  \n\n           SRCnet-Sweden   oidc      https://services.swesrc.chalmers.se/skaha\n</code></pre></p> <p>The active context (marked with \u2705) determines where your commands are sent.</p>"},{"location":"cli/authentication-contexts/#switching-contexts-canfar-auth-switch","title":"Switching Contexts (<code>canfar auth switch</code>)","text":"<p>Switch between your saved contexts safely:</p> <pre><code>canfar auth switch &lt;CONTEXT_NAME&gt;\n</code></pre> <p>Switching Examples</p> <pre><code># Switch to a different server\ncanfar auth switch SRCnet-Sweden\n\n# Switch back to CANFAR\ncanfar auth use CADC-CANFAR\n</code></pre> <p>All subsequent commands will use the newly active context.</p>"},{"location":"cli/authentication-contexts/#removing-contexts-canfar-auth-remove","title":"Removing Contexts (<code>canfar auth remove</code>)","text":"<p>Remove contexts you no longer need:</p> <pre><code>canfar auth remove &lt;CONTEXT_NAME&gt;\n</code></pre> <p>Safety Features</p> <ul> <li>You cannot remove the currently active context</li> <li>Switch to a different context first, then remove the unwanted one</li> <li>Removed contexts cannot be recovered (you'll need to login again)</li> </ul>"},{"location":"cli/authentication-contexts/#purging-all-contexts-canfar-auth-purge","title":"Purging All Contexts (<code>canfar auth purge</code>)","text":"<p>Remove all authentication contexts and credentials:</p> <pre><code>canfar auth purge\n</code></pre> <p>Complete Removal</p> <p>This command permanently deletes:</p> <ul> <li>All saved authentication contexts</li> <li>Your entire canfar configuration file (<code>~/.config/canfar/config.yaml</code>)</li> <li>You'll need to login again to use canfar</li> </ul> <p>Options: <pre><code># Skip confirmation prompt\ncanfar auth purge --yes\n\n# Interactive confirmation (default)\ncanfar auth purge\n</code></pre></p>"},{"location":"cli/authentication-contexts/#programmatic-authentication","title":"Programmatic Authentication","text":"<p>Once you have authentication contexts set up via the CLI, you can use them programmatically in your Python code.</p>"},{"location":"cli/authentication-contexts/#using-active-context","title":"Using Active Context","text":"<p>The simplest approach uses your currently active authentication context:</p> <pre><code>from canfar.session import Session\n\n# Uses the active authentication context automatically\nsession = Session()\n\n# Check which context is being used\nprint(f\"Active context: {session.config.active}\")\nprint(f\"Auth Context: {session.config.context}\")\n</code></pre>"},{"location":"cli/authentication-contexts/#authentication-priority","title":"Authentication Priority","text":"<p>When creating a session, canfar follows this priority order:</p> <ol> <li>User-provided token (highest priority)</li> <li>User-provided certificate</li> <li>Active authentication context</li> <li>Default certificate location (<code>~/.ssl/cadcproxy.pem</code>)</li> </ol> <pre><code>from pathlib import Path\nfrom pydantic import SecretStr\nfrom canfar.session import Session\n\n# Priority 1: Direct token (overrides everything)\nsession = Session(token=SecretStr(\"your-bearer-token\"))\n\n# Priority 2: Direct certificate (overrides context)\nsession = Session(certificate=Path(\"/path/to/cert.pem\"))\n\n# Priority 3: Uses active context (most common)\nsession = Session()\n</code></pre>"},{"location":"cli/authentication-contexts/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli/authentication-contexts/#common-authentication-issues","title":"Common Authentication Issues","text":"<p>Login Problems</p> <p>No servers found during discovery</p> <ul> <li>Check your internet connection</li> <li>Try <code>canfar auth login --dead</code> to include non-responsive servers</li> <li>Verify you're not behind a restrictive firewall</li> </ul> <p>Authentication failed</p> <ul> <li>Verify your username and password are correct</li> <li>Check if your account is active on the Science Platform</li> <li>Try logging into the web interface first</li> </ul> <p>Certificate expired</p> <ul> <li>X.509 certificates typically last 10 days</li> <li>Run <code>canfar auth login --force</code> to refresh</li> <li>Check expiry with your authentication status code above</li> </ul> <p>Context Management Issues</p> <p>No active context found</p> <ul> <li>Run <code>canfar auth list</code> to see available contexts</li> <li>Use <code>canfar auth switch &lt;context&gt;</code> to activate one</li> <li>If no contexts exist, run <code>canfar auth login</code></li> </ul> <p>Cannot remove active context</p> <ul> <li>Switch to a different context first: <code>canfar auth switch &lt;other&gt;</code></li> <li>Then remove the unwanted context: <code>canfar auth remove &lt;unwanted&gt;</code></li> </ul>"},{"location":"cli/authentication-contexts/#debug-mode","title":"Debug Mode","text":"<p>Enable detailed authentication logging:</p> <pre><code># CLI debug mode\ncanfar auth login --debug\n</code></pre>"},{"location":"cli/cli-help/","title":"CLI Reference","text":"<p>The CLI provides a comprehensive command-line interface for interacting with the CANFAR Science Platform. This reference covers all available commands and their options.</p> <p>Getting Started</p> <p>The CLI can be accessed using the <code>canfar</code> command in your environment: <pre><code>canfar --help\n</code></pre></p>"},{"location":"cli/cli-help/#main-command","title":"Main Command","text":"<pre><code>canfar [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Description: Command Line Interface for Science Platform.</p>"},{"location":"cli/cli-help/#global-options","title":"Global Options","text":"Option Description <code>--install-completion</code> Install completion for the current shell <code>--show-completion</code> Show completion for the current shell, to copy it or customize the installation <code>--help</code> Show help message and exit <p>Shell Completion</p> <p>Enable shell completion for a better CLI experience by running: <pre><code>canfar --install-completion\n</code></pre></p>"},{"location":"cli/cli-help/#authentication","title":"Authentication","text":""},{"location":"cli/cli-help/#canfar-auth","title":"<code>canfar auth</code>","text":"<p>This command group provides tools for managing authentication contexts for connecting to Science Platform servers.</p>"},{"location":"cli/cli-help/#canfar-auth-login","title":"<code>canfar auth login</code>","text":"<p>Login to Science Platform with automatic server discovery.</p> <pre><code>canfar auth login [OPTIONS]\n</code></pre> <p>Description: This command guides you through the authentication process, automatically discovering the upstream server and choosing the appropriate authentication method based on the server's configuration.</p>"},{"location":"cli/cli-help/#options","title":"Options","text":"Option Type Default Description <code>--force</code> Flag - Force re-authentication <code>--debug</code> Flag - Enable debug logging <code>--dead</code> Flag - Include dead servers in discovery <code>--dev</code> Flag - Include dev servers in discovery <code>--details</code> Flag - Include server details in discovery <code>--timeout</code>, <code>-t</code> INTEGER 2 Timeout for server response <code>--discovery-url</code> TEXT <code>https://ska-iam.stfc.ac.uk/.well-known/openid-configuration</code> OIDC Discovery URL <p>Basic Login</p> <pre><code>canfar auth login\n</code></pre> <p>Login with Debug Information</p> <pre><code>canfar auth login --debug --details\n</code></pre>"},{"location":"cli/cli-help/#canfar-auth-list-canfar-auth-ls","title":"<code>canfar auth list</code> / <code>canfar auth ls</code>","text":"<p>Shows all available authentication contexts.</p> <pre><code>canfar auth list [OPTIONS]\n</code></pre>"},{"location":"cli/cli-help/#canfar-auth-switch-canfar-auth-use","title":"<code>canfar auth switch</code> / <code>canfar auth use</code>","text":"<p>Switch the active authentication context.</p> <pre><code>canfar auth switch CONTEXT\n</code></pre> <p>Arguments:</p> <ul> <li><code>CONTEXT</code> (required): The name of the context to activate</li> </ul> <p>Example</p> <pre><code>canfar auth switch SRCnet-Sweden\n</code></pre>"},{"location":"cli/cli-help/#canfar-auth-remove-canfar-auth-rm","title":"<code>canfar auth remove</code> / <code>canfar auth rm</code>","text":"<p>Remove a specific authentication context from the configuration.</p> <pre><code>canfar auth remove CONTEXT\n</code></pre> <p>Arguments:</p> <ul> <li><code>CONTEXT</code> (required): The name of the context to remove</li> </ul> <p>Permanent Action</p> <p>This action permanently removes the authentication context and cannot be undone.</p>"},{"location":"cli/cli-help/#canfar-auth-purge","title":"<code>canfar auth purge</code>","text":"<p>Remove all authentication contexts and credentials from the configuration.</p> <pre><code>canfar auth purge [OPTIONS]\n</code></pre>"},{"location":"cli/cli-help/#options_1","title":"Options","text":"Option Description <code>--yes</code>, <code>-y</code> Skip confirmation prompt <p>Destructive Action</p> <p>This command removes ALL authentication contexts. Use with caution!</p>"},{"location":"cli/cli-help/#session-management","title":"Session Management","text":""},{"location":"cli/cli-help/#canfar-create","title":"<code>canfar create</code>","text":"<p>Create a new session on the Science Platform.</p> <pre><code>canfar create [OPTIONS] KIND IMAGE [-- CMD [ARGS]...]\n</code></pre> <p>Passing Commands &amp; Arguments</p> <p><pre><code>canfar create headless skaha/terminal:1.1.2 -- python3 /path/to/script.py --arg1\n</code></pre> In this example, <code>python3</code> is the command and <code>/path/to/script.py --arg1</code> are the arguments.</p> <ul> <li>If you need to pass arguments to the command, you must use the <code>--</code> separator.</li> <li>If you don\u2019t supply a <code>CMD</code>, the <code>--</code> is not necessary.</li> </ul> <p>Arguments:</p> <ul> <li><code>KIND</code> (required): Session type - one of: <code>desktop</code>, <code>notebook</code>, <code>carta</code>, <code>headless</code>, <code>firefly</code>, <code>contributed</code></li> <li><code>IMAGE</code> (required): Container image to use</li> <li><code>CMD [ARGS]...</code> (optional): Runtime command and arguments</li> </ul>"},{"location":"cli/cli-help/#options_2","title":"Options","text":"Option Short Type Default Description <code>--name</code> <code>-n</code> TEXT Auto-generated Name of the session <code>--cpu</code> <code>-c</code> INTEGER flexible Number of CPU cores (optional - uses flexible allocation if not specified) <code>--memory</code> <code>-m</code> INTEGER flexible Amount of RAM in GB (optional - uses flexible allocation if not specified) <code>--gpu</code> <code>-g</code> INTEGER None Number of GPUs <code>--env</code> <code>-e</code> TEXT None Environment variables (e.g., <code>--env KEY=VALUE</code>) <code>--replicas</code> <code>-r</code> INTEGER 1 Number of replicas to create <code>--debug</code> - Flag - Enable debug logging <code>--dry-run</code> - Flag - Dry run. Parse parameters and exit <p>Create a Flexible Notebook Session (Default)</p> <pre><code># Uses flexible resource allocation - adapts to user load\ncanfar create notebook skaha/astroml:latest\n</code></pre> <p>Create a Fixed Resource Notebook Session</p> <pre><code># Uses fixed resource allocation - guaranteed resources\ncanfar create --cpu 4 --memory 8 notebook skaha/astroml:latest\n</code></pre> <p>Create a Headless Session with Command &amp; Arguments</p> <pre><code>canfar create headless skaha/terminal:1.1.2 -- python3 /path/to/script.py --arg1 --arg2\n</code></pre> <p>Create Replicated Headless Sessions</p> <pre><code># Create 10 headless replicas running the `env` command\ncanfar create --replicas 10 headless skaha/terminal:1.1.2 -- env\n</code></pre>"},{"location":"cli/cli-help/#resource-allocation-modes","title":"Resource Allocation Modes","text":"<p>CANFAR Science Platform supports two resource allocation modes, see platform concepts for more information.</p>"},{"location":"cli/cli-help/#canfar-ps","title":"<code>canfar ps</code>","text":"<p>Show running sessions.</p> <pre><code>canfar ps [OPTIONS]\n</code></pre>"},{"location":"cli/cli-help/#options_3","title":"Options","text":"Option Short Type Description <code>--all</code> <code>-a</code> Flag Show all sessions (default shows just running) <code>--quiet</code> <code>-q</code> Flag Only show session IDs <code>--kind</code> <code>-k</code> Choice Filter by session kind: <code>desktop</code>, <code>notebook</code>, <code>carta</code>, <code>headless</code>, <code>firefly</code>, <code>desktop-app</code>, <code>contributed</code> <code>--status</code> <code>-s</code> Choice Filter by status: <code>Pending</code>, <code>Running</code>, <code>Terminating</code>, <code>Succeeded</code>, <code>Error</code>, <code>Failed</code> <code>--debug</code> - Flag Enable debug logging <p>List All Sessions</p> <pre><code>canfar ps --all\n</code></pre> <p>List Only Notebook Sessions</p> <pre><code>canfar ps --kind notebook\n</code></pre>"},{"location":"cli/cli-help/#canfar-events","title":"<code>canfar events</code>","text":"<p>Show session events for debugging and monitoring.</p> <pre><code>canfar events [OPTIONS] SESSION_IDS...\n</code></pre> <p>Arguments:</p> <ul> <li><code>SESSION_IDS...</code> (required): One or more session IDs</li> </ul>"},{"location":"cli/cli-help/#options_4","title":"Options","text":"Option Description <code>--debug</code> Enable debug logging <p>Example</p> <pre><code>canfar events abc123 def456\n</code></pre>"},{"location":"cli/cli-help/#canfar-info","title":"<code>canfar info</code>","text":"<p>Show detailed information about sessions.</p> <pre><code>canfar info [OPTIONS] SESSION_IDS...\n</code></pre> <p>Arguments:</p> <ul> <li><code>SESSION_IDS...</code> (required): One or more session IDs</li> </ul>"},{"location":"cli/cli-help/#options_5","title":"Options","text":"Option Description <code>--debug</code> Enable debug logging <p>Example</p> <pre><code>canfar info abc123\n</code></pre>"},{"location":"cli/cli-help/#canfar-open","title":"<code>canfar open</code>","text":"<p>Open sessions in a web browser.</p> <pre><code>canfar open [OPTIONS] SESSION_IDS...\n</code></pre> <p>Arguments:</p> <ul> <li><code>SESSION_IDS...</code> (required): One or more session IDs</li> </ul>"},{"location":"cli/cli-help/#options_6","title":"Options","text":"Option Description <code>--debug</code> Enable debug logging <p>Browser Integration</p> <p>This command automatically opens the session URLs in your default web browser.</p> <p>Example</p> <pre><code>canfar open abc123 def456\n</code></pre>"},{"location":"cli/cli-help/#canfar-logs","title":"<code>canfar logs</code>","text":"<p>Show session logs for troubleshooting.</p> <pre><code>canfar logs [OPTIONS] SESSION_IDS...\n</code></pre> <p>Arguments:</p> <ul> <li><code>SESSION_IDS...</code> (required): One or more session IDs</li> </ul>"},{"location":"cli/cli-help/#options_7","title":"Options","text":"Option Description <code>--debug</code> Enable debug logging <p>Example</p> <pre><code>canfar logs abc123\n</code></pre>"},{"location":"cli/cli-help/#canfar-delete","title":"<code>canfar delete</code>","text":"<p>Delete one or more sessions.</p> <pre><code>canfar delete [OPTIONS] SESSION_IDS...\n</code></pre> <p>Arguments:</p> <ul> <li><code>SESSION_IDS...</code> (required): One or more session IDs to delete</li> </ul>"},{"location":"cli/cli-help/#options_8","title":"Options","text":"Option Short Description <code>--force</code> <code>-f</code> Force deletion without confirmation <code>--debug</code> - Enable debug logging <p>Permanent Action</p> <p>Deleted sessions cannot be recovered. Use <code>--force</code> to skip confirmation prompts.</p> <p>Delete with Confirmation</p> <pre><code>canfar delete abc123\n</code></pre> <p>Force Delete Multiple Sessions</p> <pre><code>canfar delete abc123 def456 --force\n</code></pre>"},{"location":"cli/cli-help/#canfar-prune","title":"<code>canfar prune</code>","text":"<p>Prune sessions by criteria for bulk cleanup.</p> <pre><code>canfar prune [OPTIONS] PREFIX KIND STATUS\n</code></pre> <p>Arguments:</p> <ul> <li><code>PREFIX</code> (required): Prefix to match session names; regex is used if metacharacters are present</li> <li><code>KIND</code> (optional): Session kind - default: <code>headless</code> (one of: <code>desktop</code>, <code>notebook</code>, <code>carta</code>, <code>headless</code>, <code>firefly</code>, <code>desktop-app</code>, <code>contributed</code>)</li> <li><code>STATUS</code> (optional): Session status - default: <code>Succeeded</code> (one of: <code>Pending</code>, <code>Running</code>, <code>Terminating</code>, <code>Succeeded</code>, <code>Error</code>, <code>Failed</code>)</li> </ul>"},{"location":"cli/cli-help/#options_9","title":"Options","text":"Option Short Description <code>--debug</code> - Enable debug logging <code>--help</code> <code>-h</code> Show help message and exit <p>Prune Completed Headless Sessions</p> <pre><code>canfar prune \"test-\" headless Running\ncanfar prune \".*-was-\" notebook Running  # regex because of metacharacters\n</code></pre> <p>Bulk Cleanup</p> <p>Use prune to clean up multiple sessions that match specific criteria, especially useful for automated workflows.</p>"},{"location":"cli/cli-help/#cluster-info","title":"Cluster Info","text":""},{"location":"cli/cli-help/#canfar-stats","title":"<code>canfar stats</code>","text":"<p>Show cluster statistics and resource usage.</p> <pre><code>canfar stats [OPTIONS]\n</code></pre>"},{"location":"cli/cli-help/#options_10","title":"Options","text":"Option Description <code>--debug</code> Enable debug logging <p>Example</p> <pre><code>canfar stats\n</code></pre> <p>Resource Monitoring</p> <p>This command provides insights into cluster resource usage, helping you understand available capacity.</p>"},{"location":"cli/cli-help/#client-configuration","title":"Client Configuration","text":""},{"location":"cli/cli-help/#canfar-config","title":"<code>canfar config</code>","text":"<p>Manage client configuration settings.</p>"},{"location":"cli/cli-help/#canfar-config-show-canfar-config-list-canfar-config-ls","title":"<code>canfar config show</code> / <code>canfar config list</code> / <code>canfar config ls</code>","text":"<p>Display the current configuration.</p> <pre><code>canfar config show [OPTIONS]\n</code></pre> <p>Example</p> <pre><code>canfar config ls\n</code></pre>"},{"location":"cli/cli-help/#canfar-config-path","title":"<code>canfar config path</code>","text":"<p>Display the path to the configuration file.</p> <pre><code>canfar config path [OPTIONS]\n</code></pre> <p>Example</p> <pre><code>canfar config path\n</code></pre> <p>Configuration Location</p> <p>Use this command to find where your configuration file is stored for manual editing if needed.</p>"},{"location":"cli/cli-help/#canfar-config-get","title":"<code>canfar config get</code>","text":"<p>Get a single configuration value by dotted path.</p> <pre><code>canfar config get PATH\n</code></pre> <p>Example</p> <pre><code>canfar config get console.width\n</code></pre>"},{"location":"cli/cli-help/#canfar-config-set","title":"<code>canfar config set</code>","text":"<p>Set a single configuration value by dotted path (value is parsed as YAML).</p> <pre><code>canfar config set PATH VALUE\n</code></pre> <p>Example</p> <pre><code>canfar config set console.width 130\n</code></pre>"},{"location":"cli/cli-help/#canfar-version","title":"<code>canfar version</code>","text":"<p>View client version and system information.</p> <pre><code>canfar version [OPTIONS]\n</code></pre>"},{"location":"cli/cli-help/#options_11","title":"Options","text":"Option Default Description <code>--debug</code> / <code>--no-debug</code> <code>--no-debug</code> Show detailed information for bug reports <p>Basic Version Info</p> <pre><code>canfar version\n</code></pre> <p>Detailed Debug Information</p> <pre><code>canfar version --debug\n</code></pre>"},{"location":"cli/quick-start/","title":"5-Minute Quick Start","text":"<p>Goal</p> <p>By the end of this guide, you'll have a Jupyter Notebook Session on CANFAR with astronomy tools ready to use.</p> <p>Prerequisites</p> <ul> <li>A CADC Account (Canadian Astronomy Data Centre) - Sign up here</li> <li>You have at least once logged into the CANFAR Science Platform and Harbor Container Registry.</li> <li>Python 3.10+</li> <li>Basic familiarity with Python and Jupyter notebooks</li> </ul>"},{"location":"cli/quick-start/#installation","title":"Installation","text":"&gt; pip install canfar --upgradeInstalled"},{"location":"cli/quick-start/#authentication","title":"Authentication","text":"Login to CANFAR Science Platform<pre><code>canfar auth login\n</code></pre> canfar auth loginStarting Science Platform LoginFetched CADC in 0.12sFetched SRCnet in 1.15sDiscovery completed in 3.32s (5/18 active)Select a Canfar Server: (Use arrow keys)   \ud83d\udfe2 Canada  SRCnet   \ud83d\udfe2 UK-CAM  SRCnet   \ud83d\udfe2 Swiss   SRCnet   \ud83d\udfe2 Spain   SRCnet \u00bb \ud83d\udfe2 CANFAR  CADCSelected a Canfar Server: \ud83d\udfe2 CANFAR  CADCX509 Certificate AuthenticationUsername: usernameusername@ws.cadc-ccda.hia-iha.nrc-cnrc.gc.caPassword: ***********\u2713 Saving configurationLogin completed successfully! <p>Login Pathways</p> CADC Users with Existing <code>~/.ssl/cadcproxy.pem</code>SRCnet Users <p>If you\u2019re using the CADC CANFAR Science Platform and already have a valid certificate at <code>~/.ssl/cadcproxy.pem</code>, the CLI will log in automatically</p> <pre><code>Starting Science Platform Login\n\u2713 Credentials valid\n\u2713 Authenticated with CADC-CANFAR @ https://ws-uv.canfar.net/skaha\nUse --force to re-authenticate.\n</code></pre> <p>If you are a SRCnet user, you will be required to go through the OpenID Connect login process in your web browser.</p> <pre><code>Starting Science Platform Login\nFetched CADC in 0.13s\nFetched SRCnet in 1.03s\nDiscovery completed in 3.20s (13/19 active)\n? Select a Canfar Server: \ud83d\udfe2 Canada  SRCnet\nDiscovering capabilities for https://src.canfar.net/skaha\nOIDC Authentication for https://src.canfar.net/skaha\nStarting OIDC Device Authentication\n\u2713 OIDC Configuration discovered successfully\n\u2713 OIDC device registered successfully\n\u2713 Follow the link below to authorize:\n</code></pre> Force Re-Login<pre><code>canfar auth login --force\n</code></pre> <p>What just happened?</p> <ul> <li><code>canfar</code> discovered all available Science Platform servers around the world</li> <li>You selected the <code>CADC CANFAR Server</code></li> <li>You logged into the Science Platform using your CADC credentials</li> <li>The Science Platform generated a certificate for you valid for 30 days</li> <li>The certificate is stored in <code>~/.ssl/cadcproxy.pem</code></li> </ul>"},{"location":"cli/quick-start/#launch-your-first-notebook","title":"Launch Your First Notebook","text":"<p>Lets launch a Jupyter notebook with astronomy tools pre-installed, </p> # Launch a notebook sessioncanfar create notebook skaha/astroml:latestSuccessfully created session 'finish-inmate' (ID: d1tsqexh) <p>What just happened?</p> <ul> <li>We connected to CANFAR using your certificate</li> <li>The CLI defaulted the container image to <code>images.canfar.net/skaha/astroml:latest</code></li> <li>A Jupyter notebook was launched with the container image in flexible mode</li> <li>A random name was generated for your session, <code>finish-inmate</code> in this case</li> <li>The Science Platform allocated flexible resources for your notebook and started it</li> </ul>"},{"location":"cli/quick-start/#peek-under-the-hood","title":"Peek Under the Hood","text":"# Timeline of events taken to launch the notebook sessioncanfar events $(canfar ps -q) <p>What just happened?</p> <ul> <li>We connected to CANFAR using your certificate</li> <li>We queried the Science Platform for all running sessions via <code>canfar ps -q</code></li> <li>We fetched the events (actions performed by the Science Platform to start your session) for your session</li> <li>The events show the progress of your session being created</li> </ul>"},{"location":"cli/quick-start/#check-status","title":"Check Status","text":"canfar ps                                                CANFAR SessionsSESSION ID  NAME          KIND         STATUS    IMAGE                           CREATED\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500d1tsqexh    finish-inmate notebook     Running   skaha/astroml:latest   7 minutes <p>What just happened?</p> <ul> <li>We connected to CANFAR using your certificate</li> <li>The status of your session was checked</li> <li>The session is in <code>Running</code> state, ready to use</li> </ul>"},{"location":"cli/quick-start/#get-session-information","title":"Get Session Information","text":"canfar info $(canfar ps -q)  Session ID    d1tsqexh  Name          finish-inmate  Status        Running  Type          notebook  Image         images.canfar.net/skaha/astroml:latest  User ID       brars  Start Time    13 minutes ago  Expiry Time   3 days and 23.77 hours  Connect URL   https://connect.to/notebook/here  UID           123456789  GID           123456789  Groups        [12345, 67890]  App ID        &lt;none&gt;  CPU Usage     0% of 1 core(s)  RAM Usage     0% of 2G GB  GPU Usage     Not Requested <p>What just happened?</p> <ul> <li>We connected to CANFAR using your certificate</li> <li>The information for your session was fetched</li> <li>When we created your session, we never specified a name, CPU or memory, so flexible mode was used</li> <li>Flexible mode allows your session to adapt its resource usage based on cluster availability</li> <li>The session lifetime defaults to 4 days</li> </ul>"},{"location":"cli/quick-start/#access-your-notebook","title":"Access Your Notebook","text":"<p>Check the status and get the URL to access your notebook:</p> canfar open $(canfar ps -q)Opening session tcgle3m3 in a new tab. <p>What just happened?</p> <ul> <li>We connected to CANFAR using your certificate</li> <li><code>canfar ps -q</code> returns only the session ID of your session</li> <li>Your browser opened the notebook in a new tab</li> </ul> <p>Pro Tip</p> <p>The notebook usually takes 60-120 seconds to start. You can also check status from the command line:</p>"},{"location":"cli/quick-start/#resource-allocation-modes","title":"Resource Allocation Modes","text":"<p>CANFAR Science Platform supports two resource allocation modes, see platform concepts for more information.</p>"},{"location":"cli/quick-start/#start-analyzing","title":"Start Analyzing!","text":"<p>Once your notebook is running, click the URL to open it in your browser. You'll have access to:</p> <ul> <li>Jupyter Lab with a full Python environment</li> <li>Pre-installed astronomy libraries: AstroPy, Matplotlib, SciPy, PyTorch, etc.</li> <li>Storages<ul> <li>Persistent: Your work is automatically saved at <code>/arc/home/username/</code></li> <li>Project: Large datasets shared within your project at <code>/arc/projects/name</code></li> <li>Ephemeral: For temporary data staging, use <code>/scratch/</code></li> </ul> </li> </ul> <p>Try This First</p> <p>In JupyterLab, open a new Notebook and run the following code to verify your environment:</p> <pre><code>import astropy\nfrom astropy.io import fits\nimport matplotlib\nimport numpy as np\n\nprint(f\"AstroPy version: {astropy.__version__}\")\nprint(f\"Matplotlib version: {matplotlib.__version__}\")\nprint(f\"Numpy version: {np.__version__}\")\nprint(f\"GPU available: {torch.cuda.is_available()}\")\nprint(\"Ready for astronomy!\")\n</code></pre>"},{"location":"cli/quick-start/#clean-up","title":"Clean Up","text":"<p>When you're done, clean up your session to free up resources for others:</p> canfar delete $(canfar ps -q)Confirm deletion of 1 session(s)? [y/n] (n): ySuccessfully deleted {'tcgle3m3': True} session(s)."},{"location":"cli/quick-start/#congratulations","title":"Congratulations!","text":"<p>You now have a fully-equipped astronomy computing environment running in the cloud. No software installation, no environment conflicts, no waiting for local resources.</p>"},{"location":"cli/quick-start/#troubleshooting","title":"Troubleshooting","text":"<p>Common Issues</p> <ul> <li>Notebook won't start?<ul> <li>Check available resources: <code>canfar stats</code></li> <li>Try flexible mode (default) for faster scheduling</li> <li>If using fixed mode, try smaller resource values (fewer cores/RAM)</li> <li>Check session status: <code>canfar ps</code></li> </ul> </li> <li>Can't access notebook URL?<ul> <li>Wait 1-2 minutes for full startup</li> <li>Check if you're on a VPN that might block the connection</li> <li>Verify the session is in \"Running\" status</li> </ul> </li> <li>Variable performance in flexible mode?<ul> <li>This is normal - performance adapts to cluster load</li> <li>For consistent performance, use fixed mode with specific <code>--cpu</code> and <code>--memory</code> values</li> </ul> </li> </ul>"},{"location":"client/advanced-examples/","title":"Advanced Examples","text":"<p>Complex use cases and power-user examples for CANFAR Science Platform.</p>"},{"location":"client/advanced-examples/#quick-start","title":"Quick Start","text":"<p>Info</p> <p><code>canfar</code> automatically sets these environment variables in each container:</p> <ul> <li><code>REPLICA_ID</code>: Current container ID (1, 2, 3, ...)</li> <li><code>REPLICA_COUNT</code>: Total number of containers</li> </ul>"},{"location":"client/advanced-examples/#massively-parallel-processing","title":"Massively Parallel Processing","text":"<p>Let's assume you have a large dataset of 1000 FITS files that you want to process in parallel. You have a Python script that can process a single FITS file, and you want to run this script in parallel on 100 different CANFAR sessions, with each container processing a subset of the files. This is a common pattern for distributed computing on CANFAR, and can be achieved with a few lines of code.</p> Batch Processing Script<pre><code>from canfar.helpers import distributed\nfrom glob import glob\nfrom your.code import analysis\n\n# Find all FITS files to process\ndatafiles = glob(\"/path/to/data/files/*.fits\")\n\n# Each replica processes its assigned chunk of files\n# The chunk function automatically handles 1-based REPLICA_ID values\nfor datafile in distributed.chunk(datafiles):\n    analysis(datafile)\n</code></pre>"},{"location":"client/advanced-examples/#large-scale-parallel-processing","title":"Large Scale Parallel Processing","text":"Flexible Mode (Recommended) Fixed Mode CLI Flexible Mode CLI Fixed Mode <pre><code>from canfar.session import AsyncSession\n\nasync with AsyncSession() as session:\n    # Flexible resource allocation - adapts to cluster availability\n    sessions = await session.create(\n        name=\"fits-processing\",\n        image=\"images.canfar.net/your/analysis-container:latest\",\n        kind=\"headless\",\n        cmd=\"python\",\n        args=[\"/path/to/batch_processing.py\"],\n        replicas=100,\n    )\n    return sessions\n</code></pre> <pre><code>from canfar.session import AsyncSession\n\nasync with AsyncSession() as session:\n    # Fixed resource allocation - guaranteed resources\n    sessions = await session.create(\n        name=\"fits-processing\",\n        image=\"images.canfar.net/your/analysis-container:latest\",\n        kind=\"headless\",\n        cores=8,\n        ram=32,\n        cmd=\"python\",\n        args=[\"/path/to/batch_processing.py\"],\n        replicas=100,\n    )\n    return sessions\n</code></pre> <pre><code># Flexible resource allocation (default)\ncanfar create -r 100 -n fits-processing headless images.canfar.net/your/analysis-container:latest -- python /path/to/batch_processing.py\n</code></pre> <pre><code># Fixed resource allocation\ncanfar create -c 8 -m 32 -r 100 -n fits-processing headless images.canfar.net/your/analysis-container:latest -- python /path/to/batch_processing.py\n</code></pre>"},{"location":"client/advanced-examples/#advanced-resource-allocation-strategies","title":"Advanced Resource Allocation Strategies","text":"<p>For complex workflows, choosing the right resource allocation mode can significantly impact performance:</p>"},{"location":"client/advanced-examples/#mixed-resource-allocation","title":"Mixed Resource Allocation","text":"<p>You can combine flexible and fixed modes within the same workflow:</p> <pre><code>from canfar.sessions import AsyncSession\n\nasync def mixed_workflow():\n    async with AsyncSession() as session:\n        # Use flexible mode for data preprocessing (variable workload)\n        preprocessing_sessions = await session.create(\n            name=\"preprocess\",\n            image=\"images.canfar.net/your/preprocessing:latest\",\n            kind=\"headless\",\n            cmd=\"python\",\n            args=[\"preprocess.py\"],\n            replicas=50,\n        )\n\n        # Use fixed mode for intensive analysis (predictable workload)\n        analysis_sessions = await session.create(\n            name=\"analysis\",\n            image=\"images.canfar.net/your/analysis:latest\",\n            kind=\"headless\",\n            cores=16,\n            ram=64,\n            cmd=\"python\",\n            args=[\"analyze.py\"],\n            replicas=10,\n        )\n\n        return preprocessing_sessions + analysis_sessions\n</code></pre>"},{"location":"client/advanced-examples/#resource-allocation-guidelines-for-advanced-workflows","title":"Resource Allocation Guidelines for Advanced Workflows","text":"Workflow Type Recommended Mode Reasoning Data Preprocessing Flexible Variable I/O patterns, benefits from burst capacity Machine Learning Training Fixed Consistent performance needed for convergence Monte Carlo Simulations Flexible Independent tasks, can handle variable performance Image Processing Pipelines Fixed Memory-intensive, predictable resource needs Interactive Development Flexible Exploratory work, cost-effective Production Batch Jobs Fixed Reliable performance for scheduled workflows"},{"location":"client/advanced-examples/#distributed-processing-strategies","title":"Distributed Processing Strategies","text":"<p>The <code>canfar.helpers.distributed</code> module provides two main strategies for distributing data across replicas:</p>"},{"location":"client/advanced-examples/#chunking-distributedchunk","title":"Chunking (<code>distributed.chunk</code>)","text":"<p>The <code>chunk</code> function divides your data into contiguous blocks, with each replica processing a consecutive chunk. The function uses 1-based replica IDs (matching <code>canfar</code> <code>REPLICA_ID</code> environment variable):</p> Chunking Example<pre><code>from canfar.helpers import distributed\n\n# With 1000 files and 100 replicas:\n# - Replica 1 processes files 0-9\n# - Replica 2 processes files 10-19  \n# - Replica 3 processes files 20-29\n# - And so on...\n\ndatafiles = glob(\"/path/to/data/*.fits\")\nfor datafile in distributed.chunk(datafiles):\n    process_datafile(datafile)\n</code></pre>"},{"location":"client/advanced-examples/#striping-distributedstripe","title":"Striping (<code>distributed.stripe</code>)","text":"<p>The <code>stripe</code> function distributes data in a round-robin fashion, which is useful when file sizes vary significantly:</p> Striping Example<pre><code>from canfar.helpers import distributed\n\n# With 1000 files and 100 replicas:\n# - Replica 1 processes files 0, 100, 200, 300, ...\n# - Replica 2 processes files 1, 101, 201, 301, ...\n# - Replica 3 processes files 2, 102, 202, 302, ...\n# - And so on...\n\ndatafiles = glob(\"/path/to/data/*.fits\")\nfor datafile in distributed.stripe(datafiles):\n    process_datafile(datafile)\n</code></pre>"},{"location":"client/advanced-examples/#when-to-use-each-strategy","title":"When to Use Each Strategy","text":"<ul> <li>Use <code>chunk</code> when files are similar in size and you want each replica to process a contiguous block of data</li> <li>Use <code>stripe</code> when file sizes vary significantly, as it distributes the workload more evenly across replicas</li> </ul>"},{"location":"client/advanced-examples/#real-world-example-processing-astronomical-data","title":"Real-World Example: Processing Astronomical Data","text":"<pre><code>import os\nimport json\nfrom pathlib import Path\nfrom canfar.helpers.distributed import chunk\n\ndef process_observations():\n    \"\"\"Process FITS files across multiple containers.\"\"\"\n\n    # Get all observation files\n    fits_files = list(Path(\"/data/observations\").glob(\"*.fits\"))\n    my_files = list(chunk(fits_files))\n\n    if not my_files:\n        print(\"No files assigned to this container\")\n        return\n\n    replica_id = os.environ.get('REPLICA_ID')\n    print(f\"Container {replica_id} processing {len(my_files)} files\")\n\n    # Process each file\n    results = []\n    for fits_file in my_files:\n        # Your analysis code here\n        result = {\"file\": fits_file.name, \"stars_detected\": analyze_fits(fits_file)}\n        results.append(result)\n\n    # Save results with container ID\n    output_file = f\"/results/container_{replica_id}_results.json\"\n    with open(output_file, 'w') as f:\n        json.dump(results, f, indent=2)\n\n    print(f\"Saved {len(results)} results to {output_file}\")\n\ndef analyze_fits(fits_path):\n    \"\"\"Your FITS analysis logic here.\"\"\"\n    return 42  # Placeholder\n</code></pre>"},{"location":"client/advanced-examples/#best-practices","title":"Best Practices","text":"<p>Choose the right function: - Use <code>chunk()</code> when you need contiguous data blocks - Use <code>stripe()</code> for round-robin distribution</p> <p>Handle empty containers: <pre><code>my_data = list(chunk(data))\nif not my_data:\n    print(\"No data for this container\")\n    return\n</code></pre></p> <p>Save results with container ID: <pre><code>import os\nreplica_id = os.environ.get('REPLICA_ID')\noutput_file = f\"/results/container_{replica_id}_results.json\"\n</code></pre></p> <p>Combine results from all containers: <pre><code>from pathlib import Path\nimport json\n\ndef combine_results():\n    \"\"\"Merge results from all containers.\"\"\"\n    all_results = []\n    for result_file in Path(\"/results\").glob(\"container_*_results.json\"):\n        with open(result_file) as f:\n            all_results.extend(json.load(f))\n\n    with open(\"/results/final_results.json\", 'w') as f:\n        json.dump(all_results, f, indent=2)\n</code></pre></p>"},{"location":"client/advanced-examples/#common-issues","title":"Common Issues","text":"<p>Some containers get no data This happens when you have more containers than data items. Handle it gracefully: <pre><code>my_data = list(chunk(data))\nif not my_data:\n    print(\"No data assigned to this container\")\n    return\n</code></pre></p> <p>Debugging distribution <pre><code>import os\nreplica_id = os.environ.get('REPLICA_ID')\nreplica_count = os.environ.get('REPLICA_COUNT')\nprint(f\"Container {replica_id} of {replica_count} processing {len(my_data)} items\")\n</code></pre></p>"},{"location":"client/async_session/","title":"Asynchronous Sessions","text":"<p>Overview</p> <p><code>canfar</code> supports asynchronous sessions using the <code>AsyncSession</code> class while maintaining 1-to-1 compatibility with the <code>Session</code> class.</p> <p>               Bases: <code>HTTPClient</code></p> <p>Asynchronous CANFAR Session Management Client.</p> <p>This class provides methods to manage sessions in the system, including fetching session details, creating new sessions, retrieving logs, and destroying existing sessions.</p> <p>This class is a subclass of the <code>HTTPClient</code> class and inherits its attributes and methods.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession(\n        server=\"https://something.example.com\",\n        version=\"v1\",\n        token=\"token\",\n        timeout=30,\n        concurrency=100,\n        loglevel=40,\n    )\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>class AsyncSession(HTTPClient):\n    \"\"\"Asynchronous CANFAR Session Management Client.\n\n    This class provides methods to manage sessions in the system,\n    including fetching session details, creating new sessions,\n    retrieving logs, and destroying existing sessions.\n\n    This class is a subclass of the `HTTPClient` class and inherits its\n    attributes and methods.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession(\n                server=\"https://something.example.com\",\n                version=\"v1\",\n                token=\"token\",\n                timeout=30,\n                concurrency=100,\n                loglevel=40,\n            )\n    \"\"\"\n\n    async def fetch(\n        self,\n        kind: Kind | None = None,\n        status: Status | None = None,\n        view: View | None = None,\n    ) -&gt; list[dict[str, str]]:\n        \"\"\"List open sessions for the user.\n\n        Args:\n            kind (Kind | None, optional): Session kind. Defaults to None.\n            status (Status | None, optional): Session status. Defaults to None.\n            view (View | None, optional): Session view level. Defaults to None.\n\n        Notes:\n            By default, only the calling user's sessions are listed. If views is\n            set to 'all', all user sessions are listed (with limited information).\n\n        Returns:\n            list: Sessions information.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; await session.fetch(kind=\"notebook\")\n            [{'id': 'vl91sfzz',\n            'userid': 'brars',\n            'runAsUID': '166169204',\n            'runAsGID': '166169204',\n            'supplementalGroups': [34241,\n            34337,\n            35124,\n            36227,\n            1902365706,\n            1454823273,\n            1025424273],\n            'appid': '&lt;none&gt;',\n            'image': 'image-server/repo/image:version',\n            'type': 'notebook',\n            'status': 'Running',\n            'name': 'notebook1',\n            'startTime': '2025-03-05T21:48:29Z',\n            'expiryTime': '2025-03-09T21:48:29Z',\n            'connectURL': 'https://canfar.net/session/notebook/some/url',\n            'requestedRAM': '8G',\n            'requestedCPUCores': '2',\n            'requestedGPUCores': '0',\n            'ramInUse': '&lt;none&gt;',\n            'gpuRAMInUse': '&lt;none&gt;',\n            'cpuCoresInUse': '&lt;none&gt;',\n            'gpuUtilization': '&lt;none&gt;'}]\n        \"\"\"\n        parameters: dict[str, Any] = build.fetch_parameters(kind, status, view)\n        response: Response = await self.asynclient.get(url=\"session\", params=parameters)\n        data: list[dict[str, str]] = response.json()\n        log.debug(data)\n        return data\n\n    async def stats(self) -&gt; dict[str, Any]:\n        \"\"\"Get statistics for the canfar cluster.\n\n        Returns:\n            Dict[str, Any]: Cluster statistics.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; await session.stats()\n            {'cores': {'requestedCPUCores': 377,\n             'coresAvailable': 960,\n             'maxCores': {'cores': 32, 'withRam': '147Gi'}},\n             'ram': {'maxRAM': {'ram': '226Gi', 'withCores': 32}}}\n        \"\"\"\n        parameters = {\"view\": \"stats\"}\n        response: Response = await self.asynclient.get(\"session\", params=parameters)\n        data: dict[str, Any] = response.json()\n        log.debug(data)\n        return data\n\n    async def info(self, ids: list[str] | str) -&gt; list[dict[str, Any]]:\n        \"\"\"Get information about session[s].\n\n        Args:\n            ids (Union[List[str], str]): Session ID[s].\n\n        Returns:\n            Dict[str, Any]: Session information.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; await session.info(session_id=\"hjko98yghj\")\n            &gt;&gt;&gt; await session.info(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        # Convert id to list if it is a string\n        if isinstance(ids, str):\n            ids = [ids]\n        results: list[dict[str, Any]] = []\n        tasks: list[Any] = []\n        semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n\n        async def bounded(value: str) -&gt; dict[str, Any]:\n            async with semaphore:\n                response = await self.asynclient.get(url=f\"session/{value}\")\n                data: dict[str, Any] = response.json()\n                return data\n\n        tasks = [bounded(value) for value in ids]\n        responses = await asyncio.gather(*tasks, return_exceptions=True)\n        for reply in responses:\n            if isinstance(reply, Exception):\n                log.error(reply)\n            elif isinstance(reply, dict):\n                results.append(reply)\n        log.debug(results)\n        return results\n\n    async def logs(\n        self,\n        ids: list[str] | str,\n        verbose: bool = False,\n    ) -&gt; dict[str, str] | None:\n        \"\"\"Get logs from a session[s].\n\n        Args:\n            ids (Union[List[str], str]): Session ID[s].\n            verbose (bool, optional): Print logs to stdout. Defaults to False.\n\n        Returns:\n            Dict[str, str]: Logs in text/plain format.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; await session.logs(id=\"hjko98yghj\")\n            &gt;&gt;&gt; await session.logs(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        if isinstance(ids, str):\n            ids = [ids]\n        parameters: dict[str, str] = {\"view\": \"logs\"}\n        results: dict[str, str] = {}\n\n        semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n        tasks: list[Any] = []\n\n        async def bounded(value: str) -&gt; tuple[str, str]:\n            async with semaphore:\n                response = await self.asynclient.get(\n                    url=f\"session/{value}\",\n                    params=parameters,\n                )\n                return value, response.text\n\n        tasks = [bounded(value) for value in ids]\n        responses = await asyncio.gather(*tasks, return_exceptions=True)\n        for reply in responses:\n            if isinstance(reply, Exception):\n                log.error(reply)\n            elif isinstance(reply, tuple):\n                results[reply[0]] = reply[1]\n\n        # Print logs to stdout if verbose is set to True\n        if verbose:\n            for key, value in results.items():\n                log.info(\"Session ID: %s\\n\", key)\n                log.info(value)\n            return None\n        return results\n\n    async def create(\n        self,\n        name: str,\n        image: str,\n        cores: int | None = None,\n        ram: int | None = None,\n        kind: Kind = \"headless\",\n        gpu: int | None = None,\n        cmd: str | None = None,\n        args: str | None = None,\n        env: dict[str, Any] | None = None,\n        replicas: int = 1,\n    ) -&gt; list[str]:\n        \"\"\"Launch a canfar session.\n\n        Args:\n            name (str): A unique name for the session.\n            image (str): Container image to use for the session.\n            cores (int, optional): Number of cores.\n                Defaults to None, i.e. flexible mode.\n            ram (int, optional): Amount of RAM (GB).\n                Defaults to None, i.e. flexible mode.\n            kind (str, optional): Type of canfar session. Defaults to \"headless\".\n            gpu (Optional[int], optional): Number of GPUs. Defaults to None.\n            cmd (Optional[str], optional): Command to run. Defaults to None.\n            args (Optional[str], optional): Arguments to the command. Defaults to None.\n            env (Optional[Dict[str, Any]], optional): Environment variables to inject.\n                Defaults to None.\n            replicas (int, optional): Number of sessions to launch. Defaults to 1.\n\n        Notes:\n            - If cores and ram are not specified, the session will be created with\n              flexible resource allocation of upto 8 cores and 32GB of RAM.\n            - The name of the session suffixed with the replica number. eg. test-42\n              when replicas &gt; 1.\n            - Each container will have the following environment variables injected:\n                * REPLICA_ID - The replica number\n                * REPLICA_COUNT - The total number of replicas\n\n        Returns:\n            List[str]: A list of session IDs for the launched sessions.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; session.create(\n                    name=\"test\",\n                    image='images.canfar.net/skaha/terminal:1.1.1',\n                    cores=2,\n                    ram=8,\n                    gpu=1,\n                    kind=\"headless\",\n                    cmd=\"env\",\n                    env={\"TEST\": \"test\"},\n                    replicas=2,\n                )\n            &gt;&gt;&gt; [\"hjko98yghj\", \"ikvp1jtp\"]\n        \"\"\"\n        payloads: list[list[tuple[str, Any]]] = build.create_parameters(\n            name,\n            image,\n            cores,\n            ram,\n            kind,\n            gpu,\n            cmd,\n            args,\n            env,\n            replicas,\n        )\n        results: list[str] = []\n        tasks: list[Any] = []\n        semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n\n        async def bounded(parameters: list[tuple[str, Any]]) -&gt; Any:\n            async with semaphore:\n                log.debug(\"HTTP Request Parameters: %s\", parameters)\n                response = await self.asynclient.post(url=\"session\", params=parameters)\n                return response.text.rstrip(\"\\r\\n\")\n\n        tasks = [bounded(payload) for payload in payloads]\n        msg = f\"Creating {replicas} {kind} session[s].\"\n        log.debug(msg)\n        responses = await asyncio.gather(*tasks, return_exceptions=True)\n        for reply in responses:\n            if isinstance(reply, Exception):\n                log.error(reply)\n            elif isinstance(reply, str):\n                results.append(reply)\n        log.debug(results)\n        return results\n\n    async def events(\n        self,\n        ids: str | list[str],\n        verbose: bool = False,\n    ) -&gt; list[dict[str, str]] | None:\n        \"\"\"Get deployment events for a session[s].\n\n        Args:\n            ids (Union[str, List[str]]): Session ID[s].\n            verbose (bool, optional): Print events to stdout. Defaults to False.\n\n        Returns:\n            Optional[List[Dict[str, str]]]: A list of events for the session[s].\n\n        Notes:\n            When verbose is True, the events will be printed to stdout only.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; await session.events(id=\"hjko98yghj\")\n            &gt;&gt;&gt; await session.events(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        if isinstance(ids, str):\n            ids = [ids]\n        results: list[dict[str, str]] = []\n        parameters: dict[str, str] = {\"view\": \"events\"}\n        tasks: list[Any] = []\n        semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n\n        async def bounded(value: str) -&gt; dict[str, str]:\n            async with semaphore:\n                response = await self.asynclient.get(\n                    url=f\"session/{value}\",\n                    params=parameters,\n                )\n                return {value: response.text}\n\n        tasks = [bounded(value) for value in ids]\n        responses = await asyncio.gather(*tasks, return_exceptions=True)\n        for reply in responses:\n            if isinstance(reply, Exception):\n                log.error(reply)\n            elif isinstance(reply, dict):\n                results.append(dict(reply))\n\n        if verbose and results:\n            for result in results:\n                for key, value in result.items():\n                    log.info(\"Session ID: %s\", key)\n                    log.info(value)\n        log.debug(results)\n        return results if not verbose else None\n\n    async def destroy(self, ids: str | list[str]) -&gt; dict[str, bool]:\n        \"\"\"Destroy session[s].\n\n        Args:\n            ids (Union[str, List[str]]): Session ID[s].\n\n        Returns:\n            Dict[str, bool]: A dictionary of session IDs\n            and a bool indicating if the session was destroyed.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; await session.destroy(id=\"hjko98yghj\")\n            &gt;&gt;&gt; await session.destroy(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        if isinstance(ids, str):\n            ids = [ids]\n        results: dict[str, bool] = {}\n        semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n        tasks: list[Any] = []\n\n        async def bounded(value: str) -&gt; tuple[str, bool]:\n            async with semaphore:\n                try:\n                    await self.asynclient.delete(url=f\"session/{value}\")\n                except HTTPError as err:\n                    msg = f\"Failed to destroy session {value}: {err}\"\n                    log.exception(msg)\n                    return value, False\n                else:\n                    return value, True\n\n        tasks = [bounded(value) for value in ids]\n        responses = await asyncio.gather(*tasks, return_exceptions=True)\n        for reply in responses:\n            if isinstance(reply, tuple):\n                results[reply[0]] = reply[1]\n        log.debug(results)\n        return results\n\n    async def destroy_with(\n        self,\n        prefix: str,\n        kind: Kind = \"headless\",\n        status: Status = \"Completed\",\n    ) -&gt; dict[str, bool]:\n        \"\"\"Destroy session[s] matching a prefix or regex pattern.\n\n        Args:\n            prefix (str): Prefix to match.\n                Treated literally unless regex meta-characters are found.\n            kind (Kind): Type of session. Defaults to \"headless\".\n            status (Status): Status of the session. Defaults to \"Completed\".\n\n\n        Returns:\n            Dict[str, bool]: A dictionary of session IDs\n            and a bool indicating if the session was destroyed.\n\n        Notes:\n            - If the value contains regex metacharacters (e.g., `.^$*+?{}[]()|`), it is\n                treated as a regex with :func:`re.search`.\n            - Otherwise it is treated as a literal prefix (anchored with `^`).\n            This method is useful for destroying multiple sessions at once.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; await session.destroy_with(prefix=\"test\")  # literal prefix\n            &gt;&gt;&gt; await session.destroy_with(prefix=\"desktop$\")  # regex\n        \"\"\"\n        meta: bool = any(char in set(\".^$*+?{}[]()|\") for char in prefix)\n        try:\n            if meta:\n                log.info(\"Regex Pattern: %s\", prefix)\n                regex = re.compile(prefix)\n            else:\n                log.info(\"Literal Prefix: %s\", prefix)\n                regex = re.compile(rf\"^{re.escape(prefix)}\")\n        except re.error as err:\n            msg = f\"Invalid regex pattern '{prefix}': {err}\"\n            log.exception(msg)\n            raise ValueError(msg) from err\n\n        ids: list[str] = [\n            session[\"id\"]\n            for session in await self.fetch(kind=kind, status=status)\n            if regex.search(session[\"name\"])\n        ]\n        return await self.destroy(ids)\n\n    async def connect(self, ids: list[str] | str) -&gt; None:\n        \"\"\"Connect to a session[s] in a web browser.\n\n        Args:\n            ids (Union[List[str], str]): Session ID[s].\n\n        Examples:\n            &gt;&gt;&gt; from canfar.sessions import AsyncSession\n            &gt;&gt;&gt; session = AsyncSession()\n            &gt;&gt;&gt; await session.connect(ids=\"hjko98yghj\")\n            &gt;&gt;&gt; await session.connect(ids=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        if isinstance(ids, str):\n            ids = [ids]\n        info = await self.info(ids)\n        log.debug(info)\n        for session in info:\n            status: str = session.get(\"status\", \"unknown\")\n            if status != \"Running\":\n                log.warning(\"Session %s is currently %s.\", session[\"id\"], status)\n                log.warning(\"Please wait for the session to be ready.\")\n                continue\n            connect_url = session.get(\"connectURL\")\n            if connect_url:\n                open_new_tab(connect_url)\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.connect","title":"connect  <code>async</code>","text":"<pre><code>connect(ids)\n</code></pre> <p>Connect to a session[s] in a web browser.</p> PARAMETER DESCRIPTION <code>ids</code> <p>Session ID[s].</p> <p> TYPE: <code>Union[List[str], str]</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.sessions import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; await session.connect(ids=\"hjko98yghj\")\n&gt;&gt;&gt; await session.connect(ids=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def connect(self, ids: list[str] | str) -&gt; None:\n    \"\"\"Connect to a session[s] in a web browser.\n\n    Args:\n        ids (Union[List[str], str]): Session ID[s].\n\n    Examples:\n        &gt;&gt;&gt; from canfar.sessions import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; await session.connect(ids=\"hjko98yghj\")\n        &gt;&gt;&gt; await session.connect(ids=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    if isinstance(ids, str):\n        ids = [ids]\n    info = await self.info(ids)\n    log.debug(info)\n    for session in info:\n        status: str = session.get(\"status\", \"unknown\")\n        if status != \"Running\":\n            log.warning(\"Session %s is currently %s.\", session[\"id\"], status)\n            log.warning(\"Please wait for the session to be ready.\")\n            continue\n        connect_url = session.get(\"connectURL\")\n        if connect_url:\n            open_new_tab(connect_url)\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.create","title":"create  <code>async</code>","text":"<pre><code>create(\n    name,\n    image,\n    cores=None,\n    ram=None,\n    kind=\"headless\",\n    gpu=None,\n    cmd=None,\n    args=None,\n    env=None,\n    replicas=1,\n)\n</code></pre> <p>Launch a canfar session.</p> PARAMETER DESCRIPTION <code>name</code> <p>A unique name for the session.</p> <p> TYPE: <code>str</code> </p> <code>image</code> <p>Container image to use for the session.</p> <p> TYPE: <code>str</code> </p> <code>cores</code> <p>Number of cores. Defaults to None, i.e. flexible mode.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>ram</code> <p>Amount of RAM (GB). Defaults to None, i.e. flexible mode.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>kind</code> <p>Type of canfar session. Defaults to \"headless\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'headless'</code> </p> <code>gpu</code> <p>Number of GPUs. Defaults to None.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>cmd</code> <p>Command to run. Defaults to None.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>args</code> <p>Arguments to the command. Defaults to None.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>env</code> <p>Environment variables to inject. Defaults to None.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>replicas</code> <p>Number of sessions to launch. Defaults to 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Notes <ul> <li>If cores and ram are not specified, the session will be created with   flexible resource allocation of upto 8 cores and 32GB of RAM.</li> <li>The name of the session suffixed with the replica number. eg. test-42   when replicas &gt; 1.</li> <li>Each container will have the following environment variables injected:<ul> <li>REPLICA_ID - The replica number</li> <li>REPLICA_COUNT - The total number of replicas</li> </ul> </li> </ul> RETURNS DESCRIPTION <code>list[str]</code> <p>List[str]: A list of session IDs for the launched sessions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; session.create(\n        name=\"test\",\n        image='images.canfar.net/skaha/terminal:1.1.1',\n        cores=2,\n        ram=8,\n        gpu=1,\n        kind=\"headless\",\n        cmd=\"env\",\n        env={\"TEST\": \"test\"},\n        replicas=2,\n    )\n&gt;&gt;&gt; [\"hjko98yghj\", \"ikvp1jtp\"]\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def create(\n    self,\n    name: str,\n    image: str,\n    cores: int | None = None,\n    ram: int | None = None,\n    kind: Kind = \"headless\",\n    gpu: int | None = None,\n    cmd: str | None = None,\n    args: str | None = None,\n    env: dict[str, Any] | None = None,\n    replicas: int = 1,\n) -&gt; list[str]:\n    \"\"\"Launch a canfar session.\n\n    Args:\n        name (str): A unique name for the session.\n        image (str): Container image to use for the session.\n        cores (int, optional): Number of cores.\n            Defaults to None, i.e. flexible mode.\n        ram (int, optional): Amount of RAM (GB).\n            Defaults to None, i.e. flexible mode.\n        kind (str, optional): Type of canfar session. Defaults to \"headless\".\n        gpu (Optional[int], optional): Number of GPUs. Defaults to None.\n        cmd (Optional[str], optional): Command to run. Defaults to None.\n        args (Optional[str], optional): Arguments to the command. Defaults to None.\n        env (Optional[Dict[str, Any]], optional): Environment variables to inject.\n            Defaults to None.\n        replicas (int, optional): Number of sessions to launch. Defaults to 1.\n\n    Notes:\n        - If cores and ram are not specified, the session will be created with\n          flexible resource allocation of upto 8 cores and 32GB of RAM.\n        - The name of the session suffixed with the replica number. eg. test-42\n          when replicas &gt; 1.\n        - Each container will have the following environment variables injected:\n            * REPLICA_ID - The replica number\n            * REPLICA_COUNT - The total number of replicas\n\n    Returns:\n        List[str]: A list of session IDs for the launched sessions.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; session.create(\n                name=\"test\",\n                image='images.canfar.net/skaha/terminal:1.1.1',\n                cores=2,\n                ram=8,\n                gpu=1,\n                kind=\"headless\",\n                cmd=\"env\",\n                env={\"TEST\": \"test\"},\n                replicas=2,\n            )\n        &gt;&gt;&gt; [\"hjko98yghj\", \"ikvp1jtp\"]\n    \"\"\"\n    payloads: list[list[tuple[str, Any]]] = build.create_parameters(\n        name,\n        image,\n        cores,\n        ram,\n        kind,\n        gpu,\n        cmd,\n        args,\n        env,\n        replicas,\n    )\n    results: list[str] = []\n    tasks: list[Any] = []\n    semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n\n    async def bounded(parameters: list[tuple[str, Any]]) -&gt; Any:\n        async with semaphore:\n            log.debug(\"HTTP Request Parameters: %s\", parameters)\n            response = await self.asynclient.post(url=\"session\", params=parameters)\n            return response.text.rstrip(\"\\r\\n\")\n\n    tasks = [bounded(payload) for payload in payloads]\n    msg = f\"Creating {replicas} {kind} session[s].\"\n    log.debug(msg)\n    responses = await asyncio.gather(*tasks, return_exceptions=True)\n    for reply in responses:\n        if isinstance(reply, Exception):\n            log.error(reply)\n        elif isinstance(reply, str):\n            results.append(reply)\n    log.debug(results)\n    return results\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.destroy","title":"destroy  <code>async</code>","text":"<pre><code>destroy(ids)\n</code></pre> <p>Destroy session[s].</p> PARAMETER DESCRIPTION <code>ids</code> <p>Session ID[s].</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> RETURNS DESCRIPTION <code>dict[str, bool]</code> <p>Dict[str, bool]: A dictionary of session IDs</p> <code>dict[str, bool]</code> <p>and a bool indicating if the session was destroyed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; await session.destroy(id=\"hjko98yghj\")\n&gt;&gt;&gt; await session.destroy(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def destroy(self, ids: str | list[str]) -&gt; dict[str, bool]:\n    \"\"\"Destroy session[s].\n\n    Args:\n        ids (Union[str, List[str]]): Session ID[s].\n\n    Returns:\n        Dict[str, bool]: A dictionary of session IDs\n        and a bool indicating if the session was destroyed.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; await session.destroy(id=\"hjko98yghj\")\n        &gt;&gt;&gt; await session.destroy(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    if isinstance(ids, str):\n        ids = [ids]\n    results: dict[str, bool] = {}\n    semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n    tasks: list[Any] = []\n\n    async def bounded(value: str) -&gt; tuple[str, bool]:\n        async with semaphore:\n            try:\n                await self.asynclient.delete(url=f\"session/{value}\")\n            except HTTPError as err:\n                msg = f\"Failed to destroy session {value}: {err}\"\n                log.exception(msg)\n                return value, False\n            else:\n                return value, True\n\n    tasks = [bounded(value) for value in ids]\n    responses = await asyncio.gather(*tasks, return_exceptions=True)\n    for reply in responses:\n        if isinstance(reply, tuple):\n            results[reply[0]] = reply[1]\n    log.debug(results)\n    return results\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.destroy_with","title":"destroy_with  <code>async</code>","text":"<pre><code>destroy_with(prefix, kind='headless', status='Completed')\n</code></pre> <p>Destroy session[s] matching a prefix or regex pattern.</p> PARAMETER DESCRIPTION <code>prefix</code> <p>Prefix to match. Treated literally unless regex meta-characters are found.</p> <p> TYPE: <code>str</code> </p> <code>kind</code> <p>Type of session. Defaults to \"headless\".</p> <p> TYPE: <code>Kind</code> DEFAULT: <code>'headless'</code> </p> <code>status</code> <p>Status of the session. Defaults to \"Completed\".</p> <p> TYPE: <code>Status</code> DEFAULT: <code>'Completed'</code> </p> RETURNS DESCRIPTION <code>dict[str, bool]</code> <p>Dict[str, bool]: A dictionary of session IDs</p> <code>dict[str, bool]</code> <p>and a bool indicating if the session was destroyed.</p> Notes <ul> <li>If the value contains regex metacharacters (e.g., <code>.^$*+?{}[]()|</code>), it is     treated as a regex with :func:<code>re.search</code>.</li> <li>Otherwise it is treated as a literal prefix (anchored with <code>^</code>). This method is useful for destroying multiple sessions at once.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; await session.destroy_with(prefix=\"test\")  # literal prefix\n&gt;&gt;&gt; await session.destroy_with(prefix=\"desktop$\")  # regex\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def destroy_with(\n    self,\n    prefix: str,\n    kind: Kind = \"headless\",\n    status: Status = \"Completed\",\n) -&gt; dict[str, bool]:\n    \"\"\"Destroy session[s] matching a prefix or regex pattern.\n\n    Args:\n        prefix (str): Prefix to match.\n            Treated literally unless regex meta-characters are found.\n        kind (Kind): Type of session. Defaults to \"headless\".\n        status (Status): Status of the session. Defaults to \"Completed\".\n\n\n    Returns:\n        Dict[str, bool]: A dictionary of session IDs\n        and a bool indicating if the session was destroyed.\n\n    Notes:\n        - If the value contains regex metacharacters (e.g., `.^$*+?{}[]()|`), it is\n            treated as a regex with :func:`re.search`.\n        - Otherwise it is treated as a literal prefix (anchored with `^`).\n        This method is useful for destroying multiple sessions at once.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; await session.destroy_with(prefix=\"test\")  # literal prefix\n        &gt;&gt;&gt; await session.destroy_with(prefix=\"desktop$\")  # regex\n    \"\"\"\n    meta: bool = any(char in set(\".^$*+?{}[]()|\") for char in prefix)\n    try:\n        if meta:\n            log.info(\"Regex Pattern: %s\", prefix)\n            regex = re.compile(prefix)\n        else:\n            log.info(\"Literal Prefix: %s\", prefix)\n            regex = re.compile(rf\"^{re.escape(prefix)}\")\n    except re.error as err:\n        msg = f\"Invalid regex pattern '{prefix}': {err}\"\n        log.exception(msg)\n        raise ValueError(msg) from err\n\n    ids: list[str] = [\n        session[\"id\"]\n        for session in await self.fetch(kind=kind, status=status)\n        if regex.search(session[\"name\"])\n    ]\n    return await self.destroy(ids)\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.events","title":"events  <code>async</code>","text":"<pre><code>events(ids, verbose=False)\n</code></pre> <p>Get deployment events for a session[s].</p> PARAMETER DESCRIPTION <code>ids</code> <p>Session ID[s].</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>verbose</code> <p>Print events to stdout. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>list[dict[str, str]] | None</code> <p>Optional[List[Dict[str, str]]]: A list of events for the session[s].</p> Notes <p>When verbose is True, the events will be printed to stdout only.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; await session.events(id=\"hjko98yghj\")\n&gt;&gt;&gt; await session.events(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def events(\n    self,\n    ids: str | list[str],\n    verbose: bool = False,\n) -&gt; list[dict[str, str]] | None:\n    \"\"\"Get deployment events for a session[s].\n\n    Args:\n        ids (Union[str, List[str]]): Session ID[s].\n        verbose (bool, optional): Print events to stdout. Defaults to False.\n\n    Returns:\n        Optional[List[Dict[str, str]]]: A list of events for the session[s].\n\n    Notes:\n        When verbose is True, the events will be printed to stdout only.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; await session.events(id=\"hjko98yghj\")\n        &gt;&gt;&gt; await session.events(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    if isinstance(ids, str):\n        ids = [ids]\n    results: list[dict[str, str]] = []\n    parameters: dict[str, str] = {\"view\": \"events\"}\n    tasks: list[Any] = []\n    semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n\n    async def bounded(value: str) -&gt; dict[str, str]:\n        async with semaphore:\n            response = await self.asynclient.get(\n                url=f\"session/{value}\",\n                params=parameters,\n            )\n            return {value: response.text}\n\n    tasks = [bounded(value) for value in ids]\n    responses = await asyncio.gather(*tasks, return_exceptions=True)\n    for reply in responses:\n        if isinstance(reply, Exception):\n            log.error(reply)\n        elif isinstance(reply, dict):\n            results.append(dict(reply))\n\n    if verbose and results:\n        for result in results:\n            for key, value in result.items():\n                log.info(\"Session ID: %s\", key)\n                log.info(value)\n    log.debug(results)\n    return results if not verbose else None\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.fetch","title":"fetch  <code>async</code>","text":"<pre><code>fetch(kind=None, status=None, view=None)\n</code></pre> <p>List open sessions for the user.</p> PARAMETER DESCRIPTION <code>kind</code> <p>Session kind. Defaults to None.</p> <p> TYPE: <code>Kind | None</code> DEFAULT: <code>None</code> </p> <code>status</code> <p>Session status. Defaults to None.</p> <p> TYPE: <code>Status | None</code> DEFAULT: <code>None</code> </p> <code>view</code> <p>Session view level. Defaults to None.</p> <p> TYPE: <code>View | None</code> DEFAULT: <code>None</code> </p> Notes <p>By default, only the calling user's sessions are listed. If views is set to 'all', all user sessions are listed (with limited information).</p> RETURNS DESCRIPTION <code>list</code> <p>Sessions information.</p> <p> TYPE: <code>list[dict[str, str]]</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; await session.fetch(kind=\"notebook\")\n[{'id': 'vl91sfzz',\n'userid': 'brars',\n'runAsUID': '166169204',\n'runAsGID': '166169204',\n'supplementalGroups': [34241,\n34337,\n35124,\n36227,\n1902365706,\n1454823273,\n1025424273],\n'appid': '&lt;none&gt;',\n'image': 'image-server/repo/image:version',\n'type': 'notebook',\n'status': 'Running',\n'name': 'notebook1',\n'startTime': '2025-03-05T21:48:29Z',\n'expiryTime': '2025-03-09T21:48:29Z',\n'connectURL': 'https://canfar.net/session/notebook/some/url',\n'requestedRAM': '8G',\n'requestedCPUCores': '2',\n'requestedGPUCores': '0',\n'ramInUse': '&lt;none&gt;',\n'gpuRAMInUse': '&lt;none&gt;',\n'cpuCoresInUse': '&lt;none&gt;',\n'gpuUtilization': '&lt;none&gt;'}]\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def fetch(\n    self,\n    kind: Kind | None = None,\n    status: Status | None = None,\n    view: View | None = None,\n) -&gt; list[dict[str, str]]:\n    \"\"\"List open sessions for the user.\n\n    Args:\n        kind (Kind | None, optional): Session kind. Defaults to None.\n        status (Status | None, optional): Session status. Defaults to None.\n        view (View | None, optional): Session view level. Defaults to None.\n\n    Notes:\n        By default, only the calling user's sessions are listed. If views is\n        set to 'all', all user sessions are listed (with limited information).\n\n    Returns:\n        list: Sessions information.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; await session.fetch(kind=\"notebook\")\n        [{'id': 'vl91sfzz',\n        'userid': 'brars',\n        'runAsUID': '166169204',\n        'runAsGID': '166169204',\n        'supplementalGroups': [34241,\n        34337,\n        35124,\n        36227,\n        1902365706,\n        1454823273,\n        1025424273],\n        'appid': '&lt;none&gt;',\n        'image': 'image-server/repo/image:version',\n        'type': 'notebook',\n        'status': 'Running',\n        'name': 'notebook1',\n        'startTime': '2025-03-05T21:48:29Z',\n        'expiryTime': '2025-03-09T21:48:29Z',\n        'connectURL': 'https://canfar.net/session/notebook/some/url',\n        'requestedRAM': '8G',\n        'requestedCPUCores': '2',\n        'requestedGPUCores': '0',\n        'ramInUse': '&lt;none&gt;',\n        'gpuRAMInUse': '&lt;none&gt;',\n        'cpuCoresInUse': '&lt;none&gt;',\n        'gpuUtilization': '&lt;none&gt;'}]\n    \"\"\"\n    parameters: dict[str, Any] = build.fetch_parameters(kind, status, view)\n    response: Response = await self.asynclient.get(url=\"session\", params=parameters)\n    data: list[dict[str, str]] = response.json()\n    log.debug(data)\n    return data\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.info","title":"info  <code>async</code>","text":"<pre><code>info(ids)\n</code></pre> <p>Get information about session[s].</p> PARAMETER DESCRIPTION <code>ids</code> <p>Session ID[s].</p> <p> TYPE: <code>Union[List[str], str]</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Any]]</code> <p>Dict[str, Any]: Session information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; await session.info(session_id=\"hjko98yghj\")\n&gt;&gt;&gt; await session.info(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def info(self, ids: list[str] | str) -&gt; list[dict[str, Any]]:\n    \"\"\"Get information about session[s].\n\n    Args:\n        ids (Union[List[str], str]): Session ID[s].\n\n    Returns:\n        Dict[str, Any]: Session information.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; await session.info(session_id=\"hjko98yghj\")\n        &gt;&gt;&gt; await session.info(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    # Convert id to list if it is a string\n    if isinstance(ids, str):\n        ids = [ids]\n    results: list[dict[str, Any]] = []\n    tasks: list[Any] = []\n    semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n\n    async def bounded(value: str) -&gt; dict[str, Any]:\n        async with semaphore:\n            response = await self.asynclient.get(url=f\"session/{value}\")\n            data: dict[str, Any] = response.json()\n            return data\n\n    tasks = [bounded(value) for value in ids]\n    responses = await asyncio.gather(*tasks, return_exceptions=True)\n    for reply in responses:\n        if isinstance(reply, Exception):\n            log.error(reply)\n        elif isinstance(reply, dict):\n            results.append(reply)\n    log.debug(results)\n    return results\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.logs","title":"logs  <code>async</code>","text":"<pre><code>logs(ids, verbose=False)\n</code></pre> <p>Get logs from a session[s].</p> PARAMETER DESCRIPTION <code>ids</code> <p>Session ID[s].</p> <p> TYPE: <code>Union[List[str], str]</code> </p> <code>verbose</code> <p>Print logs to stdout. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>dict[str, str] | None</code> <p>Dict[str, str]: Logs in text/plain format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; await session.logs(id=\"hjko98yghj\")\n&gt;&gt;&gt; await session.logs(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def logs(\n    self,\n    ids: list[str] | str,\n    verbose: bool = False,\n) -&gt; dict[str, str] | None:\n    \"\"\"Get logs from a session[s].\n\n    Args:\n        ids (Union[List[str], str]): Session ID[s].\n        verbose (bool, optional): Print logs to stdout. Defaults to False.\n\n    Returns:\n        Dict[str, str]: Logs in text/plain format.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; await session.logs(id=\"hjko98yghj\")\n        &gt;&gt;&gt; await session.logs(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    if isinstance(ids, str):\n        ids = [ids]\n    parameters: dict[str, str] = {\"view\": \"logs\"}\n    results: dict[str, str] = {}\n\n    semaphore: asyncio.Semaphore = asyncio.Semaphore(self.concurrency)\n    tasks: list[Any] = []\n\n    async def bounded(value: str) -&gt; tuple[str, str]:\n        async with semaphore:\n            response = await self.asynclient.get(\n                url=f\"session/{value}\",\n                params=parameters,\n            )\n            return value, response.text\n\n    tasks = [bounded(value) for value in ids]\n    responses = await asyncio.gather(*tasks, return_exceptions=True)\n    for reply in responses:\n        if isinstance(reply, Exception):\n            log.error(reply)\n        elif isinstance(reply, tuple):\n            results[reply[0]] = reply[1]\n\n    # Print logs to stdout if verbose is set to True\n    if verbose:\n        for key, value in results.items():\n            log.info(\"Session ID: %s\\n\", key)\n            log.info(value)\n        return None\n    return results\n</code></pre>"},{"location":"client/async_session/#canfar.sessions.AsyncSession.stats","title":"stats  <code>async</code>","text":"<pre><code>stats()\n</code></pre> <p>Get statistics for the canfar cluster.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: Cluster statistics.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import AsyncSession\n&gt;&gt;&gt; session = AsyncSession()\n&gt;&gt;&gt; await session.stats()\n{'cores': {'requestedCPUCores': 377,\n 'coresAvailable': 960,\n 'maxCores': {'cores': 32, 'withRam': '147Gi'}},\n 'ram': {'maxRAM': {'ram': '226Gi', 'withCores': 32}}}\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>async def stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get statistics for the canfar cluster.\n\n    Returns:\n        Dict[str, Any]: Cluster statistics.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import AsyncSession\n        &gt;&gt;&gt; session = AsyncSession()\n        &gt;&gt;&gt; await session.stats()\n        {'cores': {'requestedCPUCores': 377,\n         'coresAvailable': 960,\n         'maxCores': {'cores': 32, 'withRam': '147Gi'}},\n         'ram': {'maxRAM': {'ram': '226Gi', 'withCores': 32}}}\n    \"\"\"\n    parameters = {\"view\": \"stats\"}\n    response: Response = await self.asynclient.get(\"session\", params=parameters)\n    data: dict[str, Any] = response.json()\n    log.debug(data)\n    return data\n</code></pre>"},{"location":"client/bug-reports/","title":"Bug Reports","text":"<p>Thank you for taking the time to report a bug! Your feedback helps us improve CANFAR for everyone. This guide will help you create effective bug reports so we can quickly identify and resolve issues.</p>"},{"location":"client/bug-reports/#before-reporting-a-bug","title":"Before Reporting a Bug","text":"<p>Before creating a new bug report, please:</p> <ul> <li>Search existing issues: Check if the bug has already been reported in our GitHub Issues.</li> <li>Update to the latest version: Ensure you are using the latest version of the CANFAR Client.</li> <li>Check the documentation: Review our documentation to confirm the expected behaviour.</li> </ul>"},{"location":"client/bug-reports/#how-to-report-a-bug","title":"How to Report a Bug","text":""},{"location":"client/bug-reports/#gather-system-information","title":"Gather System Information","text":"<p>Before reporting a bug, collect detailed system information using the CANFAR CLI:</p> <pre><code>canfar version --debug\n</code></pre> <p>This command provides information about your environment:</p> <ul> <li>Client Information: <code>canfar</code> version, git commit info, installation method</li> <li>Python Environment: Python version, executable path, implementation</li> <li>System Details: Operating system, version, architecture, platform</li> <li>Dependencies: Versions of key packages that might affect functionality</li> </ul>"},{"location":"client/bug-reports/#create-a-detailed-bug-report","title":"Create a Detailed Bug Report","text":"<p>When creating your bug report, please provide:</p> <ul> <li>Bug Description<ul> <li>What you were trying to do</li> <li>What actually happened</li> <li>What you expected to happen</li> </ul> </li> <li>Steps to Reproduce: Exact steps to reproduce the behaviour, including relevant commands and options.</li> <li>Expected Behaviour: What you expected to happen instead.</li> <li>System Information: Complete output from <code>canfar version --debug</code>.</li> <li>Error Messages and Logs: Any error messages, stack traces, or relevant log output. Use code blocks for formatting.</li> <li>Screenshots (if applicable): Screenshots that help explain the problem.</li> <li>Additional Context: Any other details, such as:<ul> <li>When the issue started occurring</li> <li>Whether it happens consistently or intermittently</li> <li>Any workarounds you have found</li> <li>Related configuration or environment details</li> </ul> </li> </ul>"},{"location":"client/bug-reports/#what-makes-a-good-bug-report","title":"What Makes a Good Bug Report","text":""},{"location":"client/bug-reports/#good-bug-reports-include","title":"\u2705 Good Bug Reports Include","text":"<ul> <li>Clear, descriptive title</li> <li>Complete system information from <code>canfar version --debug</code></li> <li>Detailed steps to reproduce</li> <li>Expected vs. actual behaviour</li> <li>Error messages and stack traces</li> <li>Relevant context and environment details</li> </ul>"},{"location":"client/bug-reports/#avoid-these-common-issues","title":"\u274c Avoid These Common Issues","text":"<ul> <li>Vague descriptions like \"it doesn't work\"</li> <li>Missing system information</li> <li>Incomplete reproduction steps</li> <li>Screenshots of text instead of copy-pasted text</li> <li>Mixing multiple unrelated issues in one report</li> </ul>"},{"location":"client/bug-reports/#security-issues","title":"Security Issues","text":"<p>If you discover a security vulnerability, please do not create a public issue. Instead, refer to our Security Policy for instructions on how to report security issues responsibly.</p>"},{"location":"client/bug-reports/#after-reporting","title":"After Reporting","text":"<p>After you submit a bug report:</p> <ul> <li>Monitor the issue: Watch for responses from maintainers</li> <li>Provide additional information: Be ready to answer follow-up questions</li> <li>Test fixes: Help test proposed solutions when available</li> <li>Update the issue: Let us know if the problem is resolved</li> </ul> <p>Thank you for helping make Canfar better! \ud83d\ude80</p>"},{"location":"client/client/","title":"HTTPClient","text":"<p>The <code>canfar.client</code> module provides a comprehensive HTTP client for interacting with CANFAR Science Platform services. Built on the powerful <code>httpx</code> library, it offers both synchronous and asynchronous interfaces with advanced authentication capabilities.</p>"},{"location":"client/client/#features","title":"Features","text":"<p>Key Capabilities</p> <ul> <li>Multiple Authentication Methods: X.509 certificates, OIDC tokens, and bearer tokens</li> <li>Automatic SSL Configuration: Seamless certificate-based authentication</li> <li>Async/Sync Support: Both synchronous and asynchronous HTTP clients</li> <li>Connection Pooling: Optimized for concurrent requests</li> <li>Debug Logging: Comprehensive logging for troubleshooting</li> <li>Context Managers: Proper resource management</li> </ul> <p>This is a low-level client that is used by all other API clients in CANFAR. It is not intended to be used directly by users, but rather as a building block for other clients and contributors.</p>"},{"location":"client/client/#authentication-modes","title":"Authentication Modes","text":"<p>The client supports multiple authentication modes that can be configured through the authentication system:</p>"},{"location":"client/client/#debug-logging","title":"Debug Logging","text":"<pre><code>import logging\nfrom canfar.client import HTTPClient\n\n# Enable debug logging to see client creation details\nclient = HTTPClient(loglevel=logging.DEBUG)\n\n# This will log:\n# - Authentication mode selection\n# - SSL context creation\n# - Header generation\n# - Client configuration\n</code></pre>"},{"location":"client/client/#configuration","title":"Configuration","text":"<p>The client inherits from the <code>Configuration</code> class and supports all configuration options:</p> <pre><code>from canfar.client import HTTPClient\n\nclient = HTTPClient(\n    timeout=60,           # Request timeout in seconds\n    concurrency=64,       # Max concurrent connections\n    loglevel=20,         # Logging level (INFO)\n)\n</code></pre>"},{"location":"client/client/#authentication-expiry","title":"Authentication Expiry","text":"<p>The client provides an <code>expiry</code> property that returns the expiry time for the current authentication method:</p> <pre><code>import time\n\nclient = HTTPClient()\n\nif client.expiry:\n    time_left = client.expiry - time.time()\n    print(f\"Authentication expires in {time_left:.0f} seconds\")\nelse:\n    print(\"No expiry tracking (user-provided credentials)\")\n</code></pre> <p>Expiry Tracking</p> <p>The <code>expiry</code> property returns <code>None</code> for user-provided certificates or tokens since the client cannot track their expiry automatically.</p>"},{"location":"client/client/#error-handling","title":"Error Handling","text":"<p>The client includes built-in error handling for HTTP responses:</p> <pre><code>from httpx import HTTPStatusError\n\ntry:\n    response = client.client.get(\"/invalid-endpoint\")\n    response.raise_for_status()\nexcept HTTPStatusError as e:\n    print(f\"HTTP error: {e.response.status_code}\")\n</code></pre>"},{"location":"client/client/#api-reference","title":"API Reference","text":""},{"location":"client/client/#canfar.client.HTTPClient","title":"canfar.client.HTTPClient","text":"<p>               Bases: <code>BaseSettings</code></p> <p>HTTP Client for interacting with CANFAR Science Platform services (V2).</p> <p>This client uses a composition-based approach and inherits from Pydantic's BaseSettings to allow for flexible configuration via arguments, environment variables, or a configuration file.</p> <p>The client prioritizes credentials in the following order:</p> <ol> <li>Runtime Arguments/Environment Variables: A <code>token</code> or <code>certificate</code>     provided at instantiation (e.g., <code>CANFAR_TOKEN=\"...\"</code>).</li> <li>Active Configuration Context: The context specified by <code>active_context</code>     in the loaded configuration file.</li> </ol> RAISES DESCRIPTION <code>ValueError</code> <p>If configuration is invalid.</p>"},{"location":"client/client/#canfar.client.HTTPClient.client","title":"client  <code>property</code>","text":"<pre><code>client: Client\n</code></pre> <p>Get the synchronous HTTPx Client.</p> RETURNS DESCRIPTION <code>Client</code> <p>The synchronous HTTPx client.</p> <p> TYPE: <code>Client</code> </p>"},{"location":"client/client/#canfar.client.HTTPClient.asynclient","title":"asynclient  <code>property</code>","text":"<pre><code>asynclient: AsyncClient\n</code></pre> <p>Get the asynchronous HTTPx Async Client.</p>"},{"location":"client/context/","title":"Context API","text":"<p>Overview</p> <p>The Context API allows the user to get information about the resources available to be requested for a session on the CANFAR Science Platform. This information can be used to configure the session to request the appropriate resources for your session.</p> Get context information<pre><code>from canfar.context import Context\n\ncontext = Context()\ncontext.resources()\n</code></pre> <pre><code>{\n    \"cores\": {\n        \"default\": 1,\n        \"defaultRequest\": 1,\n        \"defaultLimit\": 16,\n        \"defaultHeadless\": 1,\n        \"options\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16],\n    },\n    \"memoryGB\": {\n        \"default\": 2,\n        \"defaultRequest\": 4,\n        \"defaultLimit\": 192,\n        \"defaultHeadless\": 4,\n        \"options\": [1, 2, ..., 192],\n    },\n    \"gpus\": {\n        \"options\": [1, ..., 8],\n    },\n}\n</code></pre> <p>               Bases: <code>HTTPClient</code></p> <p>CANFAR Context.</p> <p>This class is a subclass of the <code>HTTPClient</code> class and inherits its attributes and methods.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.context import Context\n&gt;&gt;&gt; context = Context()\n&gt;&gt;&gt; context.resources()\n</code></pre> Source code in <code>canfar/context.py</code> <pre><code>class Context(HTTPClient):\n    \"\"\"CANFAR Context.\n\n    This class is a subclass of the `HTTPClient` class and inherits its\n    attributes and methods.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.context import Context\n        &gt;&gt;&gt; context = Context()\n        &gt;&gt;&gt; context.resources()\n    \"\"\"\n\n    def resources(self) -&gt; dict[str, Any]:\n        \"\"\"Get available resources from the canfar server.\n\n        Returns:\n            A dictionary of available resources.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.context import Context\n            &gt;&gt;&gt; context = Context()\n            &gt;&gt;&gt; context.resources()\n            {'cores': {\n              'default': 1,\n              'defaultRequest': 1,\n              'defaultLimit': 16,\n              'defaultHeadless': 1,\n              'options': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n              },\n             'memoryGB': {\n              'default': 2,\n              'defaultRequest': 4,\n              'defaultLimit': 192,\n              'defaultHeadless': 4,\n              'options': [1,2,4...192]\n             },\n            'gpus': {\n             'options': [1,2, ... 28]\n             }\n            }\n        \"\"\"\n        response: Response = self.client.get(url=\"context\")\n        return dict(response.json())\n</code></pre>"},{"location":"client/context/#canfar.context.Context.resources","title":"resources","text":"<pre><code>resources()\n</code></pre> <p>Get available resources from the canfar server.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>A dictionary of available resources.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.context import Context\n&gt;&gt;&gt; context = Context()\n&gt;&gt;&gt; context.resources()\n{'cores': {\n  'default': 1,\n  'defaultRequest': 1,\n  'defaultLimit': 16,\n  'defaultHeadless': 1,\n  'options': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n  },\n 'memoryGB': {\n  'default': 2,\n  'defaultRequest': 4,\n  'defaultLimit': 192,\n  'defaultHeadless': 4,\n  'options': [1,2,4...192]\n },\n'gpus': {\n 'options': [1,2, ... 28]\n }\n}\n</code></pre> Source code in <code>canfar/context.py</code> <pre><code>def resources(self) -&gt; dict[str, Any]:\n    \"\"\"Get available resources from the canfar server.\n\n    Returns:\n        A dictionary of available resources.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.context import Context\n        &gt;&gt;&gt; context = Context()\n        &gt;&gt;&gt; context.resources()\n        {'cores': {\n          'default': 1,\n          'defaultRequest': 1,\n          'defaultLimit': 16,\n          'defaultHeadless': 1,\n          'options': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n          },\n         'memoryGB': {\n          'default': 2,\n          'defaultRequest': 4,\n          'defaultLimit': 192,\n          'defaultHeadless': 4,\n          'options': [1,2,4...192]\n         },\n        'gpus': {\n         'options': [1,2, ... 28]\n         }\n        }\n    \"\"\"\n    response: Response = self.client.get(url=\"context\")\n    return dict(response.json())\n</code></pre>"},{"location":"client/examples/","title":"Python Client Examples","text":"<p>These examples use the asynchronous API for best performance and scalability.</p> <p>Assumption</p> Authenticated via CLI<pre><code>canfar auth login\n</code></pre>"},{"location":"client/examples/#create-sessions","title":"Create Sessions","text":""},{"location":"client/examples/#notebook","title":"Notebook","text":"Flexible Mode (Default)Fixed Mode<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nids = session.create(\n    name=\"my-notebook\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    kind=\"notebook\",\n)\nprint(ids)  # [\"d1tsqexh\"]\nsession.connect(ids)\n</code></pre> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nids = session.create(\n    name=\"my-notebook\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    kind=\"notebook\",\n    cores=2,\n    ram=4,\n)\nprint(ids)  # [\"d1tsqexh\"]\nsession.connect(ids)\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nsession = AsyncSession()\nids = await session.create(\n    name=\"my-notebook\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    kind=\"notebook\",\n)\nprint(ids)  # [\"d1tsqexh\"]\nawait session.connect(ids)\n</code></pre>"},{"location":"client/examples/#headless","title":"Headless","text":"<ul> <li>Headless sessions are are containers that execute a command and exit when complete without user interaction.</li> <li>They are useful for batch processing and distributed computing.</li> </ul> Replicated Headless Sessions<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nids = session.create(\n    name=\"my-headless\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    kind=\"headless\",\n    cmd=\"echo\",\n    args=[\"Hello, World!\"],\n)\nprint(ids)  # [\"d1tsqexh\"]\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nsession = AsyncSession()\nids = await session.create(\n    name=\"my-headless\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    kind=\"headless\",\n    cmd=\"echo\",\n    args=[\"Hello, World!\"],\n)\nprint(ids)  # [\"d1tsqexh\"]\n</code></pre> <p>Replica Environment Variables</p> <p>All containers receive the following environment variables: - <code>REPLICA_COUNT</code> \u2014 common total number of replicas - <code>REPLICA_ID</code> \u2014 1-based index of the replica (1..N)</p> <p>Use these to partition work deterministically. See Helpers API Reference for <code>chunk</code> and <code>stripe</code>.</p> <p>Private Container Registry Access</p> <p>Use a private Harbor image by providing registry credentials via configuration. <pre><code>import asyncio\nfrom canfar.sessions import AsyncSession\nfrom canfar.models.registry import ContainerRegistry\nfrom canfar.models.config import Configuration\n\nasync def main():\n    cfg = Configuration(registry=ContainerRegistry(username=\"username\", secret=\"CLI_SECRET\"))\n    session = AsyncSession(config=cfg)\n    ids = await session.create(\n        name=\"private-job\",\n        image=\"images.canfar.net/your/private-image:latest\",\n        kind=\"headless\",\n        cmd=\"python\",\n        args=[\"/app/run.py\"],\n    )\n    print(ids)\n\nasyncio.run(main())\n</code></pre></p>"},{"location":"client/examples/#resource-allocation-modes","title":"Resource Allocation Modes","text":"<p>CANFAR supports two resource allocation modes for your sessions. See the resource allocation guide for more information.</p>"},{"location":"client/examples/#examples","title":"Examples","text":"Flexible Mode (Default)Fixed Mode <pre><code>from canfar.sessions import Session\n\nsession = Session()\n# No cores/ram specification - uses flexible allocation\nids = session.create(\n    name=\"flexible-notebook\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    kind=\"notebook\"\n)\n</code></pre> <pre><code>from canfar.sessions import Session\n\nsession = Session()\n# Specify exact resources for guaranteed allocation\nids = session.create(\n    name=\"fixed-notebook\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    kind=\"notebook\",\n    cores=4,\n    ram=8\n)\n</code></pre>"},{"location":"client/examples/#discover-and-filter-sessions","title":"Discover and Filter Sessions","text":"Fetch All Sessions<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nall_sessions = session.fetch()\nprint(len(all_sessions))\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nwith AsyncSession() as session:\n    all_sessions = await session.fetch()\n    print(len(all_sessions))\n</code></pre> Fetch Running Notebooks<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nrunning = session.fetch(kind=\"notebook\", status=\"Running\")\nprint(running)\nsession.connect(running)\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    running = await session.fetch(kind=\"notebook\", status=\"Running\")\n    print(running)\n    await session.connect(running)\n</code></pre> Fetch Completed Headless Sessions<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\ncompleted = session.fetch(kind=\"headless\", status=\"Succeeded\")\nprint(completed)\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    completed = await session.fetch(kind=\"headless\", status=\"Succeeded\")\n    print(completed)\n</code></pre> <p>Kinds &amp; Status</p> <p>You can use any combination of the following kinds and status to filter sessions:</p> <ul> <li>Kinds: <code>desktop</code>, <code>notebook</code>, <code>carta</code>, <code>headless</code>, <code>firefly</code>, <code>desktop-app</code>, <code>contributed</code></li> <li>Statuses: <code>Pending</code>, <code>Running</code>, <code>Terminating</code>, <code>Succeeded</code>, <code>Error</code>, <code>Failed</code></li> </ul>"},{"location":"client/examples/#inspect-sessions","title":"Inspect Sessions","text":"<p>Detailed information about the session, including resource usage, user IDs, and more.</p> Detailed Session Information<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\ninfo = session.info(ids)\nprint(info)\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    info = await session.info(ids)\n    print(info)\n</code></pre>"},{"location":"client/examples/#events","title":"Events","text":"<p>Events describe the steps taken by the Science Platform to launch your session</p> Session Events<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nevents = session.events(ids, verbose=True)\nprint(events)\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    events = await session.events(ids, verbose=True)\n    print(events)\n</code></pre>"},{"location":"client/examples/#logs","title":"Logs","text":"<p>Logs contain the output from your session's containers. </p> <p>Log Retention</p> <p>Logs are retained until your session is deleted. A completed session, i.e., <code>Succeeded</code>, <code>Failed</code>, or <code>Error</code> is kept for 24 hours before being deleted.</p> Session Logs<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nlogs = session.logs(ids, verbose=True)\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    logs = await session.logs(ids, verbose=True)\n</code></pre>"},{"location":"client/examples/#cleanup-sessions","title":"Cleanup Sessions","text":"<p>Permanent Action</p> <p>Deleted sessions cannot be recovered.</p> Destroy Session(s)<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nresult = session.destroy(ids)\nprint(result)  # {\"id\": True, ...}\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    result = await session.destroy(ids)\n    print(result)  # {\"id\": True, ...}\n</code></pre> <p></p> Bulk Destroy<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nresult = session.destroy_with(prefix=\"test-\", kind=\"headless\", status=\"Succeeded\")\nprint(result)  # {\"id\": True, ...}\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    result = await session.destroy_with(prefix=\"test-\", kind=\"headless\", status=\"Succeeded\")\n    print(result)  # {\"id\": True, ...}\n</code></pre>"},{"location":"client/get-started/","title":"Installation &amp; Set-up","text":"<p>This guide covers everything you need to install and start using CANFAR Science Platform servers worldwide.</p> <p>New to CANFAR?</p> <p>If you want to jump right in with a hands-on tutorial, check out our 5-Minute Quick Start guide first!</p>"},{"location":"client/get-started/#prerequisites","title":"Prerequisites","text":"<p>Before you can use CANFAR, you need:</p> <ul> <li>Python 3.10+ installed on your system</li> <li>A Science Platform account - For CANFAR, request an account with CADC</li> </ul>"},{"location":"client/get-started/#installation","title":"Installation","text":"<p>Install <code>canfar</code> using <code>pip</code>:</p> <pre><code>pip install canfar --upgrade\n</code></pre> <p>Virtual Environments</p> <p>We recommend using a virtual environment to avoid conflicts with other packages: <pre><code>python -m venv canfar-env\nsource canfar-env/bin/activate  # On Windows: canfar-env\\Scripts\\activate\npip install canfar\n</code></pre></p>"},{"location":"client/get-started/#authentication-set-up","title":"Authentication Set-up","text":"<p>CANFAR uses an authentication context system to manage connections to multiple Science Platform servers. The easiest way to get started is with the CLI log in command.</p>"},{"location":"client/get-started/#quick-authentication","title":"Quick Authentication","text":"<p>To authenticate with a Science Platform server:</p> <pre><code>canfar auth login\n</code></pre> <p>This command will:</p> <ol> <li>Discover available servers worldwide</li> <li>Guide you through server selection</li> <li>Handle the authentication process (X.509 or OIDC)</li> <li>Save your credentials for future use</li> </ol> <p>Example Login Flow</p> <pre><code>$ canfar auth login\nStarting Science Platform Login\nDiscovery completed in 2.1s (5/18 active)\n\nSelect a server:\n\u00bb \ud83d\udfe2 CANFAR  CADC\n  \ud83d\udfe2 Canada  SRCnet\n  \ud83d\udfe2 UK-CAM  SRCnet\n\nX509 Certificate Authentication\nUsername: your-username\nPassword: ***********\n\u2713 Login completed successfully!\n</code></pre>"},{"location":"client/get-started/#using-canfar-programmatically","title":"Using CANFAR Programmatically","text":"<p>Once authenticated via CLI, you can use <code>canfar</code> in your Python code:</p> <pre><code>from canfar.session import Session\nfrom canfar.images import Images\n\n# Uses your active authentication context\nsession = Session()\nimages = Images()\n\n# List available images\ncontainer_images = images.fetch()\nprint(f\"Found {len(container_images)} container images\")\n\n# Create a notebook session\nsession_info = session.create(\n    kind=\"notebook\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    name=\"my-analysis\",\n    cores=2,\n    ram=4\n)\nprint(f\"Created session: {session_info.id}\")\n</code></pre>"},{"location":"client/get-started/#private-container-images","title":"Private Container Images","text":"<p>To access private container images from registries like CANFAR Harbor, provide registry credentials:</p> <pre><code>from canfar.models import ContainerRegistry\nfrom canfar.session import Session\n\n# Configure registry access\nregistry = ContainerRegistry(\n    username=\"your-username\",\n    password=\"**************\"\n)\n\n# Use with session\nsession = Session(registry=registry)\n\n# Now you can use private images\nsession_info = session.create(\n    kind=\"notebook\",\n    image=\"images.canfar.net/private/my-image:latest\",\n    cores=1,\n    ram=2\n)\n</code></pre> <p>Registry Credentials</p> <p>The registry credentials are base64 encoded and passed to the server via the <code>X-Skaha-Registry-Auth</code> header.</p>"},{"location":"client/get-started/#next-steps","title":"Next Steps","text":"<p>Now that you have <code>canfar</code> installed and configured:</p> <ul> <li> Try our 5-Minute Quick Start for a hands-on introduction to creating and managing sessions.</li> <li> Learn about Authentication Contexts for managing multiple servers and advanced authentication scenarios.</li> <li> Explore Basic Examples and Advanced Examples for common use cases.</li> <li> Check out the Python API Reference for detailed documentation of all available methods.</li> <li> Refer to the FAQ for answers to common questions.</li> </ul>"},{"location":"client/helpers/","title":"Helpers API","text":"<p>Overview</p> <p>The Helpers API provides utility functions to partition work across replicas of a CANFAR session. Containers receive <code>REPLICA_ID</code> and <code>REPLICA_COUNT</code> environment variables, and these helpers make using them simple and correct.</p>"},{"location":"client/helpers/#practical-examples","title":"Practical Examples","text":""},{"location":"client/helpers/#stripe-take-every-nth-item-with-an-offset","title":"Stripe: take every Nth item with an offset","text":"<pre><code>from canfar.helpers import distributed\n\n# Assume REPLICA_ID=2 and REPLICA_COUNT=4\n# Replica 2 (1-based) will see indices 1, 5, 9, ...\nitems = list(range(12))\nshard = list(distributed.stripe(items, replica=2, total=4))\nprint(shard)  # [1, 5, 9]\n</code></pre>"},{"location":"client/helpers/#chunk-contiguous-chunks-of-roughly-equal-size","title":"Chunk: contiguous chunks of roughly equal size","text":"<pre><code>from canfar.helpers import distributed\n\n# Assume 10 items, 4 replicas\nitems = list(range(10))\n# Replica 1 gets [0,1], 2-&gt;[2,3], 3-&gt;[4,5], 4-&gt;[6,7,8,9] (last takes remainder)\nprint(list(distributed.chunk(items, replica=1, total=4)))\nprint(list(distributed.chunk(items, replica=4, total=4)))\n</code></pre> <p>Sparse distribution</p> <p>When items &lt; replicas, <code>chunk</code> assigns exactly one item to each of the first <code>len(items)</code> replicas, and later replicas get nothing. This avoids duplication.</p>"},{"location":"client/helpers/#using-container-provided-environment-variables","title":"Using container-provided environment variables","text":"<pre><code># Inside CANFAR container replicas, you can omit replica/total and read from env\nfrom canfar.helpers import distributed\nwork = list(range(1000))\nfor item in distributed.chunk(work):\n    process(item)\n</code></pre>"},{"location":"client/helpers/#validation-and-errors","title":"Validation and errors","text":"<ul> <li><code>replica</code> must be &gt;= 1 and &lt;= <code>total</code></li> <li><code>total</code> must be &gt; 0</li> </ul>"},{"location":"client/helpers/#api-reference","title":"API Reference","text":""},{"location":"client/helpers/#canfar.helpers.distributed","title":"canfar.helpers.distributed","text":"<p>Helper functions for distributed computing.</p>"},{"location":"client/helpers/#canfar.helpers.distributed.stripe","title":"stripe","text":"<pre><code>stripe(\n    iterable: Iterable[T],\n    replica: int = int(os.environ.get(\"REPLICA_ID\", \"1\")),\n    total: int = int(os.environ.get(\"REPLICA_COUNT\", \"1\")),\n) -&gt; Iterator[T]\n</code></pre> <p>Returns every <code>total</code>-th item from the iterable with a <code>replica</code>-th offset.</p> PARAMETER DESCRIPTION <code>iterable</code> <p>The iterable to partition.</p> <p> TYPE: <code>Iterable[T]</code> </p> <code>replica</code> <p>The replica number. Defaults to int(os.environ.get(\"REPLICA_ID\", 1)).</p> <p> TYPE: <code>int</code> DEFAULT: <code>int(get('REPLICA_ID', '1'))</code> </p> <code>total</code> <p>The total number of replicas. Defaults to int(os.environ.get(\"REPLICA_COUNT\", 1)).</p> <p> TYPE: <code>int</code> DEFAULT: <code>int(get('REPLICA_COUNT', '1'))</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.helpers import distributed\n&gt;&gt;&gt; dataset = range(100)\n&gt;&gt;&gt; for data in distributed.partition(dataset, 1, 10):\n        print(data)\n0, 10, 20, 30, 40, 50, 60, 70, 80, 90\n</code></pre> YIELDS DESCRIPTION <code>T</code> <p>Iterator[T]: The <code>replica</code>-th partition of the iterable.</p>"},{"location":"client/helpers/#canfar.helpers.distributed.chunk","title":"chunk","text":"<pre><code>chunk(\n    iterable: Iterable[T],\n    replica: int = int(os.environ.get(\"REPLICA_ID\", \"1\")),\n    total: int = int(os.environ.get(\"REPLICA_COUNT\", \"1\")),\n) -&gt; Iterator[T]\n</code></pre> <p>Returns the <code>replica</code>-th chunk of the iterable split into <code>total</code> chunks.</p> <p>This function distributes items from an iterable across multiple replicas canfar provided container environment variables.</p> <p>Distribution Behavior:</p> <ul> <li>Standard Distribution (items &gt;= replicas): Items are divided into roughly   equal chunks, with the last replica receiving any remainder items.</li> <li>Sparse Distribution (items &lt; replicas): Each of the first N replicas gets   exactly one item (where N = number of items), remaining replicas get empty   results.</li> </ul> PARAMETER DESCRIPTION <code>iterable</code> <p>The iterable to distribute across replicas.</p> <p> TYPE: <code>Iterable[T]</code> </p> <code>replica</code> <p>The replica number using 1-based indexing. Must be &gt;= 1 and &lt;= total. Defaults to REPLICA_ID environment variable.</p> <p> TYPE: <code>int</code> DEFAULT: <code>int(get('REPLICA_ID', '1'))</code> </p> <code>total</code> <p>The total number of replicas. Must be &gt; 0. Defaults to REPLICA_COUNT environment variable.</p> <p> TYPE: <code>int</code> DEFAULT: <code>int(get('REPLICA_COUNT', '1'))</code> </p> RETURNS DESCRIPTION <code>Iterator[T]</code> <p>Iterator[T]: An iterator yielding items assigned to this replica.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If replica &lt; 1 (1-based indexing expected).</p> <code>ValueError</code> <p>If replica &gt; total (replica cannot exceed total replicas).</p> <code>ValueError</code> <p>If total &lt;= 0 (must have at least one replica).</p> Note <p>This function is designed for use in canfar containerized environments where REPLICA_ID and REPLICA_COUNT environment variables are automatically set. The 1-based indexing matches the container environment expectations.</p> <p>For optimal performance with large datasets, consider using this function with iterators rather than converting large datasets to lists beforehand.</p> <p>When items &lt; replicas, the sparse distribution ensures no replica receives an unfair share - each item goes to exactly one replica, and excess replicas receive empty results rather than duplicating data.</p>"},{"location":"client/home/","title":"CANFAR Clients","text":"<p>A powerful Python API and CLI for the CANFAR Science Platform.</p>  Client Download <p> API</p> <code>sync</code><code>async</code><code>async context</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nids = session.create(\n    name=\"test\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    kind=\"headless\",\n    cmd=\"env\",\n    env={\"KEY\": \"VALUE\"},\n    replicas=3,\n)\nprint(ids)\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nsession = AsyncSession()\nids = await session.create(\n    name=\"test\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    kind=\"headless\",\n    cmd=\"env\",\n    env={\"KEY\": \"VALUE\"},\n    replicas=3,\n)\nprint(ids)\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nasync with AsyncSession() as session:\n    ids = await session.create(\n        name=\"test\",\n        image=\"images.canfar.net/skaha/astroml:latest\",\n        kind=\"headless\",\n        cmd=\"env\",\n        env={\"KEY\": \"VALUE\"},\n        replicas=3,\n    )\n    print(ids)\n</code></pre> <p> CLI</p> Create a Session<pre><code>canfar launch headless --env KEY=VALUE --replicas 3 images.canfar.net/skaha/astroml:latest \n</code></pre> <p>Installation</p> Install from PyPI<pre><code>pip install canfar\n</code></pre> Add as Dependency<pre><code>uv add canfar\n</code></pre> <p> Python Client  Explore the CLI  Codebase</p>"},{"location":"client/images/","title":"Images API","text":"<p>Overview</p> <p>The Image API allows you to get information about the publicly available images on the CANFAR Science Platform through the CANFAR Harbor Registry. It can be used to get information about all images, or filter by a specific image kind.</p>"},{"location":"client/images/#getting-image-information","title":"Getting Image Information","text":"Get image information<pre><code>from canfar.images import Images\n\nimages = Images()\nimages.fetch()\n[\n    \"images.canfar.net/canfar/base-3.12:v0.4.1\",\n    \"images.canfar.net/canucs/test:1.2.5\",\n    \"images.canfar.net/canucs/canucs:1.2.9\",\n    ...,\n]\n</code></pre> <p>But most of the time, you are only interested in images of a particular type. For example, if you want to get all the images that are available for <code>headless</code> sessions, you can do the following:</p> Get headless image information<pre><code>images.fetch(kind=\"headless\")\n</code></pre> <pre><code>[\n    \"images.canfar.net/chimefrb/testing:keep\",\n    \"images.canfar.net/lsst/lsst_v19_0_0:0.1\",\n    \"images.canfar.net/skaha/lensfit:22.11\",\n    \"images.canfar.net/skaha/lensfit:22.10\",\n    \"images.canfar.net/skaha/lensingsim:22.07\",\n    \"images.canfar.net/skaha/phosim:5.6.11\",\n    \"images.canfar.net/skaha/terminal:1.1.2\",\n    \"images.canfar.net/skaha/terminal:1.1.1\",\n    \"images.canfar.net/uvickbos/pycharm:0.1\",\n    \"images.canfar.net/uvickbos/swarp:0.1\",\n    \"images.canfar.net/uvickbos/isis:2.2\",\n    \"images.canfar.net/uvickbos/find_moving:0.1\",\n]\n</code></pre>"},{"location":"client/images/#api-reference","title":"API Reference","text":"<p>               Bases: <code>HTTPClient</code></p> <p>CANFAR Image Management.</p> <p>This class is a subclass of the <code>HTTPClient</code> class and inherits its attributes and methods.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.images import Images\n&gt;&gt;&gt; images = Images()\n&gt;&gt;&gt; images.fetch()\n</code></pre> Source code in <code>canfar/images.py</code> <pre><code>class Images(HTTPClient):\n    \"\"\"CANFAR Image Management.\n\n    This class is a subclass of the `HTTPClient` class and inherits its\n    attributes and methods.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.images import Images\n        &gt;&gt;&gt; images = Images()\n        &gt;&gt;&gt; images.fetch()\n    \"\"\"\n\n    def fetch(self, kind: str | None = None) -&gt; list[str]:\n        \"\"\"Get images from CANFAR Server.\n\n        Args:\n            kind (str | None, optional): Type of image. Defaults to None.\n\n        Returns:\n            list[str]: A list of images on the server.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.images import Images\n            &gt;&gt;&gt; images = Images()\n            &gt;&gt;&gt; images.fetch(kind=\"headless\")\n            ['images.canfar.net/skaha/terminal:1.1.1']\n        \"\"\"\n        data: dict[str, str] = {}\n        # If kind is not None, add it to the data dictionary\n        if kind:\n            data[\"type\"] = kind\n        response: Response = self.client.get(\"image\", params=data)\n        payload: list[dict[str, str]] = response.json()\n        return [str(image[\"id\"]) for image in payload]\n</code></pre>"},{"location":"client/images/#canfar.images.Images.fetch","title":"fetch","text":"<pre><code>fetch(kind=None)\n</code></pre> <p>Get images from CANFAR Server.</p> PARAMETER DESCRIPTION <code>kind</code> <p>Type of image. Defaults to None.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[str]</code> <p>list[str]: A list of images on the server.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.images import Images\n&gt;&gt;&gt; images = Images()\n&gt;&gt;&gt; images.fetch(kind=\"headless\")\n['images.canfar.net/skaha/terminal:1.1.1']\n</code></pre> Source code in <code>canfar/images.py</code> <pre><code>def fetch(self, kind: str | None = None) -&gt; list[str]:\n    \"\"\"Get images from CANFAR Server.\n\n    Args:\n        kind (str | None, optional): Type of image. Defaults to None.\n\n    Returns:\n        list[str]: A list of images on the server.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.images import Images\n        &gt;&gt;&gt; images = Images()\n        &gt;&gt;&gt; images.fetch(kind=\"headless\")\n        ['images.canfar.net/skaha/terminal:1.1.1']\n    \"\"\"\n    data: dict[str, str] = {}\n    # If kind is not None, add it to the data dictionary\n    if kind:\n        data[\"type\"] = kind\n    response: Response = self.client.get(\"image\", params=data)\n    payload: list[dict[str, str]] = response.json()\n    return [str(image[\"id\"]) for image in payload]\n</code></pre>"},{"location":"client/migration/","title":"skaha \u2192 canfar","text":"<p>In summer 2025, the CANFAR Python client was moved from shinybrar/skaha to opencadc/canfar to be officially supported by the Canadian Astronomy Data Centre (CADC). As part of this move, the Python package was renamed from <code>skaha</code> to <code>canfar</code> to better reflect a unified naming scheme across the CANFAR Science Platform.</p> <p>This guide helps you migrate from the <code>skaha</code> Python package to <code>canfar</code>.</p>"},{"location":"client/migration/#summary-of-changes","title":"Summary of changes","text":"<ul> <li>Package name: <code>skaha</code> \u2192 <code>canfar</code>.</li> <li>Breaking Changes</li> <li><code>skaha.session</code> \u2192 <code>canfar.sessions</code>.</li> <li><code>headless</code> session <code>kind</code> parameter is no longer required.</li> <li><code>session.info()</code> query now returns <code>Completed</code> instead of <code>Succeeded</code>.</li> <li>Configuration path: <code>~/.skaha/config.yaml</code> \u2192 <code>~/.canfar/config.yaml</code>.</li> <li>Logger name and location: logger <code>canfar</code>; logs under <code>~/.canfar/client.log</code>.</li> <li>Environment variables: prefix change <code>SKAHA_\u2026</code> \u2192 <code>CANFAR_\u2026</code>.</li> <li>CLI entry point: <code>canfar</code> (single entry point).</li> <li>User-Agent header: <code>python-canfar/{version}</code>.</li> <li>Protocol contracts: server URLs and custom headers remain unchanged (see notes below).</li> </ul>"},{"location":"client/migration/#code-examples","title":"Code Examples","text":"<ul> <li> <p>Python client session</p> Before<pre><code>from skaha.session import AsyncSession, Session\n</code></pre> After<pre><code>from canfar.sessions import AsyncSession, Session\n</code></pre> </li> <li> <p>Client composition</p> Before<pre><code>from skaha.client import SkahaClient\n\nclient = SkahaClient(...)\n</code></pre> After<pre><code>from canfar.client import HTTPClient\n\nclient = HTTPClient(...)\n</code></pre> </li> </ul>"},{"location":"client/migration/#environment-variables","title":"Environment variables","text":"<p>Before<pre><code>`SKAHA_TIMEOUT`, `SKAHA_CONCURRENCY`, `SKAHA_TOKEN`, `SKAHA_URL`, `SKAHA_LOGLEVEL\n</code></pre> After<pre><code>`CANFAR_TIMEOUT`, `CANFAR_CONCURRENCY`, `CANFAR_TOKEN`, `CANFAR_URL`, `CANFAR_LOGLEVEL`\n</code></pre></p>"},{"location":"client/migration/#configuration","title":"Configuration","text":"<ul> <li>The default config file moves from <code>~/.skaha/config.yaml</code> to <code>~/.canfar/config.yaml</code>.</li> <li>The structure of the YAML file remains the same.</li> </ul>"},{"location":"client/migration/#documentation-and-links","title":"Documentation and links","text":"<ul> <li>Repo: <code>https://github.com/opencadc/canfar</code></li> <li>Docs: <code>https://opencadc.github.io/canfar/</code></li> <li>Changelog: <code>https://opencadc.github.io/canfar/changelog/</code></li> </ul>"},{"location":"client/migration/#notes-on-protocol-stability","title":"Notes on protocol stability","text":"<ul> <li>Server base path segments under <code>/skaha</code> are server-side contracts and remain unchanged (for example, <code>https://ws-uv.canfar.net/skaha</code>).</li> <li>Historical header names remain unchanged (for example, <code>X-Skaha-Authentication-Type</code>, <code>X-Skaha-Registry-Auth</code>).</li> </ul>"},{"location":"client/overview/","title":"Overview","text":"<p>Overview API</p> <p>The Overview API provides information about the availability of the CANFAR Science Platform.</p> <p>               Bases: <code>HTTPClient</code></p> <p>Overview of the CANFAR Server.</p> <p>This class is a subclass of the <code>HTTPClient</code> class and inherits its attributes and methods.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.overview import Overview\n&gt;&gt;&gt; overview = Overview()\n&gt;&gt;&gt; overview.availability()\nTrue\n</code></pre> Source code in <code>canfar/overview.py</code> <pre><code>class Overview(HTTPClient):\n    \"\"\"Overview of the CANFAR Server.\n\n    This class is a subclass of the `HTTPClient` class and inherits its\n    attributes and methods.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.overview import Overview\n        &gt;&gt;&gt; overview = Overview()\n        &gt;&gt;&gt; overview.availability()\n        True\n    \"\"\"\n\n    @model_validator(mode=\"after\")\n    def _update_base_url(self) -&gt; Self:\n        \"\"\"Update base URL for the server.\n\n        Returns:\n            Self: The current object.\n        \"\"\"\n        url: str = str(self.client.base_url)\n        base: str = url.split(\"/v\", maxsplit=1)[0]\n        # The overview endpoint is not versioned, so need to remove it\n        self.client.base_url = URL(base)\n        self.asynclient.base_url = URL(base)\n        return self\n\n    def availability(self) -&gt; bool:\n        \"\"\"Check if the server backend is available.\n\n        Returns:\n            bool: True if the server is available, False otherwise.\n        \"\"\"\n        response: Response = self.client.get(\"availability\")\n        data: str = response.text\n        if not data:\n            log.error(\"No data returned from availability endpoint.\")\n            return False\n        root = ElementTree.fromstring(data)\n        available = root.find(\n            \".//{http://www.ivoa.net/xml/VOSIAvailability/v1.0}available\",\n        )\n        availaibility: str | None = available.text if available is not None else None\n\n        note = root.find(\n            \".//{http://www.ivoa.net/xml/VOSIAvailability/v1.0}note\",\n        )\n        notify: str | None = note.text if note is not None else None\n        if availaibility is None:\n            log.error(\"No availability information found in the response.\")\n            return False\n        log.info(notify if notify else \"No additional information provided.\")\n        return availaibility == \"true\"\n</code></pre>"},{"location":"client/overview/#canfar.overview.Overview.availability","title":"availability","text":"<pre><code>availability()\n</code></pre> <p>Check if the server backend is available.</p> RETURNS DESCRIPTION <code>bool</code> <p>True if the server is available, False otherwise.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>canfar/overview.py</code> <pre><code>def availability(self) -&gt; bool:\n    \"\"\"Check if the server backend is available.\n\n    Returns:\n        bool: True if the server is available, False otherwise.\n    \"\"\"\n    response: Response = self.client.get(\"availability\")\n    data: str = response.text\n    if not data:\n        log.error(\"No data returned from availability endpoint.\")\n        return False\n    root = ElementTree.fromstring(data)\n    available = root.find(\n        \".//{http://www.ivoa.net/xml/VOSIAvailability/v1.0}available\",\n    )\n    availaibility: str | None = available.text if available is not None else None\n\n    note = root.find(\n        \".//{http://www.ivoa.net/xml/VOSIAvailability/v1.0}note\",\n    )\n    notify: str | None = note.text if note is not None else None\n    if availaibility is None:\n        log.error(\"No availability information found in the response.\")\n        return False\n    log.info(notify if notify else \"No additional information provided.\")\n    return availaibility == \"true\"\n</code></pre>"},{"location":"client/quick-start/","title":"5-Minute Quick Start (Python Client)","text":"<p>Goal</p> <p>By the end of this guide, you'll authenticate, launch a compute Session on CANFAR programmatically, inspect it, read logs/events, and clean it up \u2014 all from Python.</p> <p>Prerequisites</p> <ul> <li>CADC Account \u2014 Sign up</li> <li>You have logged in at least once to the CANFAR Science Platform and the Harbor Container Registry</li> <li>Python 3.10+</li> </ul>"},{"location":"client/quick-start/#installation","title":"Installation","text":"&gt; pip install canfar --upgradeInstalled"},{"location":"client/quick-start/#authentication","title":"Authentication","text":"<p>The Python client automatically uses your active authentication context created by the CLI.</p> Login to CANFAR Science Platform<pre><code>canfar auth login\n</code></pre> <p>Login Pathways</p> <ul> <li>If you already have a valid CADC X509 certificate at <code>~/.ssl/cadcproxy.pem</code>, the CLI will reuse it automatically.</li> <li>If you're an SRCnet user, you'll be guided through an OIDC device flow in your browser.</li> </ul> Force Re-Login (optional)<pre><code>canfar auth login --force\n</code></pre> <p>What just happened?</p> <ul> <li>The CLI discovered available CANFAR/SRCnet servers</li> <li>You authenticated and obtained a certificate/token</li> <li>The active context was saved for the Python client to use</li> </ul>"},{"location":"client/quick-start/#your-first-notebook-session","title":"Your First Notebook Session","text":"<p>Launch a Jupyter notebook session programmatically.</p> Notebook Session<code>async</code> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nsession_ids = session.create(\n    name=\"my-first-notebook\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    kind=\"notebook\",\n    cores=2,\n    ram=4,\n)\nprint(session_ids)  # e.g., [\"d1tsqexh\"]\n</code></pre> <pre><code>from canfar.sessions import AsyncSession\n\nsession = AsyncSession()\nids = await session.create(\n    name=\"my-first-notebook\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    kind=\"notebook\",\n    cores=2,\n    ram=4,\n)\nprint(ids)  # e.g., [\"d1tsqexh\"]\n</code></pre> <p>What just happened?</p> <ul> <li>We connected to CANFAR using your active auth context</li> <li>A notebook container was requested with 2 CPU cores and 4 GB RAM</li> <li>The API returned the newly created session ID(s)</li> </ul>"},{"location":"client/quick-start/#get-connection-url","title":"Get Connection URL","text":"<p>Fetch details and extract the connect URL to open your notebook.</p> Connect to Session<code>async</code> <pre><code>session.connect(ids)\n</code></pre> <pre><code>await session.connect(ids)\n</code></pre>"},{"location":"client/quick-start/#peek-under-the-hood","title":"Peek Under the Hood","text":"<p>When a session is created, it goes through a series of steps to be fully deployed. You can inspect the events to understand the progress, or capture them for monitoring.</p> Deployment Events<code>async</code> <pre><code>session.events(ids, verbose=True)\n</code></pre> <pre><code>await session.events(ids, verbose=True)\n</code></pre> <p>At any point, you can also inspect the logs from the session. This is especially useful when launching long-running batch jobs.</p> Session Logs<code>async</code> <pre><code>session.logs(ids, verbose=True)\n</code></pre> <pre><code>await session.logs(ids, verbose=True)\n</code></pre>"},{"location":"client/quick-start/#clean-up","title":"Clean Up","text":"<p>When you're done, delete your session(s) to free resources for other users. </p> Destroy Session(s)<code>async</code> <pre><code>session.destroy(ids)\n</code></pre> <pre><code>await session.destroy(ids)\n</code></pre>"},{"location":"client/quick-start/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>Session won't start?</p> <p>Check available resources<pre><code>session.stats()\n</code></pre> Check events/logs<pre><code>session.events(ids, verbose=True)\nsession.logs(ids, verbose=True)\n</code></pre> Try smaller resources or different image<pre><code>session.create(..., cores=1, ram=2, image=\"images.canfar.net/skaha/astroml:latest\")\n</code></pre></p> </li> <li> <p>Authentication issues?</p> Force re-authentication<pre><code>canfar auth login --force --debug\n</code></pre> </li> </ul>"},{"location":"client/session/","title":"Session API","text":"<p>Overview</p> <p>The <code>Session</code> API is the core of canfar, enabling you to create, manage, and destroy sessions on the CANFAR Science Platform.</p> <p>               Bases: <code>HTTPClient</code></p> <p>CANFAR Session Management Client.</p> <p>This class provides methods to manage sessions, including fetching session details, creating new sessions, retrieving logs, and destroying existing sessions. It is a subclass of the <code>HTTPClient</code> class and inherits its attributes and methods.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import Session\n&gt;&gt;&gt; session = Session(\n        timeout=120,\n        concurrency=100, # No effect on sync client\n        loglevel=40,\n    )\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>class Session(HTTPClient):\n    \"\"\"CANFAR Session Management Client.\n\n    This class provides methods to manage sessions, including fetching\n    session details, creating new sessions, retrieving logs, and\n    destroying existing sessions. It is a subclass of the `HTTPClient`\n    class and inherits its attributes and methods.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import Session\n        &gt;&gt;&gt; session = Session(\n                timeout=120,\n                concurrency=100, # No effect on sync client\n                loglevel=40,\n            )\n    \"\"\"\n\n    def fetch(\n        self,\n        kind: Kind | None = None,\n        status: Status | None = None,\n        view: View | None = None,\n    ) -&gt; list[dict[str, str]]:\n        \"\"\"Fetch open sessions for the user.\n\n        Args:\n            kind (Kind | None, optional): Session kind. Defaults to None.\n            status (Status | None, optional): Session status. Defaults to None.\n            view (View | None, optional): View leve. Defaults to None.\n\n        Returns:\n            list[dict[str, str]]: Session[s] information.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import Session\n            &gt;&gt;&gt; session = Session()\n            &gt;&gt;&gt; session.fetch(kind=\"notebook\")\n            [{'id': 'ikvp1jtp',\n              'userid': 'username',\n              'image': 'image-server/image/label:latest',\n              'type': 'notebook',\n              'status': 'Running',\n              'name': 'example-notebook',\n              'startTime': '2222-12-14T02:24:06Z',\n              'connectURL': 'https://something.example.com/ikvp1jtp',\n              'requestedRAM': '16G',\n              'requestedCPUCores': '2',\n              'requestedGPUCores': '&lt;none&gt;',\n              'coresInUse': '0m',\n              'ramInUse': '101Mi'}]\n        \"\"\"\n        parameters: dict[str, Any] = build.fetch_parameters(kind, status, view)\n        response: Response = self.client.get(url=\"session\", params=parameters)\n        data: list[dict[str, str]] = response.json()\n        return data\n\n    def stats(self) -&gt; dict[str, Any]:\n        \"\"\"Get statistics for the entire platform.\n\n        Returns:\n            Dict[str, Any]: Cluster statistics.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import Session\n            &gt;&gt;&gt; session = Session()\n            &gt;&gt;&gt; session.stats()\n            {'cores': {'requestedCPUCores': 377,\n             'coresAvailable': 960,\n             'maxCores': {'cores': 32, 'withRam': '147Gi'}},\n             'ram': {'maxRAM': {'ram': '226Gi', 'withCores': 32}}}\n        \"\"\"\n        parameters = {\"view\": \"stats\"}\n        response: Response = self.client.get(\"session\", params=parameters)\n        data: dict[str, Any] = response.json()\n        return data\n\n    def info(self, ids: list[str] | str) -&gt; list[dict[str, Any]]:\n        \"\"\"Get information about session[s].\n\n        Args:\n            ids (Union[List[str], str]): Session ID[s].\n\n        Returns:\n            Dict[str, Any]: Session information.\n\n        Examples:\n            &gt;&gt;&gt; session.info(ids=\"hjko98yghj\")\n            &gt;&gt;&gt; session.info(ids=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        # Convert id to list if it is a string\n        if isinstance(ids, str):\n            ids = [ids]\n        results: list[dict[str, Any]] = []\n        for value in ids:\n            try:\n                response: Response = self.client.get(url=f\"session/{value}\")\n                results.append(response.json())\n            except HTTPError:\n                err = f\"failed to fetch session info for {value}\"\n                log.exception(err)\n        return results\n\n    def logs(\n        self,\n        ids: list[str] | str,\n        verbose: bool = False,\n    ) -&gt; dict[str, str] | None:\n        \"\"\"Get logs from a session[s].\n\n        Args:\n            ids (Union[List[str], str]): Session ID[s].\n            verbose (bool, optional): Print logs to stdout. Defaults to False.\n\n        Returns:\n            Dict[str, str]: Logs in text/plain format.\n\n        Examples:\n            &gt;&gt;&gt; session.logs(id=\"hjko98yghj\")\n            &gt;&gt;&gt; session.logs(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        if isinstance(ids, str):\n            ids = [ids]\n        parameters: dict[str, str] = {\"view\": \"logs\"}\n        results: dict[str, str] = {}\n\n        for value in ids:\n            try:\n                response: Response = self.client.get(\n                    url=f\"session/{value}\",\n                    params=parameters,\n                )\n                results[value] = response.text\n            except HTTPError:\n                err = f\"failed to fetch logs for session {value}\"\n                log.exception(err)\n\n        if verbose:\n            for key, value in results.items():\n                log.info(\"Session ID: %s\\n\", key)\n                log.info(value)\n            return None\n\n        return results\n\n    def create(\n        self,\n        name: str,\n        image: str,\n        cores: int | None = None,\n        ram: int | None = None,\n        kind: Kind = \"headless\",\n        gpu: int | None = None,\n        cmd: str | None = None,\n        args: str | None = None,\n        env: dict[str, Any] | None = None,\n        replicas: int = 1,\n    ) -&gt; list[str]:\n        \"\"\"Launch a canfar session.\n\n        Args:\n            name (str): A unique name for the session.\n            image (str): Container image to use for the session.\n            cores (int, optional): Number of cores.\n                Defaults to None, i.e. flexible mode.\n            ram (int, optional): Amount of RAM (GB).\n                Defaults to None, i.e. flexible mode.\n            kind (str, optional): Type of canfar session. Defaults to \"headless\".\n            gpu (Optional[int], optional): Number of GPUs. Defaults to None.\n            cmd (Optional[str], optional): Command to run. Defaults to None.\n            args (Optional[str], optional): Arguments to the command. Defaults to None.\n            env (Optional[Dict[str, Any]], optional): Environment variables to inject.\n                Defaults to None.\n            replicas (int, optional): Number of sessions to launch. Defaults to 1.\n\n        Notes:\n            - If cores and ram are not specified, the session will be created with\n              flexible resource allocation of upto 8 cores and 32GB of RAM.\n            - The name of the session suffixed with the replica number. eg. test-42\n              when replicas &gt; 1.\n            - Each container will have the following environment variables injected:\n                * REPLICA_ID - The replica number\n                * REPLICA_COUNT - The total number of replicas\n\n        Returns:\n            List[str]: A list of session IDs for the launched sessions.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import Session\n            &gt;&gt;&gt; session = Session()\n            &gt;&gt;&gt; session.create(\n                    name=\"test\",\n                    image='images.canfar.net/skaha/terminal:1.1.1',\n                    cores=2,\n                    ram=8,\n                    gpu=1,\n                    kind=\"headless\",\n                    cmd=\"env\",\n                    env={\"TEST\": \"test\"},\n                    replicas=2,\n                )\n            &gt;&gt;&gt; [\"hjko98yghj\", \"ikvp1jtp\"]\n        \"\"\"\n        payloads = build.create_parameters(\n            name,\n            image,\n            cores,\n            ram,\n            kind,\n            gpu,\n            cmd,\n            args,\n            env,\n            replicas,\n        )\n        results: list[str] = []\n        log.debug(\"Creating %d %s session[s].\", replicas, kind)\n        for payload in payloads:\n            try:\n                response: Response = self.client.post(url=\"session\", params=payload)\n                results.append(response.text.rstrip(\"\\r\\n\"))\n            except HTTPError:\n                err = f\"Failed to create session with payload: {payload}\"\n                log.exception(err)\n        return results\n\n    def events(\n        self,\n        ids: str | list[str],\n        verbose: bool = False,\n    ) -&gt; list[dict[str, str]] | None:\n        \"\"\"Get deployment events for a session[s].\n\n        Args:\n            ids (Union[str, List[str]]): Session ID[s].\n            verbose (bool, optional): Print events to stdout. Defaults to False.\n\n        Returns:\n            Optional[List[Dict[str, str]]]: A list of events for the session[s].\n\n        Notes:\n            When verbose is True, the events will be printed to stdout only.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import Session\n            &gt;&gt;&gt; session = Session()\n            &gt;&gt;&gt; session.events(ids=\"hjko98yghj\")\n            &gt;&gt;&gt; session.events(ids=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        if isinstance(ids, str):\n            ids = [ids]\n        results: list[dict[str, str]] = []\n        parameters: dict[str, str] = {\"view\": \"events\"}\n        for value in ids:\n            try:\n                response: Response = self.client.get(\n                    url=f\"session/{value}\",\n                    params=parameters,\n                )\n                results.append({value: response.text})\n            except HTTPError:\n                err = f\"Failed to fetch events for session {value}\"\n                log.exception(err)\n        if verbose and results:\n            for result in results:\n                for key, value in result.items():\n                    log.info(\"Session ID: %s\", key)\n                    log.info(\"\\n %s\", value)\n        return results if not verbose else None\n\n    def destroy(self, ids: str | list[str]) -&gt; dict[str, bool]:\n        \"\"\"Destroy canfar session[s].\n\n        Args:\n            ids (Union[str, List[str]]): Session ID[s].\n\n        Returns:\n            Dict[str, bool]: A dictionary of session IDs\n            and a bool indicating if the session was destroyed.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import Session\n            &gt;&gt;&gt; session = Session()\n            &gt;&gt;&gt; session.destroy(id=\"hjko98yghj\")\n            &gt;&gt;&gt; session.destroy(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        if isinstance(ids, str):\n            ids = [ids]\n        results: dict[str, bool] = {}\n        for value in ids:\n            try:\n                self.client.delete(url=f\"session/{value}\")\n                results[value] = True\n            except HTTPError:\n                msg = f\"Failed to destroy session {value}\"\n                log.exception(msg)\n                results[value] = False\n        return results\n\n    def destroy_with(\n        self,\n        prefix: str,\n        *,\n        kind: Kind = \"headless\",\n        status: Status = \"Completed\",\n    ) -&gt; dict[str, bool]:\n        \"\"\"Destroy session[s] matching a prefix or regex.\n\n        Args:\n            prefix (str): Prefix to match.\n                Treated literally unless regex meta-characters are found.\n            kind (Kind): Type of session. Defaults to \"headless\".\n            status (Status): Status of the session. Defaults to \"Completed\".\n\n        Returns:\n            Dict[str, bool]: A dictionary of session IDs\n            and a bool indicating if the session was destroyed.\n\n        Notes:\n            - If the value contains regex metacharacters (e.g., `.^$*+?{}[]()|`),\n              it is treated as a regex with :func:`re.search`.\n            - Otherwise it is treated as a literal prefix (anchored with `^`).\n            This method is useful for destroying multiple sessions at once.\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import Session\n            &gt;&gt;&gt; session = Session()\n            &gt;&gt;&gt; session.destroy_with(prefix=\"test\")  # literal prefix\n            &gt;&gt;&gt; session.destroy_with(prefix=\"desktop$\")  # regex\n        \"\"\"\n        meta = set(\".^$*+?{}[]()|\")\n        has_meta = any(ch in meta for ch in prefix)\n        try:\n            if has_meta:\n                log.info(\"destroy_with using regex pattern: %s\", prefix)\n                regex = re.compile(prefix)\n            else:\n                log.info(\"destroy_with using literal prefix: %s\", prefix)\n                regex = re.compile(rf\"^{re.escape(prefix)}\")\n        except re.error as exc:\n            msg = f\"Invalid regex pattern '{prefix}': {exc}\"\n            log.exception(msg)\n            raise ValueError(msg) from exc\n\n        sessions = self.fetch(kind=kind, status=status)\n        ids: list[str] = [\n            session[\"id\"] for session in sessions if regex.search(session[\"name\"])\n        ]\n        return self.destroy(ids)\n\n    def connect(self, ids: list[str] | str) -&gt; None:\n        \"\"\"Open session[s] in a web browser.\n\n        Args:\n            ids (Union[List[str], str]): Session ID[s].\n\n        Examples:\n            &gt;&gt;&gt; from canfar.session import Session\n            &gt;&gt;&gt; session = Session()\n            &gt;&gt;&gt; session.connect(ids=\"hjko98yghj\")\n            &gt;&gt;&gt; session.connect(ids=[\"hjko98yghj\", \"ikvp1jtp\"])\n        \"\"\"\n        if isinstance(ids, str):\n            ids = [ids]\n        info = self.info(ids)\n        log.debug(info)\n        for session in info:\n            status: str = session.get(\"status\", \"unknown\")\n            if status != \"Running\":\n                log.warning(\"Session %s is currently %s.\", session[\"id\"], status)\n                log.warning(\"Please wait for the session to be ready.\")\n                continue\n            connect_url = session.get(\"connectURL\")\n            if connect_url:\n                open_new_tab(connect_url)\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.connect","title":"connect","text":"<pre><code>connect(ids)\n</code></pre> <p>Open session[s] in a web browser.</p> PARAMETER DESCRIPTION <code>ids</code> <p>Session ID[s].</p> <p> TYPE: <code>Union[List[str], str]</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import Session\n&gt;&gt;&gt; session = Session()\n&gt;&gt;&gt; session.connect(ids=\"hjko98yghj\")\n&gt;&gt;&gt; session.connect(ids=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def connect(self, ids: list[str] | str) -&gt; None:\n    \"\"\"Open session[s] in a web browser.\n\n    Args:\n        ids (Union[List[str], str]): Session ID[s].\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import Session\n        &gt;&gt;&gt; session = Session()\n        &gt;&gt;&gt; session.connect(ids=\"hjko98yghj\")\n        &gt;&gt;&gt; session.connect(ids=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    if isinstance(ids, str):\n        ids = [ids]\n    info = self.info(ids)\n    log.debug(info)\n    for session in info:\n        status: str = session.get(\"status\", \"unknown\")\n        if status != \"Running\":\n            log.warning(\"Session %s is currently %s.\", session[\"id\"], status)\n            log.warning(\"Please wait for the session to be ready.\")\n            continue\n        connect_url = session.get(\"connectURL\")\n        if connect_url:\n            open_new_tab(connect_url)\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.create","title":"create","text":"<pre><code>create(\n    name,\n    image,\n    cores=None,\n    ram=None,\n    kind=\"headless\",\n    gpu=None,\n    cmd=None,\n    args=None,\n    env=None,\n    replicas=1,\n)\n</code></pre> <p>Launch a canfar session.</p> PARAMETER DESCRIPTION <code>name</code> <p>A unique name for the session.</p> <p> TYPE: <code>str</code> </p> <code>image</code> <p>Container image to use for the session.</p> <p> TYPE: <code>str</code> </p> <code>cores</code> <p>Number of cores. Defaults to None, i.e. flexible mode.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>ram</code> <p>Amount of RAM (GB). Defaults to None, i.e. flexible mode.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>kind</code> <p>Type of canfar session. Defaults to \"headless\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'headless'</code> </p> <code>gpu</code> <p>Number of GPUs. Defaults to None.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>cmd</code> <p>Command to run. Defaults to None.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>args</code> <p>Arguments to the command. Defaults to None.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>env</code> <p>Environment variables to inject. Defaults to None.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>replicas</code> <p>Number of sessions to launch. Defaults to 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Notes <ul> <li>If cores and ram are not specified, the session will be created with   flexible resource allocation of upto 8 cores and 32GB of RAM.</li> <li>The name of the session suffixed with the replica number. eg. test-42   when replicas &gt; 1.</li> <li>Each container will have the following environment variables injected:<ul> <li>REPLICA_ID - The replica number</li> <li>REPLICA_COUNT - The total number of replicas</li> </ul> </li> </ul> RETURNS DESCRIPTION <code>list[str]</code> <p>List[str]: A list of session IDs for the launched sessions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import Session\n&gt;&gt;&gt; session = Session()\n&gt;&gt;&gt; session.create(\n        name=\"test\",\n        image='images.canfar.net/skaha/terminal:1.1.1',\n        cores=2,\n        ram=8,\n        gpu=1,\n        kind=\"headless\",\n        cmd=\"env\",\n        env={\"TEST\": \"test\"},\n        replicas=2,\n    )\n&gt;&gt;&gt; [\"hjko98yghj\", \"ikvp1jtp\"]\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def create(\n    self,\n    name: str,\n    image: str,\n    cores: int | None = None,\n    ram: int | None = None,\n    kind: Kind = \"headless\",\n    gpu: int | None = None,\n    cmd: str | None = None,\n    args: str | None = None,\n    env: dict[str, Any] | None = None,\n    replicas: int = 1,\n) -&gt; list[str]:\n    \"\"\"Launch a canfar session.\n\n    Args:\n        name (str): A unique name for the session.\n        image (str): Container image to use for the session.\n        cores (int, optional): Number of cores.\n            Defaults to None, i.e. flexible mode.\n        ram (int, optional): Amount of RAM (GB).\n            Defaults to None, i.e. flexible mode.\n        kind (str, optional): Type of canfar session. Defaults to \"headless\".\n        gpu (Optional[int], optional): Number of GPUs. Defaults to None.\n        cmd (Optional[str], optional): Command to run. Defaults to None.\n        args (Optional[str], optional): Arguments to the command. Defaults to None.\n        env (Optional[Dict[str, Any]], optional): Environment variables to inject.\n            Defaults to None.\n        replicas (int, optional): Number of sessions to launch. Defaults to 1.\n\n    Notes:\n        - If cores and ram are not specified, the session will be created with\n          flexible resource allocation of upto 8 cores and 32GB of RAM.\n        - The name of the session suffixed with the replica number. eg. test-42\n          when replicas &gt; 1.\n        - Each container will have the following environment variables injected:\n            * REPLICA_ID - The replica number\n            * REPLICA_COUNT - The total number of replicas\n\n    Returns:\n        List[str]: A list of session IDs for the launched sessions.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import Session\n        &gt;&gt;&gt; session = Session()\n        &gt;&gt;&gt; session.create(\n                name=\"test\",\n                image='images.canfar.net/skaha/terminal:1.1.1',\n                cores=2,\n                ram=8,\n                gpu=1,\n                kind=\"headless\",\n                cmd=\"env\",\n                env={\"TEST\": \"test\"},\n                replicas=2,\n            )\n        &gt;&gt;&gt; [\"hjko98yghj\", \"ikvp1jtp\"]\n    \"\"\"\n    payloads = build.create_parameters(\n        name,\n        image,\n        cores,\n        ram,\n        kind,\n        gpu,\n        cmd,\n        args,\n        env,\n        replicas,\n    )\n    results: list[str] = []\n    log.debug(\"Creating %d %s session[s].\", replicas, kind)\n    for payload in payloads:\n        try:\n            response: Response = self.client.post(url=\"session\", params=payload)\n            results.append(response.text.rstrip(\"\\r\\n\"))\n        except HTTPError:\n            err = f\"Failed to create session with payload: {payload}\"\n            log.exception(err)\n    return results\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.destroy","title":"destroy","text":"<pre><code>destroy(ids)\n</code></pre> <p>Destroy canfar session[s].</p> PARAMETER DESCRIPTION <code>ids</code> <p>Session ID[s].</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> RETURNS DESCRIPTION <code>dict[str, bool]</code> <p>Dict[str, bool]: A dictionary of session IDs</p> <code>dict[str, bool]</code> <p>and a bool indicating if the session was destroyed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import Session\n&gt;&gt;&gt; session = Session()\n&gt;&gt;&gt; session.destroy(id=\"hjko98yghj\")\n&gt;&gt;&gt; session.destroy(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def destroy(self, ids: str | list[str]) -&gt; dict[str, bool]:\n    \"\"\"Destroy canfar session[s].\n\n    Args:\n        ids (Union[str, List[str]]): Session ID[s].\n\n    Returns:\n        Dict[str, bool]: A dictionary of session IDs\n        and a bool indicating if the session was destroyed.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import Session\n        &gt;&gt;&gt; session = Session()\n        &gt;&gt;&gt; session.destroy(id=\"hjko98yghj\")\n        &gt;&gt;&gt; session.destroy(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    if isinstance(ids, str):\n        ids = [ids]\n    results: dict[str, bool] = {}\n    for value in ids:\n        try:\n            self.client.delete(url=f\"session/{value}\")\n            results[value] = True\n        except HTTPError:\n            msg = f\"Failed to destroy session {value}\"\n            log.exception(msg)\n            results[value] = False\n    return results\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.destroy_with","title":"destroy_with","text":"<pre><code>destroy_with(prefix, *, kind='headless', status='Completed')\n</code></pre> <p>Destroy session[s] matching a prefix or regex.</p> PARAMETER DESCRIPTION <code>prefix</code> <p>Prefix to match. Treated literally unless regex meta-characters are found.</p> <p> TYPE: <code>str</code> </p> <code>kind</code> <p>Type of session. Defaults to \"headless\".</p> <p> TYPE: <code>Kind</code> DEFAULT: <code>'headless'</code> </p> <code>status</code> <p>Status of the session. Defaults to \"Completed\".</p> <p> TYPE: <code>Status</code> DEFAULT: <code>'Completed'</code> </p> RETURNS DESCRIPTION <code>dict[str, bool]</code> <p>Dict[str, bool]: A dictionary of session IDs</p> <code>dict[str, bool]</code> <p>and a bool indicating if the session was destroyed.</p> Notes <ul> <li>If the value contains regex metacharacters (e.g., <code>.^$*+?{}[]()|</code>),   it is treated as a regex with :func:<code>re.search</code>.</li> <li>Otherwise it is treated as a literal prefix (anchored with <code>^</code>). This method is useful for destroying multiple sessions at once.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import Session\n&gt;&gt;&gt; session = Session()\n&gt;&gt;&gt; session.destroy_with(prefix=\"test\")  # literal prefix\n&gt;&gt;&gt; session.destroy_with(prefix=\"desktop$\")  # regex\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def destroy_with(\n    self,\n    prefix: str,\n    *,\n    kind: Kind = \"headless\",\n    status: Status = \"Completed\",\n) -&gt; dict[str, bool]:\n    \"\"\"Destroy session[s] matching a prefix or regex.\n\n    Args:\n        prefix (str): Prefix to match.\n            Treated literally unless regex meta-characters are found.\n        kind (Kind): Type of session. Defaults to \"headless\".\n        status (Status): Status of the session. Defaults to \"Completed\".\n\n    Returns:\n        Dict[str, bool]: A dictionary of session IDs\n        and a bool indicating if the session was destroyed.\n\n    Notes:\n        - If the value contains regex metacharacters (e.g., `.^$*+?{}[]()|`),\n          it is treated as a regex with :func:`re.search`.\n        - Otherwise it is treated as a literal prefix (anchored with `^`).\n        This method is useful for destroying multiple sessions at once.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import Session\n        &gt;&gt;&gt; session = Session()\n        &gt;&gt;&gt; session.destroy_with(prefix=\"test\")  # literal prefix\n        &gt;&gt;&gt; session.destroy_with(prefix=\"desktop$\")  # regex\n    \"\"\"\n    meta = set(\".^$*+?{}[]()|\")\n    has_meta = any(ch in meta for ch in prefix)\n    try:\n        if has_meta:\n            log.info(\"destroy_with using regex pattern: %s\", prefix)\n            regex = re.compile(prefix)\n        else:\n            log.info(\"destroy_with using literal prefix: %s\", prefix)\n            regex = re.compile(rf\"^{re.escape(prefix)}\")\n    except re.error as exc:\n        msg = f\"Invalid regex pattern '{prefix}': {exc}\"\n        log.exception(msg)\n        raise ValueError(msg) from exc\n\n    sessions = self.fetch(kind=kind, status=status)\n    ids: list[str] = [\n        session[\"id\"] for session in sessions if regex.search(session[\"name\"])\n    ]\n    return self.destroy(ids)\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.events","title":"events","text":"<pre><code>events(ids, verbose=False)\n</code></pre> <p>Get deployment events for a session[s].</p> PARAMETER DESCRIPTION <code>ids</code> <p>Session ID[s].</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>verbose</code> <p>Print events to stdout. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>list[dict[str, str]] | None</code> <p>Optional[List[Dict[str, str]]]: A list of events for the session[s].</p> Notes <p>When verbose is True, the events will be printed to stdout only.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import Session\n&gt;&gt;&gt; session = Session()\n&gt;&gt;&gt; session.events(ids=\"hjko98yghj\")\n&gt;&gt;&gt; session.events(ids=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def events(\n    self,\n    ids: str | list[str],\n    verbose: bool = False,\n) -&gt; list[dict[str, str]] | None:\n    \"\"\"Get deployment events for a session[s].\n\n    Args:\n        ids (Union[str, List[str]]): Session ID[s].\n        verbose (bool, optional): Print events to stdout. Defaults to False.\n\n    Returns:\n        Optional[List[Dict[str, str]]]: A list of events for the session[s].\n\n    Notes:\n        When verbose is True, the events will be printed to stdout only.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import Session\n        &gt;&gt;&gt; session = Session()\n        &gt;&gt;&gt; session.events(ids=\"hjko98yghj\")\n        &gt;&gt;&gt; session.events(ids=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    if isinstance(ids, str):\n        ids = [ids]\n    results: list[dict[str, str]] = []\n    parameters: dict[str, str] = {\"view\": \"events\"}\n    for value in ids:\n        try:\n            response: Response = self.client.get(\n                url=f\"session/{value}\",\n                params=parameters,\n            )\n            results.append({value: response.text})\n        except HTTPError:\n            err = f\"Failed to fetch events for session {value}\"\n            log.exception(err)\n    if verbose and results:\n        for result in results:\n            for key, value in result.items():\n                log.info(\"Session ID: %s\", key)\n                log.info(\"\\n %s\", value)\n    return results if not verbose else None\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.fetch","title":"fetch","text":"<pre><code>fetch(kind=None, status=None, view=None)\n</code></pre> <p>Fetch open sessions for the user.</p> PARAMETER DESCRIPTION <code>kind</code> <p>Session kind. Defaults to None.</p> <p> TYPE: <code>Kind | None</code> DEFAULT: <code>None</code> </p> <code>status</code> <p>Session status. Defaults to None.</p> <p> TYPE: <code>Status | None</code> DEFAULT: <code>None</code> </p> <code>view</code> <p>View leve. Defaults to None.</p> <p> TYPE: <code>View | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[dict[str, str]]</code> <p>list[dict[str, str]]: Session[s] information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import Session\n&gt;&gt;&gt; session = Session()\n&gt;&gt;&gt; session.fetch(kind=\"notebook\")\n[{'id': 'ikvp1jtp',\n  'userid': 'username',\n  'image': 'image-server/image/label:latest',\n  'type': 'notebook',\n  'status': 'Running',\n  'name': 'example-notebook',\n  'startTime': '2222-12-14T02:24:06Z',\n  'connectURL': 'https://something.example.com/ikvp1jtp',\n  'requestedRAM': '16G',\n  'requestedCPUCores': '2',\n  'requestedGPUCores': '&lt;none&gt;',\n  'coresInUse': '0m',\n  'ramInUse': '101Mi'}]\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def fetch(\n    self,\n    kind: Kind | None = None,\n    status: Status | None = None,\n    view: View | None = None,\n) -&gt; list[dict[str, str]]:\n    \"\"\"Fetch open sessions for the user.\n\n    Args:\n        kind (Kind | None, optional): Session kind. Defaults to None.\n        status (Status | None, optional): Session status. Defaults to None.\n        view (View | None, optional): View leve. Defaults to None.\n\n    Returns:\n        list[dict[str, str]]: Session[s] information.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import Session\n        &gt;&gt;&gt; session = Session()\n        &gt;&gt;&gt; session.fetch(kind=\"notebook\")\n        [{'id': 'ikvp1jtp',\n          'userid': 'username',\n          'image': 'image-server/image/label:latest',\n          'type': 'notebook',\n          'status': 'Running',\n          'name': 'example-notebook',\n          'startTime': '2222-12-14T02:24:06Z',\n          'connectURL': 'https://something.example.com/ikvp1jtp',\n          'requestedRAM': '16G',\n          'requestedCPUCores': '2',\n          'requestedGPUCores': '&lt;none&gt;',\n          'coresInUse': '0m',\n          'ramInUse': '101Mi'}]\n    \"\"\"\n    parameters: dict[str, Any] = build.fetch_parameters(kind, status, view)\n    response: Response = self.client.get(url=\"session\", params=parameters)\n    data: list[dict[str, str]] = response.json()\n    return data\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.info","title":"info","text":"<pre><code>info(ids)\n</code></pre> <p>Get information about session[s].</p> PARAMETER DESCRIPTION <code>ids</code> <p>Session ID[s].</p> <p> TYPE: <code>Union[List[str], str]</code> </p> RETURNS DESCRIPTION <code>list[dict[str, Any]]</code> <p>Dict[str, Any]: Session information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; session.info(ids=\"hjko98yghj\")\n&gt;&gt;&gt; session.info(ids=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def info(self, ids: list[str] | str) -&gt; list[dict[str, Any]]:\n    \"\"\"Get information about session[s].\n\n    Args:\n        ids (Union[List[str], str]): Session ID[s].\n\n    Returns:\n        Dict[str, Any]: Session information.\n\n    Examples:\n        &gt;&gt;&gt; session.info(ids=\"hjko98yghj\")\n        &gt;&gt;&gt; session.info(ids=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    # Convert id to list if it is a string\n    if isinstance(ids, str):\n        ids = [ids]\n    results: list[dict[str, Any]] = []\n    for value in ids:\n        try:\n            response: Response = self.client.get(url=f\"session/{value}\")\n            results.append(response.json())\n        except HTTPError:\n            err = f\"failed to fetch session info for {value}\"\n            log.exception(err)\n    return results\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.logs","title":"logs","text":"<pre><code>logs(ids, verbose=False)\n</code></pre> <p>Get logs from a session[s].</p> PARAMETER DESCRIPTION <code>ids</code> <p>Session ID[s].</p> <p> TYPE: <code>Union[List[str], str]</code> </p> <code>verbose</code> <p>Print logs to stdout. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>dict[str, str] | None</code> <p>Dict[str, str]: Logs in text/plain format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; session.logs(id=\"hjko98yghj\")\n&gt;&gt;&gt; session.logs(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def logs(\n    self,\n    ids: list[str] | str,\n    verbose: bool = False,\n) -&gt; dict[str, str] | None:\n    \"\"\"Get logs from a session[s].\n\n    Args:\n        ids (Union[List[str], str]): Session ID[s].\n        verbose (bool, optional): Print logs to stdout. Defaults to False.\n\n    Returns:\n        Dict[str, str]: Logs in text/plain format.\n\n    Examples:\n        &gt;&gt;&gt; session.logs(id=\"hjko98yghj\")\n        &gt;&gt;&gt; session.logs(id=[\"hjko98yghj\", \"ikvp1jtp\"])\n    \"\"\"\n    if isinstance(ids, str):\n        ids = [ids]\n    parameters: dict[str, str] = {\"view\": \"logs\"}\n    results: dict[str, str] = {}\n\n    for value in ids:\n        try:\n            response: Response = self.client.get(\n                url=f\"session/{value}\",\n                params=parameters,\n            )\n            results[value] = response.text\n        except HTTPError:\n            err = f\"failed to fetch logs for session {value}\"\n            log.exception(err)\n\n    if verbose:\n        for key, value in results.items():\n            log.info(\"Session ID: %s\\n\", key)\n            log.info(value)\n        return None\n\n    return results\n</code></pre>"},{"location":"client/session/#canfar.sessions.Session.stats","title":"stats","text":"<pre><code>stats()\n</code></pre> <p>Get statistics for the entire platform.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: Cluster statistics.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from canfar.session import Session\n&gt;&gt;&gt; session = Session()\n&gt;&gt;&gt; session.stats()\n{'cores': {'requestedCPUCores': 377,\n 'coresAvailable': 960,\n 'maxCores': {'cores': 32, 'withRam': '147Gi'}},\n 'ram': {'maxRAM': {'ram': '226Gi', 'withCores': 32}}}\n</code></pre> Source code in <code>canfar/sessions.py</code> <pre><code>def stats(self) -&gt; dict[str, Any]:\n    \"\"\"Get statistics for the entire platform.\n\n    Returns:\n        Dict[str, Any]: Cluster statistics.\n\n    Examples:\n        &gt;&gt;&gt; from canfar.session import Session\n        &gt;&gt;&gt; session = Session()\n        &gt;&gt;&gt; session.stats()\n        {'cores': {'requestedCPUCores': 377,\n         'coresAvailable': 960,\n         'maxCores': {'cores': 32, 'withRam': '147Gi'}},\n         'ram': {'maxRAM': {'ram': '226Gi', 'withCores': 32}}}\n    \"\"\"\n    parameters = {\"view\": \"stats\"}\n    response: Response = self.client.get(\"session\", params=parameters)\n    data: dict[str, Any] = response.json()\n    return data\n</code></pre>"},{"location":"client/testing/","title":"Testing","text":"<p>This document provides comprehensive information about testing opencadc/canfar.</p>"},{"location":"client/testing/#overview","title":"Overview","text":"<p>Canfar uses pytest as its testing framework. The test suite includes unit tests, integration tests, and end-to-end tests that verify the functionality of the client library.</p>"},{"location":"client/testing/#prerequisites","title":"Prerequisites","text":"<p>To run tests for Canfar, you need:</p> <ol> <li>Valid CANFAR Account: Access to the CANFAR Science Platform</li> <li>X.509 Certificate: For authentication with CANFAR services</li> <li>Python Environment: Set up with uv</li> </ol> <p>For certificate generation, refer to the get started section.</p>"},{"location":"client/testing/#running-tests","title":"Running Tests","text":""},{"location":"client/testing/#basic-test-execution","title":"Basic Test Execution","text":"<p>Run all tests: <pre><code>uv run pytest\n</code></pre></p> <p>Run tests with verbose output: <pre><code>uv run pytest -v\n</code></pre></p> <p>Run tests with coverage report: <pre><code>uv run pytest --cov\n</code></pre></p>"},{"location":"client/testing/#test-categories","title":"Test Categories","text":"<p>Canfar tests are organised with markers to help you run specific subsets:</p>"},{"location":"client/testing/#slow-tests","title":"Slow Tests","text":"<p>Some tests are marked as \"slow\" because they involve: - Network operations with CANFAR services - Waiting for session state changes - Authentication timeouts - Long-running operations</p> <p>Skip slow tests for faster development: <pre><code>uv run pytest -m \"not slow\"\n</code></pre></p> <p>Run only slow tests: <pre><code>uv run pytest -m \"slow\"\n</code></pre></p>"},{"location":"client/testing/#integration-tests","title":"Integration Tests","text":"<p>Tests that interact with external services: <pre><code>uv run pytest -m \"integration\"\n</code></pre></p>"},{"location":"client/testing/#unit-tests","title":"Unit Tests","text":"<p>Fast, isolated tests: <pre><code>uv run pytest -m \"unit\"\n</code></pre></p>"},{"location":"client/testing/#test-methodology","title":"Test Methodology","text":"<p>Tests are organised in the <code>tests/</code> directory and follow a specific naming convention that mirrors the source code structure. This approach ensures that tests are easy to locate and maintain.</p> <p>The naming convention is as follows:</p> <ul> <li>If the source file is <code>canfar/path/to/file.py</code>, the corresponding test file will be <code>tests/test_path_to_file.py</code>.</li> <li>If the source file is <code>canfar/module.py</code>, the corresponding test file will be <code>tests/test_module.py</code>.</li> </ul> <p>For example:</p> <ul> <li>The tests for <code>canfar/client.py</code> are located in <code>tests/test_client.py</code>.</li> <li>The tests for <code>canfar/auth/oidc.py</code> are located in <code>tests/test_auth_oidc.py</code>.</li> </ul> <p>This structure makes it straightforward to find the tests associated with a particular module or file.</p>"},{"location":"client/testing/#development-workflow","title":"Development Workflow","text":"<p>For efficient development, follow this testing workflow:</p> <ol> <li> <p>During Development: Run fast tests only    <pre><code>uv run pytest -m \"not slow\"\n</code></pre></p> </li> <li> <p>Before Committing: Run the full test suite    <pre><code>uv run pytest\n</code></pre></p> </li> <li> <p>Debugging Specific Issues: Run individual test files    <pre><code>uv run pytest tests/test_session.py\n</code></pre></p> </li> </ol>"},{"location":"client/testing/#test-configuration","title":"Test Configuration","text":"<p>Test configuration is defined in <code>pyproject.toml</code>:</p> <pre><code>[tool.pytest.ini_options]\nmarkers = [\n    \"integration: marks tests as integration tests\",\n    \"unit: marks tests as unit tests\",\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\n    \"order: marks tests that need to run in a specific order\",\n]\n</code></pre>"},{"location":"client/testing/#continuous-integration","title":"Continuous Integration","text":"<p>In CI environments, all tests (including slow ones) are executed to ensure complete validation. The CI pipeline:</p> <ol> <li>Sets up authentication with CANFAR</li> <li>Runs the complete test suite</li> <li>Generates coverage reports</li> <li>Cleans up authentication artifacts</li> </ol>"},{"location":"client/testing/#writing-tests","title":"Writing Tests","text":"<p>When contributing new tests:</p> <ol> <li>Follow the naming convention: Create a test file that mirrors the source file's path and name.</li> <li>Mark slow tests: Add <code>@pytest.mark.slow</code> to any test that involves network operations, interacts with external services, or has long execution times. This allows developers to skip these tests for a faster development cycle.</li> <li>Use appropriate markers: Mark tests as <code>unit</code>, <code>integration</code>, etc.</li> <li>Add docstrings: Document what each test verifies.</li> </ol> <p>Example of a slow test: <pre><code>import pytest\n\n@pytest.mark.slow\ndef test_long_running_operation():\n    \"\"\"Test that involves waiting or network operations.\"\"\"\n    # Test implementation\n    pass\n</code></pre></p>"},{"location":"client/testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"client/testing/#authentication-issues","title":"Authentication Issues","text":"<ul> <li>Ensure your X.509 certificate is valid and not expired</li> <li>Check that you have access to the CANFAR Science Platform</li> <li>Verify your certificate is in the correct location (<code>~/.ssl/</code>)</li> </ul>"},{"location":"client/testing/#slow-test-timeouts","title":"Slow Test Timeouts","text":"<ul> <li>Slow tests have built-in timeouts (typically 60 seconds)</li> <li>If tests consistently timeout, check your network connection</li> <li>Platform availability may affect test execution times</li> </ul>"},{"location":"client/testing/#test-failures","title":"Test Failures","text":"<ul> <li>Check if the CANFAR Science Platform is accessible</li> <li>Verify your authentication credentials</li> <li>Review test logs for specific error messages</li> </ul>"},{"location":"client/updates/","title":"What's New in CANFAR","text":"<p>Stay up to date with the latest features, improvements, and changes in CANFAR.</p>"},{"location":"client/updates/#recent-updates","title":"Recent Updates","text":"<p>New in v1.1+</p> <p>v1.0</p> <p> Breaking Changes</p> <ul> <li>Deprecation of support for Python 3.8 and 3.9.</li> <li>The Python package has been renamed from <code>skaha</code> to <code>canfar</code>.</li> <li>The <code>skaha.session</code> API has been deprecated in favor of <code>canfar.sessions</code>.</li> <li>See Migration guide to migrate from skaha \u2192 canfar.</li> </ul> <p> CLI Support</p> <ul> <li>Comprehensive CLI support has been added to the client under the <code>canfar</code> entry point. See CLI Reference for more information.</li> <li>The <code>canfar</code> CLI is the recommended way to manage authentication. See Authentication Contexts for more information.</li> </ul> <p>\ud83c\udf0e SRCnet Support</p> <ul> <li>CANFAR now supports launching sessions on all the SRCnet CANFAR Science Platform instances worldwide.</li> </ul> <p> OIDC Authentication</p> <ul> <li>OpenID Connect (OIDC) authentication is now supported for all SRCnet Science Platform servers where applicable.</li> </ul> <p> Documentation</p> <ul> <li>Complete overhaul to bring all documentation sources under a single roof.</li> <li>Significant improvements to the Python client and brand new CLI documentation.</li> </ul> <p>New in v0.7+</p> <p>New in v0.4+</p>"},{"location":"client/updates/#improved-session-data-validation","title":"\ud83d\udee1\ufe0f Improved Session Data Validation","text":"<p>The CANFAR CLI now features enhanced resilience when handling session data from the Science Platform API. This update improves the user experience when the API returns incomplete or malformed session information.</p> <p>What Changed:</p> <ul> <li>Graceful Degradation: The CLI commands (<code>canfar info</code>, <code>canfar ps</code>) now continue to work even when the API returns incomplete session data, displaying partial information instead of crashing.</li> <li>Better Error Reporting: Missing or invalid fields are tracked internally and can be viewed with the <code>--debug</code> flag for troubleshooting.</li> <li>Enhanced Display: Resource usage metrics for flexible sessions is now reported with better readability.</li> <li>Type Safety: Session type validation has been strengthened using Pydantic's built-in validators.</li> </ul> <p>Example:</p> Flexible Session Resource Usage<pre><code>$ canfar info n2tr1rpf\n\nCANFAR Session Info for n2tr1rpf\n\n  Session ID    n2tr1rpf\n  Name          spy-panda\n  Status        Running\n  Type          notebook\n  CPU Usage     0.001 core(s)\n  RAM Usage     0.22 GB\n  GPU Usage     Unknown # (GPU not requested)\n</code></pre> Debug Mode for Troubleshooting<pre><code>$ canfar info --debug n2tr1rpf \n\n# Shows additional warnings about missing/invalid fields\n\u26a0\ufe0f  Session Response Warnings:\n    \u2022 missing or invalid startTime in response\n    \u2022 missing or invalid expiryTime in response\n</code></pre>"},{"location":"client/updates/#enhanced-authentication-system","title":"\ud83d\udd10 Enhanced Authentication System","text":"<p>Canfar now features a comprehensive authentication system with support for multiple authentication modes and automatic credential management.</p> Authentication Examples<pre><code>from canfar.client import HTTPClient\nfrom pathlib import Path\n\n# X.509 certificate authentication\nclient = HTTPClient(certificate=Path(\"/path/to/cert.pem\"))\n\n# OIDC token authentication (configured)\nclient = HTTPClient()  # Uses auth.mode = \"oidc\"\n\n# Bearer token authentication\nfrom pydantic import SecretStr\nclient = HTTPClient(token=SecretStr(\"your-token\"))\n</code></pre>"},{"location":"client/updates/#asynchronous-sessions","title":"\ud83d\ude80 Asynchronous Sessions","text":"<p>Canfar now supports asynchronous sessions using the <code>AsyncSession</code> class while maintaining 1-to-1 compatibility with the <code>Session</code> class.</p> Asynchronous Session Creation<pre><code>from canfar.session import AsyncSession\n\nasession = AsyncSession()\nresponse = await asession.create(\n    name=\"test\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    cores=2,\n    ram=8,\n    gpu=1,\n    kind=\"headless\",\n    cmd=\"env\",\n    env={\"KEY\": \"VALUE\"},\n    replicas=3,\n)\n</code></pre>"},{"location":"client/updates/#backend-upgrades","title":"\ud83d\uddc4\ufe0f Backend Upgrades","text":"<ul> <li>\ud83d\udce1 Canfar now uses the <code>httpx</code> library for making HTTP requests instead of <code>requests</code>. This adds asynchronous support and also to circumvent the <code>requests</code> dependence on <code>urllib3</code> which was causing SSL issues on MacOS. See this issue for more details.</li> <li>\ud83d\udd11 Canfar now supports multiple authentication methods including X.509 certificates, OIDC tokens, and bearer tokens with automatic SSL context management.</li> <li>\ud83c\udfce\ufe0f\ud83d\udca8 Added <code>loglevel</code> and <code>concurrency</code> support to manage the new explosion in functionality!</li> <li>\ud83d\udd0d Comprehensive debug logging for authentication flow and client creation troubleshooting.</li> </ul>"},{"location":"client/updates/#logs-to-stdout","title":"\ud83e\uddfe Logs to <code>stdout</code>","text":"<p>The <code>[Session|AsyncSession].logs</code> method now prints colored output to <code>stdout</code> instead of returning them as a string with <code>verbose=True</code> flag.</p> Session Logs<pre><code>from canfar.session import AsyncSession\n\nasession = AsyncSession()\nawait asession.logs(ids=[\"some-uuid\"], verbose=True)\n</code></pre>"},{"location":"client/updates/#firefly-support","title":"\ud83e\udeb0 Firefly Support","text":"<p>Canfar now supports launching <code>firefly</code> session on the CANFAR Science Platform.</p> Firefly Session Creation<pre><code>session.create(\n    name=\"firefly\",\n    image=\"images.canfar.net/skaha/firefly:latest\",\n)\n</code></pre>"},{"location":"client/updates/#private-images","title":"\ud83d\udd10 Private Images","text":"<p>Starting October 2024, to create a session with a private container image from the CANFAR Harbor Registry, you will need to provide your harbor <code>username</code> and the <code>CLI Secret</code> through a <code>ContainerRegistry</code> object.</p> Private Image Registry Configuration<pre><code>from canfar.models import ContainerRegistry\nfrom canfar.session import Session\n\nregistry = ContainerRegistry(username=\"username\", secret=\"sUp3rS3cr3t\")\nsession = Session(registry=registry)\n</code></pre> <p>Alternatively, if you have environment variables, <code>CANFAR_REGISTRY_USERNAME</code> and <code>CANFAR_REGISTRY_SECRET</code>, you can create a <code>ContainerRegistry</code> object without providing the <code>username</code> and <code>secret</code>.</p> Private Image Registry with Environment Variables<pre><code>from canfar.models import ContainerRegistry\n\nregistry = ContainerRegistry()\n</code></pre>"},{"location":"client/updates/#destroy-sessions","title":"\ud83d\udca3 Destroy Sessions","text":"Destroying Sessions<pre><code>from canfar.session import Session\n\nsession = Session()\nsession.destroy_with(prefix=\"test\", kind=\"headless\", status=\"Running\")\nsession.destroy_with(prefix=\".*-analysis\", kind=\"headless\", status=\"Pending\")\n</code></pre>"},{"location":"client/updates/#previous-versions","title":"Previous Versions","text":"<p>For a complete history of changes, see the Changelog.</p>"},{"location":"client/updates/#stay-updated","title":"Stay Updated","text":"<ul> <li>\ud83d\udce2 GitHub Releases</li> <li>\ud83d\udcac Discussions</li> </ul>"},{"location":"demos/srcnet-workshop/","title":"From Interactive Notebooks to Batch Processing with CANFAR","text":"<p>Who is this for?</p> <p>This presentation is for astronomers who want to:</p> <ul> <li>Run code on the cloud without complex setup.</li> <li>Use familiar tools like Jupyter Notebooks.</li> <li>Scale their analysis from a single interactive session to hundreds of parallel jobs.</li> <li>Process large datasets efficiently.</li> </ul> <p>Whether you're new to coding or a seasoned power-user, these tools are designed to be intuitive and powerful.</p>"},{"location":"demos/srcnet-workshop/#cli-your-mission-control","title":"CLI: Your Mission Control","text":"<p>The <code>canfar</code> CLI is the easiest way to get started. It's your interactive mission control for the CANFAR science platform.</p>"},{"location":"demos/srcnet-workshop/#step-0-prerequisites","title":"Step 0: Prerequisites","text":"<p>Install <code>pipx</code> if you don't have it already. <code>pipx</code> is a tool for installing and running Python applications in isolated environments.</p> <pre><code>python3 -m pip install --user pipx\npython3 -m pipx ensurepath\n</code></pre> <p>Alternatively, on you can use OS specific package managers:</p> macOSLinux (Ubuntu/Debian)Windows (scoop) <pre><code>brew install pipx\npipx ensurepath\n</code></pre> <pre><code>sudo apt update\nsudo apt install pipx\npipx ensurepath\n</code></pre> <pre><code>scoop install pipx\npipx ensurepath\n</code></pre>"},{"location":"demos/srcnet-workshop/#step-1-installation","title":"Step 1: Installation","text":"<p>It's a single command. Open your terminal and type:</p> <pre><code>pipx install canfar\n</code></pre> Installation Walkthrough <p> #pragma: allowlist secret</p>"},{"location":"demos/srcnet-workshop/#step-2-first-contact-authentication","title":"Step 2: First Contact (Authentication)","text":"<p>Tell <code>canfar</code> who you are. This command discovers all available servers worldwide and guides you through a one-time login.</p> <pre><code>canfar auth login -f\n</code></pre> Auth Walkthrough <p> #pragma: allowlist secret</p> <p>You'll be prompted for your credentials, and the CLI handles the rest, saving a secure token for future commands.</p> <p>What just happened?</p> <ul> <li>We installed the <code>canfar</code> python package, which provides the <code>canfar</code> command-line interface (CLI).</li> <li>We authenticated with the CANFAR Science Platform.</li> <li>All future commands will use this secure authentication context automatically.</li> </ul>"},{"location":"demos/srcnet-workshop/#your-first-interactive-notebook","title":"Your First Interactive Notebook","text":"<p>Let's launch a Jupyter notebook that comes pre-loaded with common astronomy libraries like <code>AstroPy</code>, <code>SciPy</code>, and <code>Matplotlib</code>.</p>"},{"location":"demos/srcnet-workshop/#step-1-create-the-notebook","title":"Step 1: Create the Notebook","text":"<pre><code># Launch a notebook using a pre-built astronomy image\ncanfar create notebook skaha/astroml:latest\n</code></pre> Create Notebook Walkthrough <p> #pragma: allowlist secret</p> <p>The CLI will return a unique <code>SESSION_ID</code> for your notebook (e.g., <code>d1tsqexh</code>).</p>"},{"location":"demos/srcnet-workshop/#step-2-check-the-status-of-your-session","title":"Step 2: Check the Status of Your Session","text":"<pre><code>canfar ps --all\n</code></pre> Check Status Walkthrough <p> #pragma: allowlist secret</p>"},{"location":"demos/srcnet-workshop/#step-3-open-in-your-browser","title":"Step 3: Open in Your Browser","text":"<p>Use the session ID to open the notebook directly in your web browser.</p> <pre><code>canfar open &lt;SESSION_ID&gt;\n</code></pre> <p>You're in!</p> <p>You now have a fully functional JupyterLab environment running on the powerful CANFAR Science Platform. </p>"},{"location":"demos/srcnet-workshop/#step-4-clean-up","title":"Step 4: Clean Up","text":"<p>When you're finished, it's important to delete your session to free up resources for others.</p> <pre><code>canfar delete &lt;SESSION_ID&gt;\n</code></pre> Clean Up Walkthrough <p> #pragma: allowlist secret</p>"},{"location":"demos/srcnet-workshop/#the-power-of-headless-mode-from-interactive-to-batch","title":"The Power of Headless Mode: From Interactive to Batch","text":"<p>This is where the magic happens.</p> <p>What if you have a Python script that runs your analysis, and you don't need the full interactive notebook? You can run it in \"headless\" (batch) mode using the exact same container image.</p> <p>Let's say you have a script named <code>echo.py</code>.</p> <pre><code>canfar create headless skaha/astroml:latest -- python echo.py\n</code></pre> <p>Interactive to Batch, Seamlessly</p> <p>You can develop your analysis interactively in a notebook session, save your code to a python script, and then run it at scale using a headless session. No changes to your environment are needed.</p> <p>To check the output of your headless job, you can use the <code>logs</code> command.</p> <pre><code>canfar logs &lt;SESSION_ID&gt;\n</code></pre>"},{"location":"demos/srcnet-workshop/#scaling-up-from-one-to-many","title":"Scaling Up: From One to Many","text":"<p>Need to process hundreds of files? You can launch multiple copies (replicas) of your headless job with a single command.</p> <pre><code>canfar create --replicas 10 headless skaha/astroml:latest -- python echo.py\n</code></pre> <p>You now have 10 containers in parallel. But how do you divide the work?</p>"},{"location":"demos/srcnet-workshop/#the-python-client-distributing-your-workload","title":"The Python Client: Distributing Your Workload","text":"<p>For complex logic like distributing data across many jobs, we switch to the <code>canfar</code> Python Client.</p>"},{"location":"demos/srcnet-workshop/#the-problem","title":"The Problem","text":"<p>You have 1000 FITS files and 100 replicas. How does each replica know which files to process?</p>"},{"location":"demos/srcnet-workshop/#the-solution","title":"The Solution","text":"<p>The <code>canfar.helpers.distributed</code> module provides simple but powerful functions to automatically split up a list of files among your replicas.</p> Distributing Workloads<pre><code>from canfar.helpers import distributed\nfrom glob import glob\n# Assume your analysis logic is in this function\nfrom your_code import run_analysis \n\n# 1. Get a list of all your data files\nall_files = glob(\"/arc/projects/your_project/*.fits\")\n\n# 2. 'chunk' automatically gives each replica its unique subset of files\n#    It reads environment variables ($REPLICA_ID, $REPLICA_COUNT) set by CANFAR.\nmy_files = distributed.chunk(all_files)\n\n# 3. Process only your assigned files\nprint(f\"This replica will process {len(list(my_files))} files.\")\nfor datafile in my_files:\n    run_analysis(datafile)\n\nprint(\"Done!\")\n</code></pre> <p>Work Distribution Strategies</p> <ul> <li><code>distributed.chunk(items)</code>: Divides data into contiguous blocks. Good for files of similar size.</li> <li><code>distributed.stripe(items)</code>: Distributes data like dealing cards (round-robin). Good for files of varying sizes to balance the load.</li> </ul>"},{"location":"demos/srcnet-workshop/#putting-it-all-together-a-complete-workflow","title":"Putting It All Together: A Complete Workflow","text":"<p>Here is the complete workflow, from launching jobs programmatically to processing data in parallel.</p> Launching Jobs Programmatically<pre><code>from canfar.sessions import Session\n\n# This uses the same authentication from 'canfar auth login'\nsession = Session()\n\n# Launch 100 replicas, each running our processing script\nids = session.create(\n    name=\"galaxy-processing-batch\",\n    kind=\"headless\",\n    image=\"skaha/astroml:latest\",\n    cmd=\"python\",\n    args=[\"my_script.py\"],\n    replicas=100,\n)\n\nprint(f\"Successfully launched {len(ids)} processing jobs!\")\n</code></pre>"},{"location":"platform/","title":"CANFAR Science Platform","text":"<p>The Canadian Advanced Network for Astronomy Research lets you access software environments and large datasets on the cloud designed specifically for astronomical research.</p> <p>\ud83d\ude80 Platform Overview</p> <ul> <li>Scalable cloud compute and storage to analyze very large datasets from anywhere.</li> <li>Browser-based interactive environments for instant, device-independent data exploration.</li> <li>Collaborative tools that make teamwork and data sharing simple.</li> <li>Well\u2011tested default scientific containers plus customizable images and registries for large-team workflows.</li> </ul>"},{"location":"platform/#quick-access","title":"\ud83d\ude80 Quick Access","text":"\ud83c\udd95 New to CANFAR?\ud83d\udc64 Experienced Users\u2699\ufe0f Developers &amp; Automation <p>Start Your Research Journey</p> \ud83d\udcd6 Getting Started Guide Complete onboarding with tutorials, examples, and first session setup \ud83e\udde9 Platform Concepts Understanding CANFAR architecture, containers, sessions, and storage systems \ud83d\udd11 Account Setup User management, groups, and collaboration access <p>Direct Platform Access</p> \ud83c\udf10 Science Portal Launch interactive sessions and manage computing resources \ud83d\udcc1 File Manager Access and organize your research data and project files \ud83d\udc65 Group Management Manage research teams and collaborative permissions <p>Programmatic Access</p> \u2699\ufe0f CANFAR CLI Command-line tools for session management and automation \ud83d\udc0d Python Client Programmatic API access for workflows and integration \ud83d\udc33 Container Registry Browse and manage software environments"},{"location":"platform/#platform-documentation","title":"\ud83d\udcda Platform Documentation","text":""},{"location":"platform/#core-platform-guides","title":"Core Platform Guides","text":"\ud83e\udde9 Platform Concepts Comprehensive platform understanding - Architecture, containers, sessions, storage systems, and browser-based workflows \ud83d\udd11 User Management &amp; Permissions Collaboration and access control - Accounts, groups, ACLs, container registry, and API authentication \ud83d\udcc4 Data Publication Service DOI management and data preservation - Request DOIs, referee access, and freeze your data for publication \u2601\ufe0f Legacy Cloud Platform Traditional VM infrastructure - OpenStack cloud, VM batch processing"},{"location":"platform/#getting-started-resources","title":"Getting Started Resources","text":"\ud83d\udcd6 Getting Started Guide Hands-on tutorials - Account setup, first sessions, data management, and workflow examples \ud83d\uddc4\ufe0f Storage Systems Data management mastery - ARC storage, VOSpace, quotas, and collaboration workflows \ud83d\udc33 Container Environments Software environment usage - Available containers, custom builds, and workflow integration \ud83d\udda5\ufe0f Sessions Computing workflows - Notebooks, desktops, CARTA, Firefly, specialized browser-based apps"},{"location":"platform/#advanced-topics","title":"Advanced Topics","text":"\ud83c\udfed Batch Processing Automated workflows - Large-scale processing, job management, and production pipelines \u2699\ufe0f Command Line Interface Platform automation - CLI tools, scripting, and workflow management \ud83d\udc0d Python Client API integration - Programmatic access, custom applications, and automation"},{"location":"platform/#support-community","title":"\ud83c\udd98 Support &amp; Community","text":""},{"location":"platform/#getting-help","title":"Getting Help","text":"\ud83c\udd98 Help &amp; Support Technical assistance - Support channels, issue reporting, and community resources \u2753 Frequently Asked Questions Common questions and solutions - Platform usage, troubleshooting, and best practices"},{"location":"platform/#external-resources","title":"\ud83d\udd17 External Resources","text":""},{"location":"platform/#canfar-ecosystem","title":"CANFAR Ecosystem","text":"Canadian Astronomy Data Centre (CADC) Data archives and services - Search observatory data, VO services CANFAR Main Website Project information - Access various CANFAR services, news, partnerships, and organizational details"},{"location":"platform/best-practices/","title":"Best Practices for Astronomy Pipelines on CANFAR Science Platform","text":"<p>Developing astronomy data-processing pipelines for modern, cloud-native platforms (like CANFAR Science Platform) requires combining solid software practices with an understanding of scalable, containerized environments. Below are some of the key best practices, aimed at students and researchers building astronomy pipelines.</p>"},{"location":"platform/best-practices/#writing-scalable-and-batch-friendly-code","title":"Writing Scalable and Batch-Friendly Code","text":"<ul> <li> <p>Design for Batch Execution: Your pipeline code should run to completion without any manual intervention. This means no GUI pop-ups, no <code>input()</code> prompts, and no reliance on interactive environments. The science platform can execute your code in a batch session that has no interactive interface. Write your scripts to take parameters (like <code>filepaths</code> or <code>data_ids</code>) and then run autonomously, logging progress as needed.</p> </li> <li> <p>Use Command-Line Arguments or Environment Variables: Never hard-code dataset paths, filenames, or other configuration inside your code. Instead, pass them in so the pipeline is flexible. For example, use CLI tools like <code>argparse, click, or typer</code> to parse an <code>--input</code> file path and <code>--output</code> directory. Alternatively, read environment variables that the platform or user sets (e.g., your code might read an <code>$INPUT_DATA</code> env var).</p> <pre><code>import os, argparse\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input\", default=os.getenv(\"INPUT_FILE\"))\n    parser.add_argument(\"--output_dir\", default=os.getenv(\"OUTPUT_DIR\", \".\"))\n    args = parser.parse_args()\n\n    infile = args.input\n    outfile = args.output_dir\n    print(f\"Processing {infile} and saving to {outfile}\")\n</code></pre> <pre><code>python3 example.py --input something.fits --output_dir save/something/here\n</code></pre> </li> <li> <p>One Script for Interactive and Batch: It\u2019s helpful if the same code can run in JupyterLab (for debugging or exploration) and in batch mode (for large-scale runs). For example, if you develop your pipeline in a Jupyter notebook, you can export a Jupyter notebook to a Python script using, </p> <pre><code>jupyter nbconvert --to script notebook.ipynb\n</code></pre> </li> </ul>"},{"location":"platform/best-practices/#scaling-out-vs-scaling-up","title":"Scaling Out vs. Scaling Up","text":"<ul> <li> <p>Prefer Many Small Containers over One Big One: Embrace horizontal scaling. The CANFAR Science Platform can easily run hundreds of container jobs in parallel, each on separate resources, but requesting a single container with 100\u00d7 resources is impractical. For example, if you need to process 100 independent data files, it\u2019s more efficient to run 100 containers with 1 CPU and 4 GB RAM each, than to run one container with 100 CPUs and 400 GB RAM doing it serially. The platform is optimized to handle large-scale parallel workloads for big datasets</p> <ul> <li> <p>Why not one huge job? Large containers (e.g. 32 cores, 128 GB RAM) are harder to schedule and could sit waiting in a queue. They also encourage monolithic processing that can\u2019t be easily checkpointed. By contrast, many 1-core jobs can be scheduled as resources free up and can drastically reduce overall processing time by working concurrently.</p> </li> <li> <p>Memory is at a premium: Splitting data into smaller chunks means each container uses less memory. This is crucial when dealing with terabyte-scale data \u2013 no single machine might have enough RAM to hold everything, but distributed processing can handle it piecewise. In practice: If each container processes, say, 50 GB of data at a time, a 100 TB dataset can be processed by 2000 such tasks in parallel.</p> </li> </ul> </li> <li> <p>Avoid Unnecessary Resource Requests: Don\u2019t request more CPU or RAM than your job actually needs. Start with a modest amount (the platform\u2019s \u201cflexible mode\u201d will give resources as available) to prototype and test your pipeline. Only scale up to batch jobs after understanding the your resource requirements.</p> </li> <li> <p>Utilize Parallel Libraries Cautiously: Some Python libraries (NumPy, TensorFlow, etc.) will use multi-threading or multi-processing under the hood. Be mindful of this when running many containers to set these to the actual number of cores requested by your job. e.g. If you request 4 cores, set <code>OMP_NUM_THREADS=4</code> in your environment.</p> <pre><code>from canfar.sessions import Session\n\nsession = Session()\nsession_info = session.create(\n    kind=\"headless\",\n    image=\"images.canfar.net/library/pipeline:latest\",\n    cores=4,\n    ram=16,\n    env={\"OMP_NUM_THREADS\": \"4\"}\n)\n</code></pre> </li> </ul>"},{"location":"platform/best-practices/#memory-and-io-efficiency","title":"Memory and I/O Efficiency","text":"<p>When dealing with large astronomy datasets, how you handle memory and I/O can make or break your pipeline and can be the difference between a job that finishes in minutes and one that takes hours.</p> <ul> <li> <p>Stream or Chunk Your Data: Avoid loading extremely large datasets entirely into memory if possible. Libraries like <code>asttropy</code> can read FITS files in a memory-mapped mode (so data is loaded on-the-fly), and HDF5 (via <code>h5py</code>) allow chunked reads. If you have a 100 GB table, use <code>pandas</code> or <code>polars</code> to read it in chunks instead of <code>pd.read_csv</code> on the whole file at once. By processing data in streaming fashion, your job can handle inputs larger than RAM.</p> </li> <li> <p>Free Memory Early: In long-running containers that processes multiple files sequentially, make sure to free resources between files. Delete large arrays or data structures after use (e.g., <code>del big_array</code>) or wrap them in functions so they go out of scope. Python\u2019s garbage collector will reclaim memory, but you can encourage it by not holding references longer than necessary. This is important when one container processes many tasks in sequence. If each file is 5 GB in memory and you process 10 of them in one container, you need to release each one before moving to the next to stay within a 5 GB budget.</p> </li> </ul>"},{"location":"platform/best-practices/#saving-results","title":"Saving Results","text":"<ul> <li> <p>Write to Persistent Storage: Remember that container file systems are ephemeral \u2013 once a session ends, anything written to the container\u2019s own disk (other than mounted volumes) is lost. Always direct your outputs to the mounted storage provided by the platform e.g., your home directory, project space.</p> </li> <li> <p>Unique and Descriptive Output Names: When running many tasks in parallel, never have them all write to the same filename (like <code>output.fits</code> in the same folder). This would cause conflicts and overwrites. Instead, generate output filenames that incorporate a unique element, such as the input name or an index.</p> </li> <li> <p>Organize Outputs Predictably: Consider writing outputs of different pipeline stages to separate directories. For instance, raw data stays in <code>raw/</code>, calibration results in <code>calib/</code>, intermediate analysis in <code>analysis/</code>, and final catalogs or plots in <code>results/</code>. This structure helps both humans and programs (like a next-stage script) to locate what they need. It\u2019s much easier to point a plotting script at a <code>results/</code> directory of processed files than to pick through one huge directory of mixed files.</p> </li> <li> <p>Implement Checkpoints and Resume Logic: If your pipeline can take hours or days to run, add checkpointing to save progress periodically. Break the pipeline into logical stages e.g., data reduction -&gt; feature extraction -&gt; modeling -&gt; results. After each stage, output data to disk (or a database). If a later stage needs to be re-run, you don\u2019t have to redo earlier stages.</p> </li> </ul>"},{"location":"platform/best-practices/#headless-processing-vs-gui-tools","title":"Headless Processing vs. GUI Tools","text":"<ul> <li> <p>Avoid GUI Tools in Batch Pipelines: Many traditional astronomy tools (like DS9, TOPCAT, IRAF GUI, etc.) require X11 or a graphical interface. These are not suited for automated pipelines running on a cluster. In a containerized platform, there may not be an easy display available for GUI apps, and attempts to use virtual displays or X forwarding can be fragile. It\u2019s best to use command-line or library equivalents for any analysis.</p> </li> <li> <p>Separate Interactive Analysis from Batch Jobs: If you do need to use a GUI-based tool for some part of your work (for instance, visually inspecting a subset of data or interactive data exploration), do that in a dedicated interactive session separate from the batch pipeline. The platform provides specialized sessions for this purpose.</p> </li> </ul>"},{"location":"platform/best-practices/#managing-dependencies-and-python-environments","title":"Managing Dependencies and Python Environments","text":"<ul> <li> <p>Using Modern Dependency Managers: The platform\u2019s base images come with tools like uv, pipx, and conda pre-installed (Soon, Work in Progress). Prefer these for managing Python packages:</p> <ul> <li>uv: a fast Python package manager that can replace pip/venv workflows. It allows you to declare dependencies inside your scripts and automatically handles virtual environments for each run</li> <li>pipx: for installing and running standalone CLI tools in isolated environments, ensuring they don\u2019t pollute your main env.</li> <li>conda/mamba: for packages that are easier to get from conda (especially if C/C++ libs are needed). Base containers (Soon, Work in Progress) include conda (via mamba for speed).</li> </ul> </li> <li> <p>Inline Dependency Declaration with uv: One powerful pattern is to declare your script\u2019s requirements using <code>PEP 723 style</code> metadata. For example, using <code>uv</code>, you can add a header to your Python script listing its packages. When you run this script with uv run, uv will create an isolated environment with these packages installed, so your script runs with exactly the needed libraries. This approach ensures anyone running the script (or any container executing it) gets the correct dependencies without manual setup.</p> <pre><code># /// script.py\n# dependencies = [\n#   \"astropy\",\n#   \"photutils&gt;=1.9\",\n#   \"polars\"\n# ]\n# ///\nimport astropy\nimport photutils\nimport polars\n</code></pre> <pre><code>uv run python script.py\n</code></pre> </li> <li> <p>Keep Environments Reproducible: Avoid \u201cit works on my machine | session\u201d issues by documenting dependencies. If you installed something interactively in Jupyter, add it to your with your manager of choice.</p> </li> </ul>"},{"location":"platform/best-practices/#container-packaging","title":"Container Packaging","text":"<ul> <li> <p>Group tools by logical pipeline step: - Don't create a monolithic container with all tools for every stage or split every tool into its own micro-container. Create one container per logical pipeline step, bundling all tools needed for that step\u2014whether they're interactive or batch-oriented.</p> </li> <li> <p>Reuse the same container for both interactive testing (e.g. JupyterLab) and headless batch execution of the same code. This ensures consistency, debuggability, and minimizes surprises in batch jobs.</p> </li> <li> <p>Test Locally: Before scaling up, test your container build and functionality on a small dataset locally or in an interactive session. This ensures the environment has everything needed.</p> </li> </ul>"},{"location":"platform/best-practices/#keep-containers-lean","title":"Keep Containers Lean","text":"<ul> <li> <p>Use Official Base Images (Soon, Work in Progress): If building your own container image for a pipeline, start with a provided base image rather than starting from scratch. For example, the <code>base:22.04</code> image is a good general starting point \u2013 and it comes with <code>uv, pipx, conda</code> pre-installed and configured.     <pre><code>FROM images.canfar.net/library/base:22.04\n</code></pre></p> </li> <li> <p>Keep Images Lightweight: Minimize what you add to the container. Uninstall unnecessary packages and avoid including large test data or docs inside the image. A smaller image pulls faster and uses less storage, benefiting batch runs. Use a .dockerignore file to exclude files like docs, tests, and git directories from the build context</p> </li> <li> <p>Optimize Dockerfile Layers: Combine related commands into single <code>RUN</code> statements and clean up after installations to reduce image size. For example, update and install Linux packages in one layer, then remove package lists and caches:</p> </li> </ul> <pre><code># Combining update, install, and cleanup in one layer\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    astrometry.net sextractor \\\n &amp;&amp; apt-get clean \\\n &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n</code></pre>"},{"location":"platform/cloud/","title":"Legacy Cloud Platform (OpenStack VMs)","text":"<p>Legacy Platform</p> <p>This documentation covers the legacy CANFAR cloud platform based on OpenStack virtual machines. For new users, we recommend the modern CANFAR Science Platform which provides container-based sessions, improved workflows, and better resource management.</p> <p>\ud83c\udfaf What You'll Learn</p> <ul> <li>How to access legacy CANFAR cloud services via Digital Research Alliance Canada (DRAC) OpenStack</li> <li>Differences between DRAC OpenStack cloud and modern CANFAR platform access</li> <li>VM management and batch processing workflows</li> <li>Migration strategies to the modern platform</li> </ul> <p>The legacy CANFAR cloud services are hosted on the Digital Research Alliance Canada (DRAC) OpenStack infrastructure. This platform provides traditional virtual machines for users who require persistent compute environments or have specific legacy workflow requirements.</p>"},{"location":"platform/cloud/#migration-to-modern-platform","title":"\ud83d\udd04 Migration to Modern Platform","text":"<p>Before proceeding with OpenStack VMs, consider whether the modern CANFAR Science Platform meets your needs:</p>"},{"location":"platform/cloud/#modern-platform-advantages","title":"Modern Platform Advantages","text":"<ul> <li>Container-based sessions: Faster startup, better resource utilisation</li> <li>Browser-native access: No SSH or complex networking required</li> <li>Automated resource management: Dynamic scaling and optimised allocation</li> <li>Integrated storage: Seamless access to shared <code>/arc/</code></li> <li>Pre-configured environments: Ready-to-use astronomy software stacks</li> </ul>"},{"location":"platform/cloud/#when-to-use-legacy-platform","title":"When to Use Legacy Platform","text":"<ul> <li>Persistent services: Long-running applications that need to stay active</li> <li>Custom system configurations: Root access requirements</li> <li>Legacy workflows: Existing scripts and pipelines that require VMs</li> <li>Old VM Batch processing: Large-scale automated job processing which was installed on a VM</li> </ul>"},{"location":"platform/cloud/#access-and-authentication","title":"\ud83d\udd11 Access and Authentication","text":""},{"location":"platform/cloud/#key-differences-from-drac-defaults","title":"Key Differences from DRAC Defaults","text":"<ul> <li>Credentials: Sign in with CADC Username/Password (not a DRAC account)</li> <li>Portal: Use the arbutus-canfar portal (instead of arbutus)</li> <li>Resource policy: Interactive analysis gets reasonable quotas; batch processing can scale to large footprints</li> </ul>"},{"location":"platform/cloud/#registration-allocation","title":"Registration &amp; Allocation","text":"<p>A CADC account is required to access cloud services.</p> <ol> <li>Register a CADC account (if you don't have one)</li> <li>Email CANFAR support with:<ul> <li>Project Name</li> <li>CADC Account Username</li> <li>Estimated resources (storage, compute; whether you need batch)</li> <li>A short description of your use case (2\u20133 sentences)</li> </ul> </li> </ol> <p>CANFAR will review and coordinate project/quotas on the DRAC side.</p>"},{"location":"platform/cloud/#virtual-machine-management","title":"\ud83d\udda5\ufe0f Virtual Machine Management","text":""},{"location":"platform/cloud/#creating-and-configuring-vms","title":"Creating and Configuring VMs","text":""},{"location":"platform/cloud/#1-create-a-vm","title":"1. Create a VM","text":"<p>Use the DRAC web dashboard:</p> <ol> <li>Sign in to Dashboard with CADC username/password</li> <li>Each CANFAR allocation maps to an OpenStack Project. Use the top-left project picker to switch if you belong to multiple</li> <li>Follow DRAC's Creating a Linux VM documentation</li> </ol>"},{"location":"platform/cloud/#2-import-an-ssh-public-key","title":"2. Import an SSH Public Key","text":"<p>OpenStack prefers SSH key pairs over passwords:</p> <ul> <li>If you do not have a key pair, run <code>ssh-keygen</code> locally or follow DRAC's SSH Keys documentation</li> <li>In Compute \u2192 Key Pairs, click Import Key Pair</li> <li>Name the key and paste your public key (default path <code>~/.ssh/id_rsa.pub</code>)</li> </ul>"},{"location":"platform/cloud/#3-allocate-a-public-ip","title":"3. Allocate a Public IP","text":"<ul> <li>Go to Network \u2192 Floating IPs</li> <li>If none is listed, click Allocate IP to Project</li> <li>Typically, each project has one public IP; if exhausted you'll see Quota Exceeded</li> </ul>"},{"location":"platform/cloud/#4-launch-an-instance","title":"4. Launch an Instance","text":"<ul> <li>In Compute \u2192 Instances \u2192 Launch Instance, choose:</li> <li>Source: canfar-ubuntu-20.04 (important for batch compatibility)</li> <li>Flavor: e.g., <code>c2-7.5gb-30</code> (2 vCPU / 7.5 GiB RAM / ~31 GiB ephemeral disk)</li> <li>Key Pair: select your SSH key</li> <li>Click Launch</li> </ul>"},{"location":"platform/cloud/#5-connect-to-the-instance","title":"5. Connect to the Instance","text":"<p>After status becomes Running:</p> <ol> <li>Associate floating IP (menu \u2192 Associate Floating IP)</li> <li> <p>SSH to the instance:</p> <pre><code>ssh ubuntu@[floating_ip]\n</code></pre> </li> <li> <p>Create a local user matching your CADC account:</p> <pre><code>sudo canfar_create_user [user]\n</code></pre> </li> </ol>"},{"location":"platform/cloud/#vm-configuration-and-tools","title":"\ud83d\udd27 VM Configuration and Tools","text":""},{"location":"platform/cloud/#pre-built-vm-helpers","title":"Pre-built VM Helpers","text":"<p>The <code>canfar-ubuntu-20.04</code> and <code>canfar-rocky-8</code> images include helpful tools:</p> <pre><code># Obtain a CADC proxy (legacy helper)\ncadc_cert -u [user]\n\n# Create/update ~/.netrc for CADC services\ncadc_dotnetrc\n\n# Set up /mnt/scratch for temporary storage\ncanfar_setup_scratch\n\n# Create a local user and grant sudo access\ncanfar_create_user [user]\n\n# Update CANFAR scripts and CADC clients\ncanfar_update\n</code></pre>"},{"location":"platform/cloud/#system-maintenance","title":"System Maintenance","text":"<p>Keep your VM updated and secure:</p> <pre><code># Ubuntu/Debian systems\nsudo apt update &amp;&amp; sudo apt dist-upgrade\n\n# Rocky/CentOS systems\nsudo dnf update\n</code></pre>"},{"location":"platform/cloud/#batch-processing-workflow","title":"\ud83d\ude80 Batch Processing Workflow","text":""},{"location":"platform/cloud/#setting-up-batch-processing","title":"Setting Up Batch Processing","text":"<p>This tutorial demonstrates building a basic source detection pipeline for CFHT MegaCam images on a CANFAR VM with fast access to the CADC archive and VOSpace.</p> <p>You will learn to</p> <ul> <li>Create/manage VMs on DRAC OpenStack / CANFAR</li> <li>Access CADC VOSpace from VMs</li> <li>Submit batch jobs that run your pipeline</li> </ul>"},{"location":"platform/cloud/#1-create-a-vm_1","title":"1. Create a VM","text":"<p>Use the DRAC web dashboard.</p> <ol> <li>Sign in to Dashboard with CADC <code>[user]</code>/password.</li> <li>Each CANFAR allocation maps to an OpenStack <code>[project]</code>. Use the top-left project picker to switch if you belong to multiple.</li> </ol> <p>Follow DRAC's Creating a Linux VM. Summary below.</p>"},{"location":"platform/cloud/#2-import-an-ssh-public-key_1","title":"2. Import an SSH Public Key","text":"<p>OpenStack prefers SSH key pairs over passwords.</p> <ul> <li>If you do not have a key pair, run <code>ssh-keygen</code> locally or follow DRAC's SSH Keys documentation.</li> <li>In Compute \u2192 Key Pairs, click Import Key Pair.</li> <li>Name the key and paste your public key (default path <code>~/.ssh/id_rsa.pub</code>).</li> </ul>"},{"location":"platform/cloud/#3-allocate-a-public-ip_1","title":"3. Allocate a Public IP","text":"<ul> <li>Go to Network \u2192 Floating IPs.</li> <li>If none is listed, click Allocate IP to Project.</li> <li>Typically, each project has one public IP; if exhausted you'll see Quota Exceeded.</li> </ul>"},{"location":"platform/cloud/#4-launch-an-instance_1","title":"4. Launch an Instance","text":"<ul> <li>In Compute \u2192 Instances \u2192 Launch Instance, choose:</li> <li>Source: canfar-ubuntu-20.04 (important for batch)</li> <li>Flavor: e.g., <code>c2-7.5gb-30</code> (2 vCPU / 7.5 GiB RAM / ~31 GiB ephemeral disk)</li> <li>Key Pair: select your SSH key</li> <li>Click Launch.</li> </ul>"},{"location":"platform/cloud/#5-connect-to-the-instance_1","title":"5. Connect to the Instance","text":"<p>After status becomes Running, first associate the floating IP (menu \u2192 Associate Floating IP), then SSH:</p> <pre><code>ssh ubuntu@[floating_ip]\n</code></pre> <p>Create a local user matching your CADC account (for audit/minimal access):</p> <pre><code>sudo canfar_create_user [user]\nlogout\nssh [user]@[floating_ip]\n</code></pre> <p>Default image users</p> <ul> <li>Ubuntu images: <code>ubuntu</code></li> <li>Rocky Linux images: <code>rocky</code></li> </ul>"},{"location":"platform/cloud/#install-software","title":"Install software","text":"<p>The base VM image comes with only a minimal set of packages. For this example, we need to install two additional tools:</p> <ul> <li>Source Extractor (source detection): software used to detect astronomical sources in FITS images, producing catalogues of stars and galaxies.</li> <li>funpack (FITS decompressor; Ubuntu package <code>libcfitsio-bin</code>): a decompression utility for FITS images. Most FITS images provided by CADC are Rice-compressed and stored with an <code>.fz</code> extension. Since Source Extractor only accepts uncompressed images, we will use <code>funpack</code> to uncompress them. The <code>funpack</code> executable is distributed as part of the <code>libcfitsio-bin</code> package in Debian/Ubuntu.</li> </ul> <p>Because both tools are available from the Ubuntu software repository, we can install them system-wide after updating the package index:</p> Install packages<pre><code>sudo apt update -y\nsudo apt install -y source-extractor libcfitsio-bin\n</code></pre>"},{"location":"platform/cloud/#test-on-the-vm","title":"Test on the VM","text":"<p>Use the ephemeral disk (mounted at <code>/mnt</code>) for scratch.</p> <pre><code>sudo canfar_setup_scratch  # create /mnt/scratch with proper permissions\ncd /mnt/scratch\ncp /usr/share/source-extractor/default* .\ncat &gt; default.param &lt;&lt;'EOF'\nNUMBER\nMAG_AUTO\nX_IMAGE\nY_IMAGE\nEOF\ncadcget cadc:CFHT/1056213p.fits.fz\nfunpack -D 1056213p.fits.fz\nsource-extractor 1056213p.fits -CATALOG_NAME 1056213p.cat\n</code></pre> <p>Scratch space</p> <ul> <li>Run <code>canfar_setup_scratch</code> each time you boot a new instance.</li> <li>In batch mode, each job gets its own scratch directory (not <code>/mnt/scratch</code>).</li> </ul>"},{"location":"platform/cloud/#persist-results-to-vospace","title":"Persist results to VOSpace","text":"<p>Ephemeral storage is wiped when the VM terminates. Upload the output <code>1056213p.cat</code> to VOSpace (the VM includes the <code>vos</code> client).</p> <p>Obtain a proxy certificate for automated access:</p> <pre><code>cadc_dotnetrc      # one-time helper to create ~/.netrc\ncadc-get-cert -n   # generate an X509 proxy (default 10 days)\n</code></pre> <pre><code>vcp 1056213p.cat vos:[project]/\n</code></pre> <p>Credential hygiene</p> <p><code>.netrc</code> stores credentials in plaintext. Use only on controlled hosts and restrict permissions: <code>chmod 600 ~/.netrc</code>.</p>"},{"location":"platform/cloud/#snapshot-the-instance","title":"Snapshot the instance","text":"<p>In the Instances view, click Create Snapshot (e.g., name it <code>image-reduction-2023-08-21</code>).</p> <p>Warning</p> <p>Avoid writes on the VM while a snapshot is being created.</p> <p>Without a snapshot, ephemeral data is lost when the instance is deleted. Volume-backed VMs persist data but are not suitable for batch.</p>"},{"location":"platform/cloud/#automate-as-a-batch-script","title":"Automate as a batch script","text":"<p>CANFAR batch is powered by HTCondor; Cloud Scheduler launches worker VMs on demand.</p> <p>Create <code>~/do_catalogue.bash</code>:</p> <pre><code>#!/usr/bin/env bash\nset -euo pipefail\nid=\"$1\"\ncadcget \"cadc:CFHT/${id}.fits.fz\"\nfunpack -D \"${id}.fits.fz\"\ncp /usr/share/source-extractor/default* .\ncat &gt; default.param &lt;&lt;'EOF'\nNUMBER\nMAG_AUTO\nX_IMAGE\nY_IMAGE\nEOF\nsource-extractor \"${id}.fits\" -CATALOG_NAME \"${id}.cat\"\nvcp \"${id}.cat\" \"vos:[project]/\"\n</code></pre>"},{"location":"platform/cloud/#write-a-submission-file","title":"Write a submission file","text":"<p>Submit four image IDs: <code>1056215p 1056216p 1056217p 1056218p</code>.</p> do_catalogue.sub<pre><code>executable = do_catalogue.bash\n\noutput = do_catalogue-$(arguments).out\nerror  = do_catalogue-$(arguments).err\nlog    = do_catalogue-$(arguments).log\n\nqueue arguments from (\n  1056215p\n  1056216p\n  1056217p\n  1056218p\n)\n</code></pre>"},{"location":"platform/cloud/#submit-jobs","title":"Submit jobs","text":"<p>Two authorizations are needed:</p> <ul> <li>Access to snapshots in <code>[project]</code></li> <li>Write access to <code>vos:[project]</code></li> </ul> <p>On the batch login node <code>batch.canfar.net</code>:</p> <pre><code>ssh [user]@batch.canfar.net\n. [project]-openrc.sh     # set OpenStack env (once per session)\n</code></pre> <p>Submit:</p> <pre><code>canfar_submit do_catalogue.sub image-reduction-2023-08-21 c2-7.5gb-30\n</code></pre> <p>Where:</p> <ul> <li><code>do_catalogue.sub</code>: submission file</li> <li><code>image-reduction-2023-08-21</code>: snapshot image name</li> <li><code>c2-7.5gb-30</code>: VM flavor (list via <code>openstack flavor list</code>)</li> </ul> <p>Monitor:</p> <pre><code>condor_q\ncondor_q -all   # all users summary\n</code></pre> <p>When the interactive VM is no longer needed, delete it from the dashboard (Delete Instances).</p>"},{"location":"platform/cloud/#extras-helpful-commands-vm-maintenance","title":"Extras: Helpful Commands &amp; VM Maintenance","text":"<p>Keep the OS updated:</p> <pre><code># Ubuntu/Debian systems\nsudo apt update &amp;&amp; sudo apt dist-upgrade\n\n# Rocky/CentOS systems  \nsudo dnf update\n</code></pre> <p>Prebuilt VM helpers (<code>canfar-ubuntu-20.04</code> / <code>canfar-rocky-8</code>):</p> <ul> <li><code>cadc_cert -u [user]</code>: obtain a CADC proxy (legacy helper)</li> <li><code>cadc_dotnetrc</code>: create/update <code>~/.netrc</code></li> <li><code>canfar_setup_scratch</code>: set up <code>/mnt/scratch</code></li> <li><code>canfar_create_user [user]</code>: create a local user and grant sudo</li> <li><code>canfar_update</code>: update CANFAR scripts and CADC clients</li> </ul>"},{"location":"platform/cloud/#migration-strategies","title":"Migration Strategies","text":""},{"location":"platform/cloud/#option-1-containerise-your-workflow","title":"Option 1: Containerise Your Workflow","text":"<p>Convert your VM-based pipeline to containers:</p> <ol> <li>Create a Dockerfile based on your VM configuration</li> <li>Test the container on the modern platform</li> <li>Submit container-based jobs instead of VM jobs</li> </ol>"},{"location":"platform/cloud/#option-2-hybrid-approach","title":"Option 2: Hybrid Approach","text":"<p>Use both platforms as appropriate:</p> <ul> <li>Development: Modern platform for interactive analysis</li> <li>Production: VM batch jobs for large-scale processing</li> <li>Data sharing: Common storage accessible from both</li> </ul>"},{"location":"platform/cloud/#option-3-gradual-migration","title":"Option 3: Gradual Migration","text":"<p>Migrate components incrementally:</p> <ol> <li>Start with interactive work on the modern platform</li> <li>Keep batch processing on VMs initially</li> <li>Gradually containerise pipeline components</li> <li>Complete migration when ready</li> </ol>"},{"location":"platform/cloud/#migration-resources","title":"\ud83d\udd17 Migration Resources","text":""},{"location":"platform/cloud/#modern-platform-documentation","title":"Modern Platform Documentation","text":"<ul> <li>CANFAR Science Platform \u2192: Overview of modern container-based platform</li> <li>Interactive Sessions \u2192: Browser-based computing environments</li> <li>Container Usage \u2192: Working with pre-built and custom containers</li> <li>Batch Jobs \u2192: Modern batch processing workflows</li> </ul>"},{"location":"platform/cloud/#support-and-migration-assistance","title":"Support and Migration Assistance","text":"<ul> <li>Email: support@canfar.net for migration planning</li> <li>Documentation: Platform comparison and migration guides</li> <li>Consultation: Schedule time to discuss your specific use case</li> </ul>"},{"location":"platform/cloud/#platform-comparison","title":"\ud83d\udccb Platform Comparison","text":"Feature Legacy OpenStack VMs Modern CANFAR Platform Access Method SSH, web dashboard Browser-based interface Startup Time 5-10 minutes 30-60 seconds Resource Management Manual VM sizing Dynamic allocation Software Installation Manual setup required Pre-configured containers Collaboration Shared VM access Session sharing, unified storage Maintenance User responsibility Platform managed Best For Persistent services, custom configs Interactive analysis, quick workflows <p>Choosing the Right Platform</p> <ul> <li>New users: Start with the modern CANFAR platform</li> <li>Existing VM users: Consider migration for better efficiency</li> <li>Persistent services: Continue using VMs where appropriate</li> <li>Hybrid workflows: Use both platforms as needed</li> </ul>"},{"location":"platform/concepts/","title":"CANFAR Platform Concepts","text":"<p>Understanding the architecture and core concepts behind the CANFAR Science Platform for astronomical research.</p> <p>\ud83c\udfaf Core Concepts</p> <p>Essential platform knowledge for all users:</p> <ul> <li>Cloud Architecture: Container-based platform design</li> <li>Container Environments: Pre-built software stacks for astronomy</li> <li>Session Management: Interactive and batch computing resources</li> <li>Storage Systems: Data persistence and collaboration</li> <li>Browser Access: Minimal-installation web-based workflows</li> </ul>"},{"location":"platform/concepts/#canfar-science-platform-overview","title":"\ud83d\ude80 CANFAR Science Platform Overview","text":"<p>The Canadian Advanced Network for Astronomy Research (CANFAR) Science Platform is a cloud-native computing environment designed specifically for astronomical research workflows.</p>"},{"location":"platform/concepts/#platform-design-philosophy","title":"Platform Design Philosophy","text":"<p>CANFAR eliminates traditional barriers to astronomical computing:</p> <ul> <li>Minimal Software Installation: Pre-built environments with astronomy packages ready to use</li> <li>Browser-Based Access: Complete workflows accessible through web interfaces</li> <li>Scalable Resources: Computing power that grows with your project needs</li> <li>Collaborative Infrastructure: Shared storage and standardised environments</li> <li>Reproducible Science: Container-based workflows ensure consistent results</li> </ul>"},{"location":"platform/concepts/#core-benefits","title":"Core Benefits","text":"Individual ResearchersResearch TeamsLarge Projects <ul> <li>Minimal Setup: Pre-configured containers ready to use immediately</li> <li>Hardware Liberation: Access powerful computing without owning servers</li> <li>Location Independence: Work from anywhere with just a web browser</li> <li>Data Protection: Automatic backups and managed storage systems</li> </ul> <ul> <li>Environment Standardisation: Identical software stacks across the team</li> <li>Seamless Collaboration: Shared workspaces and data access</li> <li>Session Sharing: Live collaboration on analysis workflows</li> <li>Project Management: Centralised resource and permission management</li> </ul> <ul> <li>Dynamic Scaling: Resources adjust to computational demands</li> <li>Batch Processing: Automated workflows for large dataset processing</li> <li>Custom Environments: Specialised containers for unique requirements</li> <li>Archive Integration: Fast access to astronomical data repositories</li> </ul>"},{"location":"platform/concepts/#platform-architecture","title":"\ud83c\udfd7\ufe0f Platform Architecture","text":"<p>CANFAR is built on modern cloud-native technologies designed for scalability, reliability, and ease of use. Understanding the architecture helps you leverage the platform effectively.</p>"},{"location":"platform/concepts/#system-components","title":"System Components","text":"<pre><code>graph LR\n    %% User Entry Point\n    User[\"\ud83d\udc64 Scientist\"]:::user\n\n    %% Portal Layer\n    Portal[\"\ud83c\udf10 Science Portal&lt;br/&gt;canfar.net\"]:::portal\n    Auth[\"\ud83d\udd10 CADC Authentication\"]:::auth\n    Sessions[\"\ud83d\udda5\ufe0f Session Manager&lt;br/&gt;Skaha\"]:::sessions\n\n    %% Infrastructure Layer\n    K8s[\"\u2638\ufe0f Kubernetes Cluster\"]:::k8s\n    Containers[\"\ud83d\udc33 Container Images&lt;br/&gt;Harbor Registry\"]:::containers\n    Storage[\"\ud83d\udcbe Storage Systems\"]:::storage\n\n    %% Storage Systems\n    arc[\"\ud83d\udcc1 arc POSIX Storage&lt;br/&gt;Shared Filesystem\"]:::arc\n    vault[\"\u2601\ufe0f VOSpace Object Store&lt;br/&gt;Long-term Storage\"]:::vospace\n    scratch[\"\u26a1 Scratch&lt;br/&gt;Temporary SSDs\"]:::scratch\n\n    %% Session Types\n    Types[\"Session Types\"]:::types\n    Notebook[\"\ud83d\udcd3 Jupyter Notebooks\"]:::notebooks\n    Desktop[\"\ud83d\udda5\ufe0f Desktop Environment\"]:::desktop\n    CARTA[\"\ud83d\udcca CARTA Viewer\"]:::carta\n    Firefly[\"\ud83d\udd25 Firefly Viewer\"]:::firefly\n    Contrib[\"\u2699\ufe0f Contributed Apps\"]:::contrib\n    Batch[\"\ud83c\udfed Batch Jobs\"]:::batch\n\n    %% Connections\n    User --&gt; Portal\n    Portal --&gt; Auth\n    Portal --&gt; Sessions\n\n    Auth --&gt; K8s\n    Sessions --&gt; K8s\n\n    K8s --&gt; Containers\n    K8s --&gt; Storage\n\n    Storage --&gt; arc\n    Storage --&gt; vault\n    Storage --&gt; scratch\n\n    Sessions --&gt; Types\n    Types --&gt; Notebook\n    Types --&gt; Desktop\n    Types --&gt; CARTA\n    Types --&gt; Firefly\n    Types --&gt; Contrib\n    Types --&gt; Batch\n\n    %% Styling\n    classDef user fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    classDef portal fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    classDef auth fill:#ffebee,stroke:#c62828,stroke-width:2px,color:#000\n    classDef sessions fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#000\n    classDef k8s fill:#fff3e0,stroke:#ef6c00,stroke-width:3px,color:#000\n    classDef containers fill:#f1f8e9,stroke:#558b2f,stroke-width:2px,color:#000\n    classDef storage fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000\n    classDef arc fill:#fce4ec,stroke:#ad1457,stroke-width:2px,color:#000\n    classDef vospace fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px,color:#000\n    classDef scratch fill:#fff8e1,stroke:#f57f17,stroke-width:2px,color:#000\n    classDef types fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    classDef notebooks fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    classDef desktop fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000\n    classDef carta fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    classDef firefly fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#000\n    classDef contrib fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000\n    classDef batch fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000</code></pre>"},{"location":"platform/concepts/#architecture-components","title":"Architecture Components","text":"Browser-Based Portal (<code>canfar.net</code>) Single entry point to the platform - usually no software installation required. Provides access to all CANFAR services through web interfaces. Authentication System (CADC / OIDC) Secure identity management through the Canadian Astronomy Data Centre, providing single sign-on and access control across astronomical data archives and user-created group management Container Orchestration (kubernetes) Manages computing resources automatically, handling container deployment, scaling, and resource allocation behind the scenes. Software Environments (Harbor Registry) Pre-built and customised container images with astronomy software packages, from basic Python environments to specialised tools like CASA and CARTA. Session Management (<code>skaha</code>) Orchestrates your computing sessions, connecting containers with storage systems and managing resource allocation. Storage Infrastructure Multiple storage systems optimised for different use cases - from high-performance computing to long-term archival."},{"location":"platform/concepts/#key-architectural-principles","title":"Key Architectural Principles","text":"Container-First Design All software runs in containers, ensuring consistency, reproducibility, and easy distribution of complex software environments. Kubernetes-Native Built on Kubernetes for automatic scaling, resource management, and high availability without manual intervention. Storage Separation Data persistence is handled separately from computing, allowing containers to be ephemeral while keeping your data safe. Web-Based Access Everything accessible through standard web browsers for portability and ease of installation. API-Driven All platform functions available through REST APIs, enabling automation and integration with external tools. <p>For Developers</p> <p>The platform provides REST APIs for programmatic access. See the CANFAR Python Client for automation and scripting examples.</p>"},{"location":"platform/concepts/#container-environments","title":"\ud83d\udc33 Container Environments","text":"<p>Containers are the foundation of CANFAR's flexibility and reproducibility. They provide complete, portable software environments with all astronomy tools pre-configured and ready to use.</p>"},{"location":"platform/concepts/#container-fundamentals","title":"Container Fundamentals","text":"What are Containers? Lightweight, portable packages that include an application and all its dependencies (libraries, tools, system settings) in a single, consistent environment. Why Containers for Astronomy? Solve the traditional \"dependency hell\" of astronomical software by packaging complex tool chains into reproducible, shareable environments."},{"location":"platform/concepts/#traditional-vs-container-workflows","title":"Traditional vs Container Workflows","text":"Traditional Software InstallationContainer Approach <p>Common Problems:</p> <ul> <li>Conflicting library versions and dependencies  </li> <li>Missing system requirements and packages</li> <li>Different behaviour across different machines</li> <li>Time-consuming setup and configuration</li> <li>Version compatibility issues between tools</li> <li>\"It works on my machine\" syndrome</li> </ul> <p>Solutions Provided:</p> <ul> <li>Consistent environment that works identically everywhere</li> <li>All dependencies pre-installed and tested together</li> <li>No installation or configuration required</li> <li>Easy sharing and collaboration</li> <li>Reproducible analysis results</li> <li>Instant access to complex software stacks</li> </ul>"},{"location":"platform/concepts/#popular-canfar-containers","title":"Popular CANFAR Containers","text":"Container Purpose Key Software Best For astroml General astronomy analysis scipy, astropy, matplotlib, pandas, scikit-learn, pytorch, STILTS Data analysis, visualization, ML, research improc Image processing CASUTools, SExtractor, SWarp, IRAF, STILTS Photometry, astrometry, source detection, PSF fitting casa Radio/MM astronomy CASA software suite, Python Radio astronomy, interferometry lsst LSST Analysis LSST Software Stack Image processing, LSST software stack and data access carta Data visualization CARTA viewer, analysis tools Interactive datacubes visualization <p>Container Selection</p> <p>Start with <code>astroml</code> for general astronomy work - it includes most common packages and is regularly updated with the latest astronomy and machine learning software.</p>"},{"location":"platform/concepts/#container-lifecycle-performance","title":"Container Lifecycle &amp; Performance","text":"First Launch (2-3 minutes) kubernetes downloads the container image to node-local storage. This only happens once per container type. Subsequent Launches (30-60 seconds) Fast startup using cached images. Container starts with your storage systems already connected. During Session Full access to pre-configured software environment with your data mounted and ready to use. Session End Container is destroyed, scratch is wiped out, but all data in persistent storage remains safely preserved."},{"location":"platform/concepts/#container-registry-management","title":"Container Registry &amp; Management","text":"Harbor Registry (<code>images.canfar.net</code>) Browse all available container images. Image Updates Containers should be regularly updated with latest software versions and security patches. Custom Containers Advanced users can build and maintain specialized containers for unique workflows or software requirements. <p>Reproducible Science</p> <p>Containers ensure your analysis runs identically for you, your collaborators, and future researchers. This is crucial for reproducible scientific workflows.</p> <p>Advanced Usage</p> <p>Use the CANFAR CLI to list available containers, check versions, and manage sessions programmatically.</p>"},{"location":"platform/concepts/#sessions-computing-resources","title":"\u2638\ufe0f Sessions &amp; Computing Resources","text":"<p>CANFAR uses Kubernetes to manage your computing sessions automatically. Sessions connect container environments with storage systems and provide different interfaces optimized for various workflows.</p>"},{"location":"platform/concepts/#session-fundamentals","title":"Session Fundamentals","text":"Session Lifecycle Each session creates a fresh container instance that runs until you stop it or it times out. Your data persists independently in storage systems. Resource Management Kubernetes automatically handles resource allocation, scaling, and availability without requiring infrastructure knowledge. Data Persistence Container instances are temporary and destroyed at session end, but your files persist through the storage systems."},{"location":"platform/concepts/#session-types-interfaces","title":"Session Types &amp; Interfaces","text":"<p>CANFAR provides different session types, each optimised for specific workflows:</p> \ud83d\udcd3 Notebook Sessions\ud83d\udda5\ufe0f Desktop Sessions\ud83d\udcca CARTA Sessions\ud83d\udd25 Firefly Sessions\u2699\ufe0f Contributed Sessions\ud83c\udfed Batch Sessions <p>JupyterLab Interface for interactive data science workflows</p> <p>Best For: Data exploration, visualization, prototyping, interactive analysis</p> <p>Features:</p> <ul> <li>Rich text, code, and visualization in unified interface</li> <li>Python default, and other languagecustom kernels possible</li> <li>Cell-based execution for iterative development</li> <li>Built-in file browser and terminal access</li> <li>Collaborative sharing and version control</li> </ul> <p>Linux Desktop Environment for traditional GUI applications</p> <p>Best For: CASA, DS9, TOPCAT, Aladin, traditional desktop workflows</p> <p>Features:</p> <ul> <li>Minimal Ubuntu desktop with window manager</li> <li>Multiple applications, each running on a different cluster node</li> <li>GUI-based tools and traditional software</li> <li>File managers and system utilities</li> <li>Firefox browser-on-browser</li> </ul> <p>Visualization and analysis for FITS and HDF5 astronomy data</p> <p>Best For: Radio astronomy data analysis, data cubes visualization</p> <p>Features:</p> <ul> <li>Interactive data exploration and analysis</li> <li>Optimised for large radio astronomy datasets</li> <li>Advanced visualization and measurement tools</li> <li>Browser-native interface with desktop-class performance</li> </ul> <p>Table and Image Visualization tools</p> <p>Best For: Catalogue analysis, image display, multi-wavelength data, LSST</p> <p>Features:</p> <ul> <li>Astronomical table viewing and analysis</li> <li>FITS image display and manipulation</li> <li>Cross-matching and catalog operations</li> <li>Browser-native interface for data exploration</li> <li>Integration with astronomical databases</li> </ul> <p>Community-Maintained Applications and specialised tools</p> <p>Best For: Communitity-supported web apps, Specialised workflows, experimental features, niche applications</p> <p>Features:</p> <ul> <li>Custom applications contributed by the community</li> <li>Specialised tools for specific research areas</li> <li>Experimental features and beta software</li> <li>Domain-specific analysis environments</li> <li>Research group customisations</li> </ul> <p>Automated Processing without interactive interfaces</p> <p>Best For: Large-scale processing, automated workflows, production pipelines</p> <p>Features:</p> <ul> <li>Headless execution for automated processing</li> <li>Script-based workflows and command execution</li> <li>Integration with workflow management systems</li> <li>Scalable processing for large datasets</li> <li>Programmatic job submission and monitoring</li> </ul>"},{"location":"platform/concepts/#resource-allocation-modes","title":"Resource Allocation Modes","text":"<p>CANFAR supports two resource definition approaches:</p> \ud83d\udd04 Flexible Mode (Default)\ud83c\udfaf Fixed Mode <p>Dynamic resource allocation that adapts to cluster availability</p> <p>Characteristics:</p> <ul> <li>Adaptive Usage: Can use more CPU/memory when cluster resources available</li> <li>Fast Scheduling: Sessions start quickly as they're easier to place</li> <li>Variable Performance: Performance adapts to cluster load</li> <li>Efficient Sharing: Resources shared optimally across users</li> </ul> <p>Best For: Interactive work, development, data exploration, most research workflows</p> <p>CLI Usage: <pre><code>canfar launch notebook skaha/astroml:latest  # Uses flexible mode\n</code></pre></p> <p>Guaranteed resource allocation with dedicated resources</p> <p>Characteristics:</p> <ul> <li>Predictable Performance: Consistent CPU/memory regardless of cluster load</li> <li>Resource Reservation: Resources reserved exclusively for your session</li> <li>Potential Delays: May wait for exact resources to become available</li> <li>Dedicated Resources: No sharing with other users</li> </ul> <p>Best For: Production workflows, time-sensitive analysis, performance-critical tasks</p> <p>CLI Usage: <pre><code>canfar launch notebook skaha/astroml:latest --cpu 4 --memory 8\n</code></pre></p>"},{"location":"platform/concepts/#resource-selection-guidelines","title":"Resource Selection Guidelines","text":"Workflow Type Recommended Mode Reasoning Interactive Analysis Flexible Variable resource needs, benefits from burst capacity Data Exploration Flexible Unpredictable resource patterns, fast startup preferred Production Processing Fixed Predictable performance requirements Time-Critical Analysis Fixed Deadline-driven work requiring consistent performance Large Batch Jobs Fixed Known resource requirements, consistent runtime needed Development &amp; Testing Flexible Variable needs, frequent session creation/destruction <p>Getting Started</p> <p>Start with flexible mode (the default) for most research work. Only use fixed mode when you have specific performance requirements or time constraints.</p> <p>Advanced Session Management</p> <p>Use the CANFAR CLI to monitor sessions with <code>canfar stats</code>, get detailed information with <code>canfar info</code>, and manage multiple sessions programmatically.</p>"},{"location":"platform/concepts/#storage-systems-data-management","title":"\ud83d\udcbe Storage Systems &amp; Data Management","text":"<p>CANFAR provides multiple storage systems optimised for different use cases in astronomical research. Understanding data persistence is crucial for effective platform use.</p>"},{"location":"platform/concepts/#data-persistence-fundamentals","title":"Data Persistence Fundamentals","text":"<p>Critical: Understanding Data Persistence</p> <p>Where your files are saved determines whether they survive session restarts:</p> Storage Location Persistence Purpose Performance <code>/arc/projects/[project]/</code> \u2705 Permanent, backed up Shared project data, results Network-based shared POSIX <code>/arc/home/[user]/</code> \u2705 Permanent, backed up Personal configs, scripts Network-based shared POSIX <code>vos:[user\\|project]</code> \u2705 Permanent, archived Long-term storage, sharing Network-based shared object store <code>/scratch/</code> \u274c Wiped at session end Large temporary computations Local SSD POSIX"},{"location":"platform/concepts/#arc-storage-arc-active-research-storage","title":"ARC Storage (<code>/arc/</code>) - Active Research Storage","text":"<p>High-Performance POSIX Filesystem for active research workflows:</p> <p>Key Features:</p> <ul> <li>Speed: Direct filesystem access optimised for large computations</li> <li>Collaboration: Group-based access control for team projects</li> <li>Backup: Daily snapshots for data protection</li> <li>Quotas: Managed per-project and per-user allocations</li> <li>POSIX Compliance: Standard Unix/Linux filesystem operations</li> </ul> <p>Directory Structure: <pre><code>/arc/\n\u251c\u2500\u2500 home/[user]/          # Personal user space\n\u251c\u2500\u2500 projects/[project]/   # Shared project directories\n/scratch/                 # Fast temporary storage (session-local)\n</code></pre></p> <p>Best For:</p> <ul> <li>Active data analysis and processing</li> <li>Shared datasets within research teams</li> <li>Large computational workflows requiring fast I/O</li> <li>Collaborative software development</li> </ul>"},{"location":"platform/concepts/#vault-vospace-vosuserproject-long-term-object-storage","title":"Vault VOSpace (<code>vos:[user|project]</code>) - Long-Term Object Storage","text":"<p>Vault is an IVOA-Compliant VOSpace for archival and sharing.</p> <p>Key Features:</p> <ul> <li>Standards-Based: International Virtual Observatory Alliance (IVOA) VOSpace standard</li> <li>Web Access: RESTful APIs and web interfaces</li> <li>Metadata Support: Rich astronomical metadata and annotation capabilities</li> <li>Versioning: Track changes to datasets over time</li> <li>Geo-Redundant: Multiple copies across different locations</li> <li>Access Control: Fine-grained permissions and sharing</li> </ul> <p>Access Methods: <pre><code># Command-line tools\nvcp myfile.fits vos:[user|project]/     # Copy to VOSpace\nvls vos:[user|project]/                 # List VOSpace contents\nvmkdir vos:newproject/                  # Create directories\n\n# Web interface\nhttps://www.canfar.net/storage/list\n\n# Python APIs\nimport vos\n</code></pre></p> <p>NB: <code>arc</code> is also available through the VOSpace API (<code>arc:</code>).</p> <p>Best For:</p> <ul> <li>Long-term data archival and preservation</li> <li>Sharing datasets with external collaborators</li> <li>Metadata-rich astronomical data</li> <li>Cross-institutional data exchange</li> <li>Backup copies of important results</li> </ul>"},{"location":"platform/concepts/#scratch-storage-scratch-high-performance-temporary","title":"Scratch Storage (<code>/scratch/</code>) - High-Performance Temporary","text":"<p>Fast SSD Storage for intensive computations:</p> <p>Key Features:</p> <ul> <li>Performance: Fastest available storage for I/O-intensive operations</li> <li>Temporary: Automatically cleared when sessions end</li> <li>Capacity: Up to few hundreds GBs per session for big computational jobs</li> <li>No Backup: Data is not preserved or backed up</li> </ul> <p>Best For:</p> <ul> <li>Large intermediate files during processing</li> <li>I/O-intensive computations requiring maximum speed</li> <li>Temporary datasets that don't need preservation</li> <li>Cache storage for repeated computations</li> </ul> <p>Scratch Storage Warning</p> <p>All data in <code>/scratch/</code> is permanently deleted when your session ends. Always copy important results to <code>/arc/</code> or <code>vos:</code> storage before ending sessions.</p>"},{"location":"platform/concepts/#storage-strategy-best-practices","title":"Storage Strategy &amp; Best Practices","text":"\ud83d\udcca Active Research Computing\ud83d\uddc4\ufe0f Long-Term Archival Workflow\u26a1 High-Performance Computing <p>Use <code>/arc/</code> for active work Example of structure:</p> <ol> <li>Input Data: Store working datasets in <code>/arc/projects/[project]/data/</code></li> <li>Analysis Scripts: Keep analysis code in <code>/arc/projects/[project]/scripts/</code></li> <li>Results: Save outputs to <code>/arc/projects/[project]/results/</code></li> <li>Collaboration: Share via project directory access permissions</li> </ol> <p>Use <code>vos:</code> for preservation</p> <ol> <li>Final Results: Archive completed analysis results</li> <li>Publication Data: Store data associated with published papers</li> <li>Metadata: Add rich descriptions and provenance information</li> <li>Sharing: Grant access to external collaborators</li> </ol> <p>Use <code>/scratch/</code> for intensive processing:</p> <ol> <li>Large Intermediates: Store temporary large files during processing</li> <li>Cache: Keep frequently accessed data for fast retrieval</li> <li>I/O Intensive: Use for operations requiring maximum disk speed</li> <li>Copy Results: Always copy important outputs to persistent storage</li> </ol>"},{"location":"platform/concepts/#storage-integration-automation","title":"Storage Integration &amp; Automation","text":"<p>Command-Line Tools: <pre><code># ARC storage (standard Unix commands)\ncp analysis.py /arc/projects/[project]/scripts/\nls -la /arc/home/[user]/\n\n# VOSpace operations\nvcp /arc/projects/[project]/results/ vos:[project]/analysis-v1/\nvmv vos:[user]/oldname vos:[user]/newname\n</code></pre></p> <p>Programmatic Access: <pre><code># Python integration examples\nimport vos\n\nfilename = \"myimage.fits\"\nvclient = vos.Client()\nvclient.copy(filename, 'vos:[project]/public/{filename}')\n</code></pre></p>"},{"location":"platform/concepts/#storage-quotas-management","title":"Storage Quotas &amp; Management","text":"<p>Quota Information:</p> <ul> <li>ARC Storage: Project-based quotas managed by CANFAR administrators, request increase when necessary anytime</li> <li>Vault: User and project allocations with expansion available, increase when necessary anytime</li> <li>Scratch: Per-session allocation, automatically managed</li> </ul> <p>Storage Efficiency</p> <p>Optimise your storage strategy:</p> <ul> <li>Use <code>/arc/</code> for active work requiring file system access</li> <li>Archive to <code>vos:</code> for long-term preservation and sharing</li> <li>Leverage <code>/scratch/</code> for temporary high-performance needs</li> <li>Regularly clean up unnecessary files to stay within quotas</li> </ul>"},{"location":"platform/concepts/#browser-based-access-automation","title":"\ud83c\udf10 Browser-Based Access &amp; Automation","text":"<p>CANFAR provides comprehensive browser-based access to all platform features, eliminating the need for local software installation while supporting advanced automation workflows.</p>"},{"location":"platform/concepts/#web-based-computing","title":"Web-Based Computing","text":"Science Portal (<code>canfar.net</code>) Complete platform access through standard web browsers - no plugins or software installation required. Session Interfaces All session types accessible through web interfaces, from Jupyter notebooks to full desktop environments delivered via browser. Data Management Web-based file browsers, transfer tools, and storage management interfaces integrated into the portal."},{"location":"platform/concepts/#programmatic-platform-access","title":"Programmatic Platform Access","text":"<p>CANFAR provides REST APIs for programmatic access, enabling automation and integration with external tools:</p> CANFAR Python Client Comprehensive Python library for session management, data operations, and workflow automation. See the CANFAR Python Client documentation. VOSpace API IVOA-standard APIs for programmatic data storage operations and metadata management. Authentication APIs CADC integration providing secure programmatic access to platform resources and astronomical data archives."},{"location":"platform/concepts/#key-api-services","title":"Key API Services","text":"Service Purpose Documentation Session Management Launch, monitor, and control computing sessions Python Client VOSpace Operations File transfer, storage, and metadata operations VOSpace API Access Control Authentication and authorization management CADC Services"},{"location":"platform/concepts/#automation-examples","title":"Automation Examples","text":"<p>Session Automation:</p> <pre><code># Launch and manage sessions programmatically\nfrom canfar.client import Session\n\nsession = Session()\njob = session.create('notebook', 'skaha/astroml:latest')\nsession.monitor(job.id)\n</code></pre> <p>Data Workflow Automation:</p> <pre><code># Automated data processing pipeline\nfrom canfar.storage import VOSpace\n\nvos = VOSpace()\nvos.upload('/local/data', 'vos:project/input/')\n# ... run analysis session ...\nvos.download('vos:project/results/', '/local/results/')\n</code></pre> <p>Integration Options</p> <p>External Workflow Integration:</p> <ul> <li>GitHub Actions: Automate CANFAR workflows from code repositories</li> <li>Jupyter Notebooks: Embed CANFAR operations in interactive analysis</li> <li>CI/CD Pipelines: Include CANFAR processing in continuous integration</li> <li>Custom Applications: Build specialised tools using CANFAR APIs</li> </ul>"},{"location":"platform/concepts/#platform-integration-next-steps","title":"\ud83d\udd17 Platform Integration &amp; Next Steps","text":""},{"location":"platform/concepts/#understanding-platform-connections","title":"Understanding Platform Connections","text":"<p>CANFAR integrates with the broader astronomical ecosystem through standards-based interfaces and established protocols:</p> Data Archive Integration Direct access to CADC and international observatory data archives through authenticated connections. VO Standards Compliance IVOA-compliant services enabling interoperability with other Virtual Observatory tools and services. Collaborative Networks Integration with academic institutions, research networks, and international astronomical organizations."},{"location":"platform/concepts/#recommended-learning-path","title":"Recommended Learning Path","text":"<p>Now that you understand CANFAR's core concepts, explore specific platform areas:</p> <ol> <li>Get Started Guide - Hands-on tutorials and first steps</li> <li>Permissions &amp; Access - User management and collaboration</li> <li>Storage Systems - Master data management workflows  </li> <li>Container Environments - Work with software environments</li> <li>Interactive Sessions - Start analyzing data</li> <li>Legacy Cloud Platform - Understanding legacy VM infrastructure</li> </ol>"},{"location":"platform/concepts/#advanced-platform-usage","title":"Advanced Platform Usage","text":"<p>For Power Users:</p> <ul> <li>CANFAR CLI - Command-line tools for platform automation</li> <li>Python Client - Programmatic access and workflow development</li> <li>Container Building - Create custom software environments</li> <li>Batch Processing - Large-scale automated workflows</li> </ul> <p>For Administrators:</p> <ul> <li>Project Management - Managing research teams and resources</li> <li>Resource Allocation - Understanding quotas and limits</li> <li>Access Control - Fine-grained permission management</li> </ul> <p>Key Platform Concepts</p> <p>CANFAR provides the computing power of a research institution without the infrastructure overhead.</p> <p>Core Principles:</p> <ul> <li>Container-first: All software runs in reproducible, portable environments</li> <li>Browser-based: Complete workflows accessible through web interfaces  </li> <li>Storage-centric: Data persistence separate from computing resources</li> <li>Kubernetes-native: Automatic resource management and scaling</li> <li>API-driven: Full platform functionality available programmatically</li> </ul> <p>Focus on your science - let CANFAR handle the infrastructure, software, and data management.</p>"},{"location":"platform/doi/","title":"Data Publication Service (DOIs)","text":"<p>CANFAR's Data Publication Service (DPS) provides permanent Digital Object Identifiers (DOIs) for research data packages, ensuring long-term accessibility and proper citation of datasets supporting astronomical publications.</p> <p>\ud83c\udfaf DOI Service Overview</p> <p>Essential data publication workflows:</p> <ul> <li>DOI Registration: Reserve permanent identifiers through DataCite</li> <li>Data Packaging: Organise and upload research datasets</li> <li>Referee Access: Provide controlled access during peer review</li> <li>Publication: Mint final DOIs with locked data directories</li> <li>Long-term Preservation: Ensure data accessibility and citation</li> </ul>"},{"location":"platform/doi/#service-purpose-access","title":"\ud83d\ude80 Service Purpose &amp; Access","text":""},{"location":"platform/doi/#data-publication-service-overview","title":"Data Publication Service Overview","text":"<p>The CANFAR Data Publication Service (DPS) creates permanent links between research papers and their supporting data packages. DPS provides:</p> <ul> <li>Permanent Storage: Reliable hosting for research data packages</li> <li>DOI Registration: Official Digital Object Identifiers through DataCite</li> <li>Landing Pages: Professional presentation of datasets and metadata</li> <li>Citation Support: Proper attribution for data reuse and collaboration</li> </ul>"},{"location":"platform/doi/#access-points","title":"Access Points","text":"Web Interface CANFAR Science Portal \u2192 Data Publication Direct Service Data Publication Service Account Requirements First author requires a CADC account for DPS access and VOSpace data management."},{"location":"platform/doi/#doi-workflow-guide","title":"\ud83d\udccb DOI Workflow Guide","text":""},{"location":"platform/doi/#step-1-request-a-doi","title":"Step 1: Request a DOI","text":"<p>Reserve Your DOI:</p> <ul> <li>A permanent DOI is assigned to your data package (e.g., 10.11570/20.0006)</li> <li>A dedicated Data Directory (VOSpace) is created</li> <li>A professional landing page is generated</li> </ul>"},{"location":"platform/doi/#step-2-upload-data-package","title":"Step 2: Upload Data Package","text":"<p>Choose Upload Method Based on Data Size:</p> Small/Few Files: Use the Web Storage UI for direct browser upload Large/Many Files: Use <code>vos</code> CLI tools for efficient bulk transfer <p>See Data Package Guidelines for content organization recommendations.</p>"},{"location":"platform/doi/#step-3-referee-access-optional","title":"Step 3: Referee Access (Optional)","text":"<p>Peer Review Support:</p> <p>CADC can create read-only accounts for editors/referees to access your data directory during the review process. The temporary account is disabled after review completion.</p>"},{"location":"platform/doi/#step-4-publish-with-datacite","title":"Step 4: Publish with DataCite","text":"<p>Final Publication:</p> <p>Click Publish in the DPS interface to:</p> <ul> <li>Complete DOI registration with DataCite</li> <li>Lock the data directory (preventing further changes)</li> <li>Make the landing page publicly accessible</li> </ul> <p>Important: Publication Locks Data</p> <p>After publishing, the data directory becomes read-only. Metadata changes require contacting CANFAR support.</p>"},{"location":"platform/doi/#using-the-data-publication-service","title":"\ud83d\udd27 Using the Data Publication Service","text":""},{"location":"platform/doi/#managing-your-dois","title":"Managing Your DOIs","text":"DOI Dashboard: The DPS interface displays all your DOIs with status, title, landing page links, and data directory access. Creating New DOIs: Use New from the dashboard or go directly to the request page."},{"location":"platform/doi/#doi-request-requirements","title":"DOI Request Requirements","text":"<p>Required Information:</p> <ul> <li>First Author: Primary researcher responsible for the data package</li> <li>Title: Descriptive title for the dataset</li> </ul> <p>Optional Information (editable later):</p> <ul> <li>Additional Authors: Contributing researchers</li> <li>Journal Reference: Journal name, volume, page numbers</li> <li>Publication Details: Can be added after manuscript acceptance</li> </ul>"},{"location":"platform/doi/#doi-management-interface","title":"DOI Management Interface","text":"<p>DOI Details Page (e.g., DOI.20.0016):</p> <ul> <li>DOI reference number and dataset title</li> <li>Author list and journal reference information  </li> <li>Current publication status</li> <li>Direct links to landing page and data directory</li> <li>Lock status indicator for published DOIs</li> </ul> <p>Editing Capabilities:</p> <ul> <li>Unpublished DOIs: Full editing access via Update button</li> <li>Published DOIs: Changes require CANFAR support request</li> </ul> <p>Landing Page Access:</p> <ul> <li>DOI Link: 10.11570/20.0016 (permanent identifier)</li> <li>Landing Page: Direct access (publicly accessible after publication)</li> </ul> <p>DOI Lifecycle Management:</p> <ul> <li>Unpublished: Can be edited or deleted by the author</li> <li>Published: Permanent and locked, requires support for modifications</li> </ul>"},{"location":"platform/doi/#data-package-requirements","title":"\ud83d\udce6 Data Package Requirements","text":""},{"location":"platform/doi/#storage-implementation","title":"Storage Implementation","text":"VOSpace Data Directory: Each DOI receives a dedicated folder in the CANFAR Vault (VOSpace) with a <code>data/</code> subdirectory under your control. Example Structure: Data Directory Example"},{"location":"platform/doi/#content-organization","title":"Content Organization","text":"<p>Recommended Package Contents:</p> <ul> <li>Primary Data: Core datasets supporting the research</li> <li>Analysis Code: Scripts and software used in data processing</li> <li>Documentation: README files describing structure and usage</li> <li>Supplementary Materials: Figures, tables, additional analysis outputs</li> </ul> <p>Best Practices:</p> <ul> <li>Include a top-level README describing package layout and usage instructions</li> <li>Organize files in logical subdirectories (e.g., <code>raw_data/</code>, <code>processed/</code>, <code>scripts/</code>, <code>figures/</code>)</li> <li>Use descriptive filenames and provide metadata where appropriate</li> </ul>"},{"location":"platform/doi/#upload-methods","title":"Upload Methods","text":"Web Interface Upload: Web Storage UI for small datasets and simple uploads Command-Line Upload: <code>vcp</code> and <code>vos</code> CLI tools for large datasets and automated transfers"},{"location":"platform/doi/#publication-access-control","title":"Publication &amp; Access Control","text":"<p>Pre-Publication (Referee Access):</p> <ul> <li>Contact CANFAR support for read-only reviewer accounts</li> <li>Temporary access provided during peer review process</li> <li>Reviewers may request changes before publication approval</li> </ul> <p>Post-Publication (Public Access):</p> <ul> <li>Publish button mints the final DOI and locks data directory</li> <li>Landing page becomes publicly discoverable through DataCite search</li> <li>Minimal discovery metadata appears in DataCite registry</li> </ul>"},{"location":"platform/doi/#final-publication-integration","title":"Final Publication Integration","text":"<p>Linking DOIs:</p> <p>After manuscript acceptance, coordinate the connection between your data package DOI and journal publication DOI:</p> <ol> <li>Notify Journal: Provide your data package DOI for inclusion in the published paper</li> <li>Update Metadata: Email CANFAR support with:</li> <li>Publication DOI from the journal</li> <li>Updated reference details (journal, volume, pages)</li> <li>Any additional metadata corrections</li> </ol> <p>Data Package Success</p> <p>Plan your data package early in the research process to ensure all necessary files, documentation, and metadata are preserved and organized for publication.</p>"},{"location":"platform/doi/#using-the-dps","title":"Using the DPS","text":""},{"location":"platform/doi/#listing-current-dois","title":"Listing current DOIs","text":"<p>DPS shows your DOIs (status, title, landing page, data directory). From here, you can request, view, edit, or publish depending on status.</p>"},{"location":"platform/doi/#requesting-a-new-doi","title":"Requesting a new DOI","text":"<p>Use New from the list or go to the request page.</p> <p>Required</p> <ul> <li>First Author</li> <li>Title</li> </ul> <p>Optional (can be edited later)</p> <ul> <li>Journal reference (journal, volume, page)</li> <li>Additional Authors</li> </ul> <p>After submission, a DOI Reference number is assigned and displayed.</p>"},{"location":"platform/doi/#doi-details","title":"DOI Details","text":"<p>On the details page (e.g., DOI.20.0016) you'll find:</p> <ul> <li>DOI number / Title</li> <li>Authors / Journal reference</li> <li>DOI status</li> <li>Landing page link</li> <li>Data Directory link (shows \ud83d\udd12 when frozen)</li> </ul>"},{"location":"platform/doi/#editing-details","title":"Editing details","text":"<ul> <li>Unpublished DOIs can be edited by authenticated users; click Update.</li> <li>Published DOIs require a request to CANFAR support.</li> </ul>"},{"location":"platform/doi/#viewing-the-landing-page","title":"Viewing the landing page","text":"<ul> <li>DOI: 10.11570/20.0016</li> <li>Landing page: landing page</li> </ul> <p>Published landing pages are publicly accessible.</p>"},{"location":"platform/doi/#publishing-a-doi","title":"Publishing a DOI","text":"<p>If not yet published, a Publish button appears at the top right. Publishing:</p> <ul> <li>Completes registration with DataCite</li> <li>Locks the Data Directory</li> </ul> <p>Related publication info can be added later via support.</p>"},{"location":"platform/doi/#deleting-unpublished-dois","title":"Deleting unpublished DOIs","text":"<p>Unpublished records can be deleted via Delete on the request page. Published DOIs cannot be deleted.</p>"},{"location":"platform/doi/#doi-data-package","title":"DOI Data Package","text":"<p>DPS hosts a Data Directory in the Vault (VOSpace) implementation for each DOI. A folder named <code>data/</code> is created under the DOI root; you control the structure beneath it.</p> <p>Example: Data Directory</p> <p>Locked after publish</p> <p>After publishing, the directory is locked. To modify contents or metadata, contact CANFAR support.</p>"},{"location":"platform/doi/#contents","title":"Contents","text":"<p>You decide what to include: data, figures, software, etc. We recommend a top\u2011level <code>README</code> describing layout and usage.</p>"},{"location":"platform/doi/#uploading","title":"Uploading","text":"<ul> <li>Few/small files: Web Storage UI.</li> <li>Large/many files: Use <code>vcp</code>, <code>vos</code> CLI Tools.</li> </ul>"},{"location":"platform/doi/#refereeing-access","title":"Refereeing access","text":"<p>Contact support to obtain a read\u2011only account and share with the editor/referee. They may request changes prior to publication.</p>"},{"location":"platform/doi/#publish-discoverability","title":"Publish &amp; discoverability","text":"<p>After acceptance, click Publish to mint the DOI. The directory and metadata freeze; minimal discovery metadata will appear in DataCite search.</p>"},{"location":"platform/doi/#final-linking","title":"Final linking","text":"<p>Finally, link the data package DOI to the journal DOI (currently manual):</p> <ul> <li>Email support with the publication DOI and updated reference details.</li> <li>Provide the data package DOI to the journal so it appears in the paper.</li> </ul>"},{"location":"platform/get-started/","title":"\ud83d\ude80 Getting Started with CANFAR","text":"<p>Guide to setting up and using the CANFAR Science Platform for astronomical research.</p> <p>Essential Resources:</p> <ul> <li>Permissions Guide: Account set-up and group management</li> <li>Sessions Overview: Interactive computing environments</li> <li>Storage Guide: Data management and file systems</li> <li>Container Guide: Software environments and registries</li> <li>Support Centre: Help resources and FAQ</li> </ul>"},{"location":"platform/get-started/#1-get-your-cadc-account","title":"1\ufe0f\u20e3 Get Your CADC Account","text":"<p>If you are a first-time user, request a Canadian Astronomy Data Centre (CADC) account:</p> <p>\ud83d\udd17 Request CADC Account</p> <p>See Accounts &amp; Permissions for more details.</p> <p>Account Processing Time</p> <p>CADC accounts are typically approved within 1\u20132 business days. For troubleshooting account issues, see FAQ or Contact Support.</p>"},{"location":"platform/get-started/#2-join-or-create-your-research-group","title":"2\ufe0f\u20e3 Join or Create Your Research Group","text":"<p>Once you have a CADC account:</p> Joining an Existing GroupNew Collaboration <p>Ask your collaboration administrator to add you via the CADC Group Management Interface.</p> <p>Email support@canfar.net with:</p> <ul> <li>Your project description</li> <li>Expected team size</li> <li>Storage requirements</li> <li>Timeline</li> </ul> <p>See Permissions Guide for group management details. For advanced collaboration, see Storage Guide.</p>"},{"location":"platform/get-started/#3-first-login-and-set-up","title":"3\ufe0f\u20e3 First Login and Set-up","text":"<ol> <li>Login to canfar.net with your CADC credentials.</li> <li>Accept Terms of Service to complete set-up.</li> <li>Optional (for private containers): Access the Image Registry</li> </ol> <p>See Container Guide for more about images and custom software. For building your own containers, see Building Containers.</p>"},{"location":"platform/get-started/#4-launch-your-first-session","title":"4\ufe0f\u20e3 Launch Your First Session","text":"<p>To start analyzing data, launch a Jupyter notebook:</p> <ol> <li>Click Science Portal from the main menu.</li> <li>Use the default settings.</li> <li>Click Launch.</li> <li>Wait about 30 seconds, then open your session.</li> </ol> <p>\ud83c\udf89 You're ready to go! Your session includes Python, common astronomy packages, and access to shared storage.</p> <p>See Sessions Overview for more session types and workflows. For automation, see CANFAR Python Client.</p> <p>Recommended Starting Point</p> <p>Start with the default <code>astroml</code> container \u2013 it includes most common astronomy packages and is regularly updated.</p> <p>Advanced: Custom Containers</p> <ul> <li>Build your own containers for specialised workflows. See Building Containers.</li> <li>Use Harbor Registry to browse and manage images.</li> </ul>"},{"location":"platform/get-started/#understanding-your-workspace","title":"\ud83d\udcc1 Understanding Your Workspace","text":"<p>See the Storage Guide for full details. For VOSpace scripting, see VOSpace API.</p> Location Purpose Persistence Best For <code>/arc/projects/[project]/</code> Shared research data \u2705 Permanent, backed up Datasets, results, shared code <code>/arc/home/[user]/</code> Personal files \u2705 Permanent, backed up Personal configs, small files <code>/scratch/</code> Fast temporary space \u274c Wiped at session end Large computations, temporary files"},{"location":"platform/get-started/#collaboration-features","title":"\ud83e\udd1d Collaboration Features","text":"<p>See Permissions Guide and Storage Guide for collaboration details. For team onboarding, see Getting Started.</p>"},{"location":"platform/get-started/#storage-sharing","title":"Storage Sharing","text":"<p>All group members have access to <code>/arc/projects/[project]/</code> \u2013 perfect for:</p> <ul> <li>Sharing datasets and results</li> <li>Collaborative analysis scripts</li> <li>Common software environments</li> <li>Project documentation</li> </ul>"},{"location":"platform/get-started/#need-help","title":"\ud83d\udcac Need Help?","text":"<ul> <li>\ud83d\udcac Discord Community \u2013 Chat with other users</li> <li>\ud83c\udd98 Support Centre \u2013 Help resources and contact information</li> </ul>"},{"location":"platform/permissions/","title":"User Management &amp; Permissions","text":"<p>Accounts, groups, access control, and API authentication on the CANFAR platform for collaborative astronomical research.</p> <p>\ud83c\udfaf Permission System Overview</p> <p>Essential access control concepts for all users:</p> <ul> <li>Account Management: CADC identity and authentication systems</li> <li>Group Collaboration: Team-based resource sharing and project management</li> <li>Access Control Lists: Fine-grained file and directory permissions</li> <li>Container Registry: Software environment access and distribution</li> <li>API Authentication: Programmatic access and automation</li> </ul>"},{"location":"platform/permissions/#canfar-permission-architecture","title":"\ud83d\udd10 CANFAR Permission Architecture","text":"<p>CANFAR's security model consists of multiple integrated layers providing flexible, secure access control for astronomical research collaboration.</p>"},{"location":"platform/permissions/#authentication-authorisation-layers","title":"Authentication &amp; Authorisation Layers","text":"CADC Identity System Your foundational identity for all Canadian astronomy services, providing single sign-on across CANFAR, data archives, and VO services. Group-Based Collaboration Teams and projects organized through hierarchical group membership with shared resource access and management capabilities. Harbor Container Registry Software environment access control determining who can access, modify, and distribute container images. Access Control Lists (ACLs) Fine-grained POSIX-extended permissions for precise file and directory access control on shared storage systems. API Authentication Framework Secure programmatic access enabling automation, integration, and custom application development."},{"location":"platform/permissions/#permission-model-benefits","title":"Permission Model Benefits","text":"Individual ResearchersResearch TeamsSystem Administrators <ul> <li>Single Identity: One CADC account for all astronomical services</li> <li>Self-Service: Manage personal permissions and group memberships</li> <li>Secure Access: Multi-factor authentication and token-based API access</li> <li>Data Protection: Granular control over personal and shared data</li> </ul> <ul> <li>Collaborative Workspaces: Shared storage, containers, and computing resources</li> <li>Role-Based Access: Flexible administrator and member roles</li> <li>Project Isolation: Security boundaries between different research projects</li> <li>External Collaboration: Controlled access for external partners and institutions</li> </ul> <ul> <li>Centralized Management: Unified interface for user and resource administration</li> <li>Audit Capabilities: Comprehensive logging and permission tracking</li> <li>Scalable Security: Supports large multi-institutional collaborations</li> <li>Automated Workflows: API-driven permission management and integration</li> </ul>"},{"location":"platform/permissions/#group-management-collaboration","title":"\ud83d\udc65 Group Management &amp; Collaboration","text":"<p>Groups form the foundation of collaborative research on CANFAR, providing shared access to computing resources, storage systems, and container environments while maintaining security boundaries between projects.</p>"},{"location":"platform/permissions/#group-based-resource-sharing","title":"Group-Based Resource Sharing","text":"<pre><code>graph TD\n    Admin[\"\ud83d\udc51 Group Administrator\"]\n    Members[\"\ud83d\udc64 Group Members\"]\n    Resources[\"\ud83d\udcbe Shared Resources\"]\n\n    Admin --&gt; |\"Manages\"| Members\n    Admin --&gt; |\"Controls access to\"| Resources\n    Members --&gt; |\"Access\"| Resources\n\n    Resources --&gt; Projects[\"\ud83d\udcc1 /arc/projects/[project]/\"]\n    Resources --&gt; Storage[\"\ud83d\udcbe Storage Quotas\"]\n    Resources --&gt; Containers[\"\ud83d\udc33 Container Access\"]</code></pre>"},{"location":"platform/permissions/#group-administration-interface","title":"Group Administration Interface","text":"Access Group Management: CADC Group Management Portal"},{"location":"platform/permissions/#creating-research-groups","title":"Creating Research Groups","text":"<p>Step 1: Create New Group</p> <ol> <li>Navigate to the CADC Group Management portal</li> <li>Click \"New Group\" </li> <li>Provide descriptive group name (e.g., <code>cfhtls-survey</code>, <code>exoplanet-collab</code>)</li> <li>Add comprehensive project description</li> <li>Click Create to establish the group</li> </ol> <p>Step 2: Add Team Members</p> <ol> <li>Locate your group in the management interface</li> <li>Click \"Edit\" in the Membership column</li> <li>Search by real names (not CADC usernames)</li> <li>Select appropriate users from search results</li> <li>Click \"Add member\" to grant access</li> </ol> <p>Step 3: Assign Administrative Roles</p> <ol> <li>Click \"Edit\" in the Administrators column</li> <li>Add users requiring group management capabilities</li> <li>Administrators gain full group control and resource allocation rights</li> </ol> <p>User Discovery</p> <p>Search by full names (e.g., \"John Smith\") rather than CADC usernames. The system will find users and display their associated usernames.</p>"},{"location":"platform/permissions/#group-role-hierarchy","title":"Group Role Hierarchy","text":"Role Access Level Responsibilities Best For Administrator Full group management, resource allocation, member control Group creation, permission management, resource requests Project PIs, team leads, institutional coordinators Member Shared resource access, collaboration capabilities Data analysis, research participation, resource usage Researchers, students, collaborators, external partners"},{"location":"platform/permissions/#group-resource-access","title":"Group Resource Access","text":"Shared Storage Access: Groups automatically receive shared directories in <code>/arc/projects/[project]/</code> with managed quotas and backup policies. Container Image Sharing: Group-specific namespaces in Harbor container registry for sharing custom software environments. Computing Resource Allocation: Shared computational quotas and session management across group members. Collaborative Session Management: Ability to share and handoff interactive sessions between group members."},{"location":"platform/permissions/#multi-institutional-collaboration","title":"Multi-Institutional Collaboration","text":"External User Integration: Add researchers from other institutions to your CANFAR groups while maintaining institutional security boundaries. Cross-Project Permissions: Users can belong to multiple groups, enabling interdisciplinary collaboration and resource sharing. Temporary Access: Grant time-limited access for visiting researchers, students, or short-term collaborations. <p>Collaboration Benefits</p> <p>Groups enable seamless research collaboration by providing standardized environments, shared data access, and unified resource management across institutional boundaries.</p>"},{"location":"platform/permissions/#container-registry-access-harbor","title":"\ud83d\udc33 Container Registry Access (Harbor)","text":"<p>Harbor serves as CANFAR's container registry for storing, managing, and distributing software environments. Understanding Harbor permissions is essential for teams building custom containers or managing specialized software stacks.</p>"},{"location":"platform/permissions/#registry-overview","title":"Registry Overview","text":"Harbor Registry Access: https://images.canfar.net Purpose: Centralized repository for container images with role-based access control, vulnerability scanning, and automated build integration."},{"location":"platform/permissions/#harbor-permission-levels","title":"Harbor Permission Levels","text":"Role Repository Access Image Management Project Control Guest Pull public images only View public metadata Browse public projects Developer Pull all group images, push to assigned repositories Upload, tag, and delete own images View project configurations Master Full repository access within project Complete image lifecycle management Project settings, user management"},{"location":"platform/permissions/#harbor-access-management","title":"Harbor Access Management","text":"Permission Requests: Harbor permissions are managed by CANFAR administrators. Contact support@canfar.net for: <ul> <li>Repository Access: Request developer or master access to existing projects</li> <li>New Projects: Set up dedicated projects for your research group</li> <li>Team Management: Add or modify permissions for team members</li> <li>Repository Configuration: Set up automated builds and integration workflows</li> </ul>"},{"location":"platform/permissions/#working-with-harbor","title":"Working with Harbor","text":"<p>Authentication:</p> <pre><code># Login to Harbor registry\ndocker login images.canfar.net\n</code></pre> <p>Pulling Images:</p> <pre><code># Pull public container images\ndocker pull images.canfar.net/skaha/astroml:latest\n\n# Pull private group images (requires permissions)\ndocker pull images.canfar.net/[project]/[container]:[tag]\n</code></pre> <p>Pushing Images (Developer/Master roles):</p> <pre><code># Build and tag your container\ndocker build -t images.canfar.net/[project]/[container]:[tag] .\n\n# Push to project repository\ndocker push images.canfar.net/[project]/[container]:[tag]\n</code></pre>"},{"location":"platform/permissions/#project-organization","title":"Project Organization","text":"Public Projects: CANFAR-maintained base images available to all users (e.g., <code>skaha/astroml</code>) Group Projects: Private repositories for research teams with controlled access and custom software environments Personal Projects: Individual user spaces for development and testing before team integration <p>Container Strategy</p> <p>Start with public base images and extend them for your specific needs. Request team projects for sharing custom environments across your research group.</p>"},{"location":"platform/permissions/#access-control-lists-acls","title":"\ud83d\udee1\ufe0f Access Control Lists (ACLs)","text":"<p>Access Control Lists provide fine-grained file and directory permissions beyond traditional POSIX capabilities, enabling flexible collaboration across research teams while maintaining security boundaries.</p>"},{"location":"platform/permissions/#acl-fundamentals","title":"ACL Fundamentals","text":"What ACLs Provide: Extended POSIX permissions allowing multiple users and groups to have different access levels to the same files and directories. Why ACLs Matter for Research: Enable complex collaborative scenarios where traditional owner/group/other permissions are insufficient for multi-institutional projects."},{"location":"platform/permissions/#traditional-posix-vs-acl-permissions","title":"Traditional POSIX vs ACL Permissions","text":"POSIX LimitationsACL Advantages <p>Traditional Issues:</p> <ul> <li>Only one group can own a file or directory</li> <li>No granular control for multiple collaborating groups</li> <li>Sharing across research teams requires complex workarounds</li> <li>Binary all-or-nothing access for group members</li> </ul> <p>Extended Capabilities:</p> <ul> <li>Multiple users and groups with different permissions per file</li> <li>Granular read/write access for specific researchers</li> <li>Selective collaboration without compromising security</li> <li>Fine-tuned access for external partners and institutions</li> </ul>"},{"location":"platform/permissions/#acl-vs-posix-comparison","title":"ACL vs POSIX Comparison","text":"Collaboration Scenario POSIX Solution ACL Solution Single Team Project <code>rwxrwx---</code> (group access) Same as POSIX, no advantage Multi-Group Collaboration Must choose one primary group Grant specific access to multiple groups External Researcher Access Add to group or make world-readable Grant individual read access only Selective Write Permissions All group members get write access Grant write access to specific users only Cross-Institutional Sharing Complex group management Flexible user and group combinations"},{"location":"platform/permissions/#viewing-acl-permissions","title":"Viewing ACL Permissions","text":"<p>Check Current ACLs:</p> <pre><code># View detailed ACL information for files or directories\ngetfacl /arc/projects/[project]/[directory]/\n</code></pre> <p>Example ACL Output:</p> <pre><code># file: sensitive_data/\n# owner: alice\n# group: myproject-team\nuser::rwx                    # Owner permissions\nuser:bob:r--                 # Bob has read-only access\nuser:carol:rw-               # Carol can read and write  \ngroup::r--                   # Primary group has read-only\ngroup:external-team:r--      # External group has read access\nmask::rwx                    # Maximum effective permissions\nother::---                   # No access for others\n</code></pre> <p>ACL Mask Behavior</p> <p>The ACL \"mask\" entry limits maximum effective permissions for named users and groups. If permissions seem restricted, check the mask value.</p>"},{"location":"platform/permissions/#setting-and-managing-acls","title":"Setting and Managing ACLs","text":"<p>Grant User Access:</p> <pre><code># Give user 'bob' read access to a directory\nsetfacl -m u:bob:r-- /arc/projects/[project]/shared_data/\n\n# Grant user 'alice' read and write access to specific files\nsetfacl -m u:alice:rw- /arc/projects/[project]/scripts/analysis.py\n</code></pre> <p>Grant Group Access:</p> <pre><code># Allow external group read access to results\nsetfacl -m g:external-collab:r-- /arc/projects/[project]/public_results/\n\n# Grant write access to multiple collaborating groups\nsetfacl -m g:partner-institution:rw- /arc/projects/[project]/shared_analysis/\n</code></pre> <p>Remove ACL Entries:</p> <pre><code># Remove specific user access\nsetfacl -x u:bob /arc/projects/[project]/sensitive_data/\n\n# Remove all ACL entries (revert to POSIX only)\nsetfacl -b /arc/projects/[project]/temp_data/\n</code></pre> <p>Recursive Operations:</p> <pre><code># Apply ACLs to entire directory trees\nsetfacl -R -m g:collaborators:r-- /arc/projects/[project]/results/\n</code></pre> <p>Recommended Directory Structure:</p> <pre><code>/arc/projects/[project]/\n\u251c\u2500\u2500 public/              # World-readable results\n\u2502   \u2514\u2500\u2500 (ACL: group:world:r--)\n\u251c\u2500\u2500 team/                # Full team access  \n\u2502   \u2514\u2500\u2500 (ACL: group:myproject-team:rw-)\n\u251c\u2500\u2500 admin/               # Administrator-only access\n\u2502   \u2514\u2500\u2500 (ACL: user:pi:rw-, group:admins:rw-)\n\u251c\u2500\u2500 external/            # Controlled external collaboration\n\u2502   \u2514\u2500\u2500 (ACL: user:collaborator:r--, group:external-team:r--)\n\u2514\u2500\u2500 sensitive/           # Restricted access with specific permissions\n    \u2514\u2500\u2500 (ACL: user:analyst1:rw-, user:analyst2:r--)\n</code></pre> <p>Security Best Practices:</p> <ul> <li>Principle of Least Privilege: Grant minimum access required for each user or group</li> <li>Regular Audits: Review ACLs periodically using <code>getfacl</code> to ensure appropriate access</li> <li>Documentation: Maintain records of why specific ACLs were set and who requested them</li> <li>Group Preference: Use group-based permissions when possible for easier management</li> <li>Inheritance Planning: Set default ACLs on directories to automatically apply to new files</li> </ul> <p>ACL Troubleshooting:</p> <p>If ACL changes don't take effect as expected:</p> <ol> <li>Check the ACL mask: <code>getfacl filename</code> and verify mask entry</li> <li>Update mask if needed: <code>setfacl -m m::rwx filename</code> </li> <li>Set default ACLs for directories: <code>setfacl -d -m g:groupname:rw directory/</code></li> <li>Verify group membership: Ensure users belong to specified groups</li> </ol> <p>Collaboration Success</p> <p>ACLs enable sophisticated research collaboration across institutional boundaries while maintaining data security and access control granularity.</p>"},{"location":"platform/permissions/#api-authentication-programmatic-access","title":"\ud83d\udd0c API Authentication &amp; Programmatic Access","text":"<p>CANFAR provides comprehensive REST APIs enabling automation, integration, and custom application development. Understanding authentication methods is essential for programmatic platform usage.</p>"},{"location":"platform/permissions/#authentication-framework","title":"Authentication Framework","text":"API Access Purpose: Enable automation, workflow integration, and custom tool development using CANFAR platform capabilities. Authentication Requirements: All API calls require proper authentication tokens or certificates for secure access to platform resources."},{"location":"platform/permissions/#authentication-methods","title":"Authentication Methods","text":"\ud83d\udd27 CANFAR CLI (Recommended)\ud83d\udd12 Proxy Certificates <p>Best for: Interactive use, development, short-term automation</p> <p>Setup:</p> <pre><code># Login and store authentication token\ncanfar auth login\n\n# Subsequent commands use stored credentials\ncanfar ps\ncanfar launch notebook skaha/astroml:latest\ncanfar info [session-id]\n</code></pre> <p>Benefits: - Easy setup and token management - Automatic token refresh handling - Integrated with all CANFAR platform services - Ideal for development and testing workflows</p> <p>Best for: Long-term automation, production scripts, file operations</p> <p>Setup: <pre><code># Install CADC utilities\npip install cadcutils\n\n# Generate 10-day proxy certificate  \ncadc-get-cert -u [username]\n\n# Certificate stored in ~/.ssl/cadcproxy.pem\n# Automatically used by CADC tools and APIs\n</code></pre></p> <p>Benefits: - Extended validity (10 days) - Compatible with all CADC services - Suitable for production automation - Works with VOSpace and data archive APIs</p>"},{"location":"platform/permissions/#api-integration-examples","title":"API Integration Examples","text":"<p>Session Management:</p> <pre><code>from canfar.sessions import Session\n\n# Start authenticated session client\nsession = Session()\n\n# Launch computing sessions programmatically\njob = session.launch('notebook', 'skaha/astroml:latest')\n\n# Monitor session status\nstatus = session.info(job.id)\n\n# List all active sessions\nactive_sessions = session.list()\n</code></pre> <p>Batch Processing Integration:</p> <pre><code># Automated pipeline example\nfrom canfar.sessions import Session\n\n# Submit batch job with custom parameters\nsession = Session()\njob = canfar.launch(\n    image='skaha/astroml:latest',\n    command=['python', 'analysis.py'],\n    cpu=4,\n    memory=8\n)\n</code></pre>"},{"location":"platform/permissions/#common-issues-troubleshooting","title":"\ud83d\udea8 Common Issues &amp; Troubleshooting","text":"<p>Understanding and resolving common permission issues helps maintain smooth collaborative workflows.</p>"},{"location":"platform/permissions/#permission-denied-accessing-arcprojectsproject","title":"Permission Denied Accessing <code>/arc/projects/[project]</code>","text":"<p>Symptoms: Cannot access shared project directories</p> <p>Common Causes: - Not a member of the required project group - Group membership not yet propagated to storage systems - Incorrect project name or path</p> <p>Solutions:</p> <ol> <li>Verify Group Membership: Check your groups at CADC Group Management</li> <li>Contact Project Administrator: Request addition to the appropriate project group</li> <li>Wait for Propagation: Group changes may take several minutes to propagate to storage systems</li> <li>Check Path: Ensure you're using the correct project directory path</li> </ol>"},{"location":"platform/permissions/#harbor-container-registry-access-issues","title":"Harbor Container Registry Access Issues","text":"<p>Symptoms: Cannot push to or pull from private container repositories</p> <p>Common Causes: - Insufficient Harbor permissions for the target project - Not logged into Harbor registry - Attempting to access non-existent or private repositories</p> <p>Solutions:</p> <ol> <li>Check Harbor Login: <code>docker login images.canfar.net</code></li> <li>Request Permissions: Contact support@canfar.net for repository access</li> <li>Verify Project Access: Confirm you have developer or master role for the target project</li> <li>Check Repository Names: Ensure correct project and container naming</li> </ol>"},{"location":"platform/permissions/#api-authentication-failures","title":"API Authentication Failures","text":"<p>Symptoms: 401 Unauthorized errors when using APIs or CLI tools</p> <p>Common Causes: - Expired authentication tokens - Invalid token format in API calls - Missing or corrupt proxy certificates</p> <p>Solutions:</p> <ol> <li>Refresh CLI Token: <code>canfar auth login</code></li> <li>Generate New Proxy Certificate: <code>cadc-get-cert -u [username]</code></li> <li>Check Token Format: Ensure API calls use <code>Bearer YOUR_TOKEN</code> format</li> <li>Verify Certificate Location: Check <code>~/.ssl/cadcproxy.pem</code> exists and is valid</li> </ol>"},{"location":"platform/permissions/#acl-changes-not-taking-effect","title":"ACL Changes Not Taking Effect","text":"<p>Symptoms: File permissions don't match expected ACL settings</p> <p>Common Causes: - Restrictive ACL mask limiting effective permissions - Missing default ACLs on directories - Incorrect user or group names in ACL entries</p> <p>Solutions:</p> <ol> <li>Check Effective Permissions: <code>getfacl [filename]</code></li> <li>Update ACL Mask: <code>setfacl -m m::rwx [filename]</code></li> <li>Set Default ACLs: <code>setfacl -d -m g:[groupname]:rw [directory]/</code></li> <li>Verify Names: Confirm user and group names exist and are spelled correctly</li> </ol>"},{"location":"platform/permissions/#group-changes-not-visible","title":"Group Changes Not Visible","text":"<p>Symptoms: New group memberships not recognized by platform services</p> <p>Common Causes: - Recent group changes not yet propagated - Cache timing issues across distributed services - Session authentication using stale group information</p> <p>Solutions:</p> <ol> <li>Wait for Propagation: Allow 5-10 minutes for changes to propagate</li> <li>Refresh Authentication: Log out and back in to refresh group memberships</li> <li>Contact Support: For persistent issues, contact support@canfar.net</li> </ol> <p>Security Best Practices</p> <p>Protect Your Credentials:</p> <ul> <li>Never share CADC passwords or authentication tokens</li> <li>Use group-based permissions instead of individual token sharing</li> <li>Regularly review access permissions for sensitive data</li> <li>Report suspected security issues immediately to support</li> </ul>"},{"location":"platform/permissions/#advanced-permission-management","title":"\ud83d\udd17 Advanced Permission Management","text":""},{"location":"platform/permissions/#multi-institutional-collaboration_1","title":"Multi-Institutional Collaboration","text":"Cross-Institution Access: CANFAR supports researchers from multiple institutions through flexible group membership and external user integration. Guest Researcher Workflows: Temporary access patterns for visiting researchers, students, and short-term collaborations. Resource Delegation: Project administrators can delegate specific permissions without granting full administrative access."},{"location":"platform/permissions/#enterprise-integration","title":"Enterprise Integration","text":"LDAP/Active Directory: Contact CANFAR administrators for integration with institutional identity management systems. Single Sign-On: CADC authentication integrates with Canadian academic identity federations and international collaborations. Compliance Requirements: Support for institutional data governance and compliance requirements through audit logging and access controls."},{"location":"platform/community/","title":"CANFAR Community Resources","text":"<p>As teams and projects grow, it's important to share knowledge and best practices. This section provides resources and tutorials for specific communities and their use cases within the CANFAR ecosystem.</p> <p>If you have a specific use case or community you'd like to see represented here, please contribute to the documentation or contact us.</p>"},{"location":"platform/community/CASA_and_more/","title":"CASA containers and adjacent software","text":"<p>This page contains a summary of additional packages included in CASA containers, known issues and work-arounds for specific containers, as well as a brief summary on other radio astronomy tools available.</p>"},{"location":"platform/community/CASA_and_more/#casa-add-on-tools-and-packages","title":"CASA add-on tools and packages","text":""},{"location":"platform/community/CASA_and_more/#astroquery-astropy","title":"Astroquery / Astropy","text":"<p>The astroquery tool is presently only installed on newer CASA containers (6.4.4 and above).  To use astroquery from an appropriate CASA container, type the following to initiate an astroquery-compatible version of python: <pre><code>/opt/casa/bin/python3\n</code></pre> As per the astroquery documentation, the tool can then be used on the command line within the python environment.  For example, the following sequence of commands yield a one-line table listing some basic information about M1.</p> <pre><code>from astroquery.simbad import Simbad\nresult_table = Simbad.query_object(\"m1\")\nresult_table.pprint()\n</code></pre>"},{"location":"platform/community/CASA_and_more/#analysis-utilities","title":"Analysis Utilities","text":"<p>The analysisUtils package package is pre-installed on every CASA container, and is ready to use.  You may need to type the following to load the package:</p> <pre><code>import analysisUtils as au\n</code></pre>"},{"location":"platform/community/CASA_and_more/#firefox","title":"Firefox","text":"<p>The Firefox web-browser, needed for CASA commands where you are interacting with the weblogs, should available for CASA versions 6.1.0 to 6.4.3.  Error messages will pop up in your terminal window, but minimal testing suggests that it is sufficiently functional.</p>"},{"location":"platform/community/CASA_and_more/#uvmultifit","title":"UVMultiFit","text":"<p>The UVMultiFit package is presently installed and working for all CASA 5.X versions except 5.8.  To load the UVMultiFit package, initiate casa and then type</p> <pre><code>from NordicARC import uvmultifit as uvm\n</code></pre>"},{"location":"platform/community/CASA_and_more/#known-issues","title":"Known Issues","text":"<ol> <li> <p>CASA versions <code>6.5.0</code> to <code>6.5.2</code> initially launch with some display errors in the logger window.  Exiting casa (but not the container) and re-starting casa fixes the issue, i.e.,</p> <pre><code>casa\nexit\ncasa\n</code></pre> </li> <li> <p>Running multi-thread pipeline scripts (MPI CASA) may generate error messages, as described here under the 'Running pipeline in non-interactive mode' section.  A CANFAR ALMA user reports success initiating MPI CASA in a Desktop container as follows:</p> <pre><code>xvfb-run -a mpicasa casa \u2014nologger \u2014nogui -agg -c casa_script.py\n</code></pre> </li> </ol>"},{"location":"platform/community/CASA_and_more/#casa-adjacent-containers","title":"CASA Adjacent Containers","text":""},{"location":"platform/community/CASA_and_more/#galario","title":"Galario","text":"<p>The UV data analysis package galario is available under the radio-submm menu.  Note that this container has had minimal testing, and the uvplot package commands in the quickstart.py script are not presently working, although all preceeding commands in the quickstart.py script do work.</p>"},{"location":"platform/community/CASA_and_more/#starlink","title":"Starlink","text":"<p>The JCMT's Starlink package is available under the radio-submm menu, including image analysis tools and the gaia image viewer.  Note that the starlink-pywrapper add-on package is presently not working.  Minimal testing has been done on the Starlink container.</p>"},{"location":"platform/community/alma/","title":"ALMA resources","text":"<p>This section collects ALMA-specific tutorials and tips for using the CANFAR Science Platform.</p>"},{"location":"platform/community/alma/#desktop","title":"Desktop","text":"<ul> <li>Archive download (web)</li> <li>Archive download (script)</li> <li>CASA containers</li> <li>Starting CASA</li> <li>Typical reduction &amp; imaging</li> </ul>"},{"location":"platform/community/alma/#general-tools","title":"General tools","text":"<ul> <li>File transfers overview</li> <li>Group management</li> <li>Using SSHFS</li> <li>Using VOS Tools</li> <li>Using web storage</li> </ul>"},{"location":"platform/community/alma/#new-users","title":"New users","text":"<ul> <li>Overview</li> <li>Login</li> <li>Launching Desktop</li> <li>Launching Notebook</li> <li>Project spaces</li> </ul>"},{"location":"platform/community/alma/#notebook","title":"Notebook","text":"<ul> <li>Transfer files in Notebook</li> </ul>"},{"location":"platform/community/alma/#tips-tricks","title":"Tips &amp; tricks","text":"<ul> <li>Direct URL downloads</li> <li>Increase font size</li> <li>Using clipboard</li> </ul>"},{"location":"platform/community/alma/ALMA_Desktop/archive_download/","title":"Downloading from the ALMA archive (web)","text":"<p>Instructions for downloading ALMA data via the web interface. For large downloads prefer scripted methods or transfer tools.</p>"},{"location":"platform/community/alma/ALMA_Desktop/archive_script_download/","title":"Using scripts to download ALMA archive data","text":"<p>How to download ALMA archive data in bulk using URL lists and command-line tools securely.</p> <p>Use URL lists and command-line tools (wget, curl) or transfer tools for bulk downloads; ensure credentials are handled securely.</p> <p>For large datasets obtain an URL list from the archive and use <code>wget</code> with the certificate you fetched with <code>cadc-get-cert</code>:</p> <pre><code>cadc-get-cert -u [username]\nwget --content-disposition -i url_list.txt --certificate ~/.ssl/cadcproxy.pem --ca-certificate ~/.ssl/cadcproxy.pem\n</code></pre> <p>Alternatively use <code>vcp</code> to transfer directly into a VOSpace location.</p>"},{"location":"platform/community/alma/ALMA_Desktop/casa_containers/","title":"CASA container images","text":"<p>Notes about CASA container images available in Desktop sessions and compatibility considerations.</p> <ul> <li>Container images may differ by CASA major/minor version.</li> <li>Some CASA tasks rely on external system libraries; these are usually bundled in the container but check the release notes if you see missing symbols.</li> </ul> <p>If you need GPU acceleration or special libraries, consult the container documentation for appropriate tags and runtime flags.</p>"},{"location":"platform/community/alma/ALMA_Desktop/casa_containers/#astroquery-astropy","title":"Astroquery / astropy","text":"<p>The <code>astroquery</code> tool is installed on newer CASA containers (6.4.4-6.6.3). To use astroquery from the CASA Python:</p> <pre><code>from astroquery.simbad import Simbad\nresult_table = Simbad.query_object(\"m1\")\nresult_table.pprint()\n</code></pre>"},{"location":"platform/community/alma/ALMA_Desktop/casa_containers/#analysis-utilities","title":"Analysis Utilities","text":"<p>The <code>analysisUtils</code> package is pre-installed on many CASA containers. You may need to run <code>import analysisUtils as au</code> to load it.</p>"},{"location":"platform/community/alma/ALMA_Desktop/casa_containers/#admit","title":"ADMIT","text":"<p>ADMIT (ALMA Data Mining Tool) is available on some CASA containers (typically CASA &gt;= 4.5); newer containers may exclude it.</p>"},{"location":"platform/community/alma/ALMA_Desktop/casa_containers/#known-container-notes","title":"Known Container Notes","text":"<ul> <li>Firefox is available on some CASA versions for minimal web-browser interaction.</li> </ul>"},{"location":"platform/community/alma/ALMA_Desktop/casa_containers/#example-restarting-casa-known-issue-workaround","title":"Example: restarting CASA (known issue workaround)","text":"<pre><code>casa\nexit\ncasa\n</code></pre>"},{"location":"platform/community/alma/ALMA_Desktop/casa_containers/#example-run-casa-with-mpi-and-xvfb-non-interactive","title":"Example: run CASA with MPI and Xvfb (non-interactive)","text":"<pre><code>xvfb-run -a mpicasa casa --nologger --nogui -agg -c casa_script.py\n</code></pre>"},{"location":"platform/community/alma/ALMA_Desktop/start_casa/","title":"Starting CASA in a Desktop session","text":"<p>How to launch CASA inside a Desktop session and run reduction or imaging scripts.</p> <p>CASA (Common Astronomy Software Applications) is typically provided as a container in the Desktop session. To start CASA:</p> <ol> <li>Launch a Desktop session and open a terminal.</li> <li>Start the CASA container. Depending on the configuration the command may be as simple as:</li> </ol> <pre><code>casa\n</code></pre> <ol> <li>If you have a reduction script (e.g., <code>scriptForPI.py</code>) you can run it within CASA:</li> </ol> <pre><code>execfile('scriptForPI.py')\n</code></pre> <p>If your dataset requires a specific CASA version, choose the container image for that version. the scripts distributed with ALMA Cycle 0 data on the archive). to use a non-CASA terminal for all regular linux uses.</p> <p>Once you have launched a Desktop session it is straightforward to run CASA in a terminal.</p> <p></p> <p>To start a CASA-enabled terminal, click the <code>Applications</code> menu at the top-left of the screen and choose the desired CASA version from the <code>AstroSoftware</code> menu.</p> <p>Select the CASA version you want. All versions back to CASA 3.4.0 are available; choose the one appropriate for your scripts.</p> <p>Clicking a CASA version opens a terminal where you can start CASA with <code>casa</code> or <code>casa --pipeline</code> (two dashes before <code>pipeline</code>).</p> <p>You can open a regular (non-CASA) terminal by double-clicking the <code>terminal</code> icon. CASA terminals accept a limited set of commands; use the non-CASA terminal for general Linux work.</p>"},{"location":"platform/community/alma/ALMA_Desktop/typical_reduction/","title":"Typical ALMA data reduction workflow","text":"<p>A concise overview of a typical reduction and imaging workflow using CASA in Desktop sessions.</p> <p>First, download your ALMA data onto your Desktop Session (see the archive download tutorials). If you already have the data locally, use one of the file transfer methods to move it into your session.</p> <p>Next, open a CASA container (see the Start CASA tutorial for the correct version). Start CASA in interactive or pipeline mode depending on your script:</p> <pre><code>casa\ncasa --pipeline\n</code></pre> <p>Inside CASA run the reduction script (commonly named <code>scriptForPI.py</code>):</p> <pre><code>execfile('scriptForPI.py')\n</code></pre> <p>After the reduction finishes you will find calibrated measurement sets in a <code>calibrated/</code> directory. The <code>scriptForImaging.py</code> script (if provided) can be used to create images and is often run from the <code>calibrated/</code> directory.</p> <p>When analysis is complete, transfer final files off the system using one of the transfer options (VOSpace, web download, vcp, etc.). Example using <code>vcp</code>:</p> <pre><code>vcp calibrated_final_cont_image_162622-24225.* vos:helenkirk/\n</code></pre> <p>Note: use VOSpace for long-term storage; the Science Portal sessions are not intended as persistent archival storage.</p>"},{"location":"platform/community/alma/General_tools/File_transfers/","title":"File transfer overview","text":"<p>Summary of available file transfer methods: Notebook upload, web storage, VOS Tools, SSHFS, and direct URLs.</p> <p>There are several ways to move files into and out of the Science Portal. Common options include:</p> <ul> <li>Notebook upload: small files can be uploaded via the Notebook file browser (see the Notebook transfer tutorial).</li> <li>Web storage: use the web interface to upload/download and manage files.</li> <li>VOS Tools: command-line tools (<code>vcp</code>, <code>vls</code>, <code>vrm</code>) for copying files to/from VOSpace and Science Portal locations.</li> <li>SSHFS: mount the remote file system locally and use rsync or other tools to sync files.</li> <li>Direct URL: obtain a direct ARC URL list and use <code>wget</code> with the appropriate certificate to download files.</li> </ul> <p>See the individual tutorials for step-by-step instructions:</p> <ul> <li>Notebook uploads: /science-containers/general/Notebook/transfer_file</li> <li>Web storage: /science-containers/general/General_tools/Using_webstorage</li> <li>VOS Tools: /science-containers/general/General_tools/Using_vostools</li> <li>SSHFS: /science-containers/general/General_tools/Using_sshfs</li> <li>Direct URL downloads: /science-containers/general/TipsTricks/Direct_url</li> </ul>"},{"location":"platform/community/alma/General_tools/Group_management/","title":"Group management","text":"<p>How to create and manage project groups and permissions via the Science Portal web UI.</p> <p>Create and manage groups via the CANFAR web UI. Use the edit membership controls to add users and administrators. For complex setups, group permissions can also be managed via command-line tools; see the CANFAR docs for details.</p> <p>For automated or complex permission changes, prefer the command-line tools described in the CANFAR documentation.</p>"},{"location":"platform/community/alma/General_tools/Using_sshfs/","title":"Using SSHFS","text":"<p>Mount the remote Science Platform file system locally with <code>sshfs</code> to access files directly from your machine.</p>"},{"location":"platform/community/alma/General_tools/Using_sshfs/#installation","title":"Installation","text":"<p>Software installation is required to use this tool.</p> <p>Linux: SSHFS is Linux-based software that needs to be installed on your local computer. On Ubuntu and Debian based systems, it can be installed through apt-get:</p> <pre><code>sudo apt-get install sshfs\n</code></pre> <p>Mac OSX: Often SSHFS is already installed; if not, you will need to download FUSE and SSHFS from the osxfuse site</p>"},{"location":"platform/community/alma/General_tools/Using_sshfs/#prepare-your-arc-account","title":"Prepare your Arc account","text":"<p>A public SSH key will need to be installed into your Arc home directory. Ensure you have a <code>.ssh</code> folder in your <code>/home/[your_cadc_username]</code> folder.  You can do this through the UI: https://www.canfar.net/storage/arc/list/home</p> <ol> <li> <p>Ensure you are logged in using the pulldown in the top right menu.</p> </li> <li> <p>Visit your <code>Home</code> folder.</p> </li> <li> <p>If there is no <code>.ssh</code> folder listed, create one.</p> </li> <li> <p>Ensure you have a file called <code>authorized_keys</code> with your SSH public key in it. This public key should match whichever private key you are using to authenticate with. For example, if your private key on your local machine is <code>${HOME}/.ssh/id_rsa</code>, then your public key is likely <code>${HOME}/.ssh/id_rsa.pub</code>.</p> </li> </ol>"},{"location":"platform/community/alma/General_tools/Using_sshfs/#mount-the-remote-file-system","title":"Mount the Remote File System","text":"<p>For Ubuntu/Debian Linux or Mac OSX, the instructions are below.</p> <p>To start, we will need to create a local directory in which to mount the file system, \"arc\":</p> <pre><code>mkdir $HOME/arc\n</code></pre> <p>Now we can mount the file system locally using the following command, based on which OS you are running. You will be asked for your CADC password during this step.</p> <p>On Ubuntu/Debian:</p> <pre><code>sshfs -o reconnect,ServerAliveInterval=15,ServerAliveCountMax=10 -p 64022 [your_cadc_username]@ws-uv.canfar.net:/ $HOME/arc\n</code></pre> <p>On Mac OSX:</p> <pre><code>sshfs -o reconnect,ServerAliveInterval=15,ServerAliveCountMax=10,defer_permissions -p 64022 [your_cadc_username]@ws-uv.canfar.net:/ $HOME/arc\n</code></pre> <p>The extra <code>defer_permissions</code> switch works around issues with OSX permission handling.</p>"},{"location":"platform/community/alma/General_tools/Using_sshfs/#synch-local-and-remote-directories-with-rsync","title":"Synch Local and Remote Directories with rsync","text":"<p>With the steps above in place, the rsync (\"remote synch\") command can be used. rsync uses an algorithm that minimizes the amount of data copied by only moving the portions of files that have changed.</p> <p>The synch is performed using the following:</p> <pre><code>rsync -vrltP source_dir $HOME/arc/destination_dir/\n</code></pre> <p>Pro tip: including a <code>/</code> after source_dir in the command above will transfer the directory contents without the main directory itself.</p>"},{"location":"platform/community/alma/General_tools/Using_sshfs/#unmounting-the-file-system","title":"Unmounting the File System","text":"<p>If you have finished working with your files and want to disconnect from the remote file system, you can do this by:</p> <pre><code>umount $HOME/arc\n</code></pre> <p>NB: If you run into problems with the original sshfs command and need to run it again, you will likely need to unmount first.</p>"},{"location":"platform/community/alma/General_tools/Using_vostools/","title":"Using VOS Tools","text":"<p>Examples for using VOSpace command-line tools (vcp, vls, vrm) and notes on authentication and certificates.</p> <p>VOS Tools provide command-line access to CANFAR VOSpace and the Science Portal storage.</p> <p>Install instructions and details are available on CANFAR's storage documentation (see CANFAR storage docs).</p> <p>Typical commands:</p> <pre><code>vcp localfile arc:home/[username]\nvcp vos:[username]/remotefile ./\nvls vos:[username]\nvrm vos:[username]/file\n</code></pre> <p>If you see an expired certificate error, update it with:</p> <pre><code>cadc-get-cert -u [username]\n</code></pre> <p>The most efficient way to transfer files in and out of CANFAR's Science Portal is to use the VOS Tools, which are also used for interacting with CANFAR's VOSpace.</p> <p>Instructions for installing VOS Tools on your personal computer are located in CANFAR's storage documentation under the section \"The vos Python module and command line client\".</p> <p>Instructions on how to use this tool, including some basic examples, are found on the same webpage. In brief, this tool runs on the command line with syntax similar to the linux <code>scp</code> command. File locations within CANFAR systems are specified with vos for VOSpace and arc for the Science Portal. For example, to copy a file from your personal computer to your home directory in the Science Portal, you would type the following on your local computer:</p> <pre><code>vcp myfile.txt arc:home/[username]\n</code></pre> <p>To copy a file from VOSpace to your personal computer, you would use:</p> <pre><code>vcp vos:[username]/myfile.txt ./\n</code></pre> <p>To copy files from the Science Portal to VOSpace, you would similarly use the command:</p> <pre><code>vcp myfile.txt vos:[username]\n</code></pre> <p>Note that VOS Tools use a security certificate which needs to be updated periodically. If you get an error message stating:</p> <pre><code>ERROR:: Expired cert.\n</code></pre> <p>Update by running:</p> <pre><code>cadc-get-cert -u [username]\n</code></pre> <p>and enter your password for CADC/CANFAR services at the prompt.</p>"},{"location":"platform/community/alma/General_tools/Using_webstorage/","title":"Using web storage","text":"<p>Using the web UI to upload, download, and manage files in VOSpace or project storage; use URL lists for scripting.</p>"},{"location":"platform/community/alma/General_tools/Using_webstorage/#upload-files","title":"Upload File(s)","text":"<p>To upload one or more files (or folders), navigate to the desired directory, then click the <code>Add</code> button along the top, selecting the appropriate option. Follow the instructions on the pop-up box that appears to choose and upload your files.</p>"},{"location":"platform/community/alma/General_tools/Using_webstorage/#download-files","title":"Download Files","text":"<p>Downloading files is also straightforward, and three options are outlined here: <code>URL List</code>, <code>HTML List</code>, and <code>Zip</code>. The <code>Zip</code> option will usually be the most practical, but the <code>HTML List</code> option may be preferred when downloading only a few files, and <code>URL List</code> may be best for scripting.</p>"},{"location":"platform/community/alma/General_tools/Using_webstorage/#download-url-list-option","title":"Download - URL List Option","text":"<p>First, choose the <code>URL List</code> option, then select the desired directory and file name and click <code>save</code>.</p> <p>If the file(s) is/are not publicly available, update your security certificates by running:</p> <pre><code>cadc-get-cert -u [username]\n</code></pre> <p>Then download the files using <code>wget</code> with the provided URL list and certificates:</p> <pre><code>wget --content-disposition -i cadcUrlList.txt --certificate ~/.ssl/cadcproxy.pem --ca-certificate ~/.ssl/cadcproxy.pem\n</code></pre>"},{"location":"platform/community/alma/General_tools/Using_webstorage/#download-html-list-option","title":"Download - HTML List Option","text":"<p>Clicking the <code>HTML List</code> option will bring up a pop up window with a series of long URL strings - each entry is a clickable direct link to your individual files.</p>"},{"location":"platform/community/alma/General_tools/Using_webstorage/#download-zip-option","title":"Download - Zip Option","text":"<p>The <code>Zip</code> option allows you to download a single zip file containing all of your requested files. Choose the <code>zip</code> option, and click <code>save</code> in the pop-up window after adjusting your preferred directory and zip file name.</p>"},{"location":"platform/community/alma/NewUser/LaunchCARTA/","title":"Launching CARTA","text":"<p>How to open CARTA in a Desktop session to visualise spectral cubes and images.</p> <p>In the Desktop session choose CARTA from the application menu or launch from a terminal if available. Use File -&gt; Open to select files from your project space or VOSpace.</p>"},{"location":"platform/community/alma/NewUser/LaunchDesktop/","title":"Launching a Desktop session","text":"<p>Steps to start a Desktop session from the Science Portal web UI and connect to it.</p> <p>To start a Desktop session use the Science Portal web UI. Choose the desired container image and allocate resources (CPU, memory). When the desktop is ready connect using the provided browser window.</p> <p>After logging in to the Science Portal and clicking the plus sign to launch a new session, choose a session type of <code>desktop</code>.</p> <p></p> <p>Note that the remaining menu bars and options update automatically after your session type selection. There is currently only one option for <code>container image</code>, so no selection is needed.</p> <p>Give your session a descriptive name; this will later appear on your Science Portal page if you need to log in again later</p> <p></p> <p>Now, hit the launch button and wait for your session to launch</p> <p></p> <p>Congratulations! You've started your first Desktop session. You are automatically returned to the main Science Portal page, where your desktop session appears as an icon, with your chosen descriptive name.</p> <p></p> <p>This takes you to the landing page for your Desktop session. Click the connect button to connect to the session.</p> <p></p> <p>When your session becomes inactive for some time, you are automatically returned to this page, but you can return to the session exactly where you left off by once again clicking the connect button.</p> <p></p>"},{"location":"platform/community/alma/NewUser/LaunchNotebook/","title":"Launching a Notebook","text":"<p>Create and open a Notebook session (JupyterLab) via the Science Portal and use the file browser to manage files.</p>"},{"location":"platform/community/alma/NewUser/Login/","title":"Logging in to the Science Portal","text":"<p>Authenticate using CANFAR credentials or supported OIDC providers via the web UI.</p> <p>You will need a CADC account to access the system. If you do not have one, you can request one at: https://www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/en/auth/request.html</p> <p>To request authorization to use the Science Portal, send an email to support@canfar.net. You may also wish to consider the following:</p> <ul> <li>Project space: If you intend to work on a dataset with collaborators, it is recommended that you set up a project space, where a designated group of users all has common access to the files contained within it.</li> <li>Communications on Discord: a Discord workspace is used for some aspects of communication around the Science Portal, including notice of service outages and some trouble-shooting support. You can request that you be added to this space also by contacting the email address listed above.</li> </ul> <p>Once your access has been confirmed, go to the CANFAR page: https://www.canfar.net and log in to access the Science Portal.</p> <p>Start a new session by clicking the plus sign.</p> <p>There are four different types of sessions that you can choose to launch: Desktop, CARTA, Notebook, and Contributed. All are described below; in brief, Desktop provides a linux desktop-like working environment, CARTA corresponds to ALMA's CARTA visualization tool, Notebook provides a Jupyter Notebook environment, and Contributed contains community-contributed tools such as a time estimator for the CASTOR mission.</p>"},{"location":"platform/community/alma/NewUser/Overview/","title":"New user overview","text":"<p>Quick-start notes for logging in, launching sessions, and using project spaces.</p>"},{"location":"platform/community/alma/NewUser/ProjectSpace/","title":"Project spaces","text":"<p>Short guide to requesting and sharing a project space for collaborative work.</p>"},{"location":"platform/community/alma/NewUser/ProjectSpace/#what-is-a-project-space","title":"What is a Project Space?","text":"<p>Users can work with files in two different main directories. The first is their personal home directory, found in <code>/home/[username]</code>. The second is within a project directory, found in <code>/project/[project_name]</code>. These project directories provide a space where files can easily be shared and analyzed with collaborators.</p>"},{"location":"platform/community/alma/NewUser/ProjectSpace/#how-to-request-a-project-space","title":"How to Request a Project Space","text":"<p>In order to request a project space, you will need to have a name for the space (the name of the project directory). As with home directories, the default diskspace allocation is 200GB. If you anticipate needing more than this, you will need an estimate of the amount of diskspace that you will need. Once you have this information, if you are on the Discord workspace, the easiest way to request a project space is to post the request there, with a note to Kevin Casteels. Alternatively, you can email the request to <code>support@canfar.net</code>.</p>"},{"location":"platform/community/alma/NewUser/ProjectSpace/#how-to-give-collaborators-access","title":"How to Give Collaborators Access","text":"<p>Collaborators will require a free CADC account in order to be added as people with designated access to the project space files. A CADC account can be requested at https://www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/en/auth/request.html. If your collaborators only wish to download the files and analyze them on their own computers, this step is sufficient; they can access the files on the web at https://www.canfar.net/storage/arc/list/project/[project_name] or use the VOS Tools to download them.</p>"},{"location":"platform/community/alma/Notebook/transfer_file/","title":"Uploading files in a Notebook session","text":"<p>Use the Jupyter file browser to upload small files; for large datasets prefer VOSpace or command-line transfer tools.</p>"},{"location":"platform/community/alma/Notebook/transfer_file/#transfer-file-into-notebook-session","title":"Transfer file into Notebook session","text":"<p>Smaller files can be uploaded into a Notebook session easily in two different ways. These are outlined in turn below.</p>"},{"location":"platform/community/alma/Notebook/transfer_file/#directly-upload-the-file","title":"Directly Upload the File","text":"<p>Once you have navigated into your directory of interest using the browser in the left-hand side, click the upward-pointing arrow on the top menu bar.</p> <p></p> <p>This will bring up a window that will let you select the file you wish to upload. Click the <code>Open</code> button as usual to confirm your choice of files.</p> <p></p> <p>Success! Your file is now visible in the browser, and would also be accessible in the same location in a Desktop or a CARTA session.</p> <p></p>"},{"location":"platform/community/alma/Notebook/transfer_file/#copy-paste-text","title":"Copy-Paste Text","text":"<p>Alternatively, you can copy and paste text directly into a file within your Notebook session. This might be preferred if, for example, you want to copy a snippet of code into an already existing file in your session. Start by opening up a terminal by double-clicking on the icon.</p> <p></p> <p>This opens a terminal on the right hand side of the screen which you can interact with as usual. In the example shown, the text editor vi is being initiated on the command line.</p> <p></p> <p>On your local computer, you would select and copy the text of interest.</p> <p></p> <p>You can then paste this text into a text editor in the terminal. Once the file is saved, it is accessible from the file browser in the current directory, and would be visible in a Desktop or a CARTA session as well.</p> <p></p>"},{"location":"platform/community/alma/TipsTricks/Direct_url/","title":"Direct URL downloads","text":"<p>Notes on using <code>wget</code> with URL lists and certificates for direct downloads of private data.</p>"},{"location":"platform/community/alma/TipsTricks/Increase_font/","title":"Increase font size","text":"<p>Tips to increase font sizes in Desktop containers and Jupyter notebooks for readability.</p> <p>In Desktop sessions increase font sizes via application preferences or browser zoom. In JupyterLab use the View menu or browser zoom settings.</p> <p>Images show how to change the terminal font size via the terminal's preferences or context menu.</p>"},{"location":"platform/community/alma/TipsTricks/Using_clipboard/","title":"Using the clipboard","text":"<p>How to copy and paste small text between Desktop and Notebook sessions using built-in clipboard features.</p> <p>Copy selected text with Ctrl-Shift-C and paste with Ctrl-Shift-V inside Desktop containers. For larger file transfers use VOSpace or file upload/download.</p>"},{"location":"platform/containers/","title":"Containers","text":"<p>Working and building software containers on CANFAR.</p> <p>\ud83c\udfaf Container Guide Overview</p> <p>Master CANFAR's containerized environments:</p> <ul> <li>Container Concepts: Understanding reproducible software environments</li> <li>Available Containers: Pre-built astronomy software stacks</li> <li>Container Building: Creating custom environments for specialised workflows</li> <li>Registry Management: Harbor registry access and image distribution</li> </ul> <p>Containers provide pre-packaged software environments that include everything needed to run astronomy applications. On CANFAR, containers eliminate the \"works on my machine\" problem by ensuring consistent, reproducible computational environments across different sessions and workflows.</p>"},{"location":"platform/containers/#what-are-containers","title":"\ud83d\udccb What Are Containers?","text":"<p>Think of containers as complete software packages that bundle an operating system (typically Ubuntu Linux), astronomy software like CASA or Python packages, programming tools, system libraries, and environment configuration into a single portable unit. When you launch a session on CANFAR, you're essentially starting up one of these pre-configured environments with your data and home directory automatically mounted and accessible.</p> <p>Key Concept: Reproducible Environments</p> <p>Containers provide consistent, reproducible software environments for astronomy work across sessions and teams.</p>"},{"location":"platform/containers/#why-containers-matter-for-astronomy","title":"Why Containers Matter for Astronomy","text":""},{"location":"platform/containers/#traditional-software-installation","title":"Traditional Software Installation","text":"<ul> <li>Struggle with dependencies and conflicting versions</li> <li>Missing libraries and system requirements</li> <li>Different behaviour across different machines</li> <li>Time-consuming setup and configuration</li> </ul>"},{"location":"platform/containers/#canfar-containers","title":"CANFAR Containers","text":"<ul> <li>Consistent environment: Works the same everywhere</li> <li>Pre-configured: Astronomy packages included</li> <li>No installation hassles: Ready to use immediately</li> <li>Easy sharing: Reproducible results across teams</li> </ul> <p>Research Reproducibility</p> <p>Containers ensure your analysis runs the same way for you, your collaborators, and future researchers. This is crucial for reproducible science.</p>"},{"location":"platform/containers/#container-architecture-on-canfar","title":"Container Architecture on CANFAR","text":"<p>The container ecosystem on CANFAR follows a layered approach:</p> <pre><code>graph TB\n    BaseOS[Ubuntu Linux Base] --&gt; SystemLibs[\"OS Packages\"]\n    SystemLibs --&gt; CondaPython[\"conda-forge packages\"]\n\n    CondaPython --&gt; Astroml[\"astroml Container\"]\n    Astroml --&gt; Casa[\"casa Container\"]\n    CondaPython --&gt; Custom[Custom Containers]\n\n    Runtime[CANFAR Runtime] --&gt; Storage[Storage Mounting]\n    Runtime --&gt; UserContext[User Context]\n    Runtime --&gt; Resources[Resource Allocation]</code></pre> <p>Base containers provide fundamental tools and the conda package manager, while specialised containers build upon these foundations to offer domain-specific software stacks. This architecture ensures consistency while allowing flexibility for different research needs.</p>"},{"location":"platform/containers/#build-time-vs-runtime","title":"\ud83c\udfd7\ufe0f Build Time vs Runtime","text":"<p>Understanding the distinction between build time and runtime is crucial for effective container usage:</p>"},{"location":"platform/containers/#build-time","title":"Build Time","text":"<p>What happens when containers are created:</p> <ul> <li>Base image selection: Choose Ubuntu, Python, or specialised astronomy base</li> <li>Software installation: Install system packages, Python libraries, astronomy tools</li> <li>Environment configuration: Set up paths, environment variables, user permissions</li> <li>Code packaging: Include stable scripts and analysis tools</li> <li>Image optimization: Layer caching, size reduction, security patches</li> </ul> <pre><code># Build time example\nFROM ubuntu:24.04\n\n# Install system dependencies (build time)\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    python3-dev \\\n    libcfitsio-dev \\\n    &amp;&amp; apt-get clean\n\n# Install Python packages (build time)\nRUN pip install astropy numpy matplotlib\n\n# Package stable code (build time)\nCOPY analysis_tools/ /opt/tools/\n</code></pre>"},{"location":"platform/containers/#runtime","title":"Runtime","text":"<p>What happens when you launch a session:</p> <ul> <li>User context: Container runs as your CADC username (not root)</li> <li>Storage mounting: <code>/arc/home</code>, <code>/arc/projects</code>, <code>/scratch</code> mounted automatically</li> <li>Resource allocation: CPU, memory, GPU assigned based on session request</li> <li>Network access: Internet connectivity for downloading data or documentation</li> <li>Session integration: Jupyter, desktop, or headless execution mode activated</li> </ul> <pre><code># Runtime environment (inside your running container)\necho $USER                      # [user]\necho $HOME                      # /arc/home/[user]\nls /arc/projects/               # Your accessible project directories\ndf -h /scratch/                 # Temporary high-speed storage\n</code></pre> <p>Persistence Boundary</p> <p>Build time changes are permanent and part of the container image. Runtime changes (like <code>pip install --user package</code>) are on <code>/arc</code> and not on the container. Keep stable software in the image; keep development scripts in <code>/arc/home/[user]</code> or <code>/arc/projects/[project]</code>.</p>"},{"location":"platform/containers/#how-containers-relate-to-sessions","title":"\ud83d\udd17 How Containers Relate to Sessions","text":"<p>CANFAR containers are designed to work seamlessly with different session types, each optimised for specific workflows:</p>"},{"location":"platform/containers/#session-type-integration","title":"Session Type Integration","text":"<pre><code>graph TB\n    Container[Container Image] --&gt; SessionType{Session Type}\n\n    SessionType --&gt; Notebook[notebook]\n    SessionType --&gt; Desktop[desktop]\n    SessionType --&gt; Carta[carta]\n    SessionType --&gt; Firefly[firefly]\n    SessionType --&gt; Contributed[contributed]\n    SessionType --&gt; Batch[headless]\n\n    Notebook --&gt; JupyterLab[JupyterLab Interface]\n    Desktop --&gt; DesktopEnv[noVNC Ubuntu GUI]\n    Carta --&gt; CartaWeb[CARTA Interface]\n    Firefly --&gt; FireflyWeb[Firefly Interface]\n    Contributed --&gt; WebApp[Custom Web Interface]\n    Batch --&gt; ScriptExec[Script Execution]\n\n    JupyterLab --&gt; Storage[ \\/arc and \\/scratch Storage]\n    DesktopEnv --&gt; Storage\n    CartaWeb --&gt; Storage\n    FireflyWeb --&gt; Storage\n    WebApp --&gt; Storage\n    ScriptExec --&gt; Storage</code></pre> <p>Same container, different interfaces: The <code>astroml</code> container can run as a notebook (<code>JupyterLab</code>) session, as a desktop app**lication (as an <code>xterm</code>) in a **desktop session, or batch job (headless) session as an executable script.</p>"},{"location":"platform/containers/#notebook-sessions","title":"Notebook Sessions","text":"<p>Requirements for notebook containers:</p> <ol> <li>JupyterLab installed: The <code>jupyterlab</code> package must be installed and in path in the container</li> <li>Container labelling: Tagged as notebook in the registry</li> </ol> <p>When you launch a notebook session, CANFAR automatically:</p> <ul> <li>Starts JupyterLab on dedicated port</li> <li>Mounts your storage directories</li> <li>Provides web-based access to the Python environment</li> </ul> <p>The <code>astroml</code> container exemplifies this perfectly - it's a comprehensive Python astronomy stack with <code>astropy</code>, <code>scipy</code>, <code>pandas</code>, <code>matplotlib</code>, <code>numpy</code>, <code>scikit-learn</code>, <code>pytorch</code> and many more packages pre-installed.</p> <pre><code># Check installed packages in a running astroml container\nmamba list                     # conda/mamba/pip installed system packages  \napt list --installed           # OS system packages (Ubuntu)\nls /build_info/                # Container build information\n</code></pre> <p>Runtime package installation:</p> <pre><code># Install Python packages at runtime (will install to /arc/home/[user]/.local)\n# in non-astroml containers, add the --user flag.\npip install fireducks\n\n# Show where it was installed\npip show -f fireducks           # Should show ~/.local/lib/python*/\n</code></pre> <p>GPU Support</p> <p>For GPU acceleration, use the <code>astroml-cuda</code> container which extends <code>astroml</code> with CUDA libraries and GPU-enabled <code>pytorch</code>, <code>pyarrow</code>, and many other CUDA-capable libraries.</p>"},{"location":"platform/containers/#desktop-application-sessions","title":"Desktop Application Sessions","text":"<p>Desktop sessions integrate containers in two ways:</p>"},{"location":"platform/containers/#1-base-desktop-container","title":"1. Base Desktop Container","text":"<p>The core desktop container provides the Ubuntu desktop environment with Firefox, file managers, and terminals. This runs as your main desktop session as a <code>noVNC</code> web application. Each desktop application will run and communicate with this desktop session.</p>"},{"location":"platform/containers/#2-desktop-app-containers","title":"2. Desktop-App Containers","text":"<p>Specialised containers that run specific GUI applications within desktop sessions.</p> <p>Requirements for desktop-app containers:</p> <ol> <li>X11/Xorg application: Must have at least one GUI application installed and available</li> <li>Startup script: Application launcher at <code>/skaha/startup.sh</code> (if not specified, assumed to be <code>xterm</code> which must be installed in the container)</li> <li>Container labelling: Tagged as <code>desktop-app</code> in the registry</li> </ol> <p>How it works:</p> <ul> <li>Each desktop-app container runs on its own worker node</li> <li>Applications connect to your desktop session via X11 forwarding</li> <li>Shared storage provides data access across all containers</li> <li>Applications appear in the Astro Software menu</li> </ul> <pre><code># Example desktop-app startup script (/skaha/startup.sh)\n#!/bin/bash\nexport DISPLAY=${DISPLAY}\ncd /arc/home/$USER\nexec your-gui-application\n</code></pre> <p>The same astroml container can run as both notebook (has JupyterLab) and desktop-app (has xterm), demonstrating the flexibility of container usage.</p>"},{"location":"platform/containers/#batchheadless-sessions","title":"Batch/Headless Sessions","text":"<p>Headless containers execute without graphical interfaces, perfect for automated processing:</p> <ul> <li>No GUI requirements: Command-line tools only</li> <li>Script execution: Runs your specified command and exits</li> <li>Background processing: Perfect for large datasets and automation</li> <li>Resource optimization: Can use different resource priorities</li> </ul> <pre><code># Example headless execution\npython /arc/projects/[project]/scripts/reduce_data.py --input=/arc/projects/[project]/data/ --output=/arc/projects/[project]/results/\n</code></pre>"},{"location":"platform/containers/#contributed-application-sessions","title":"Contributed Application Sessions","text":"<p>Contributed applications are custom web-based tools that integrate with CANFAR:</p> <p>Requirements:</p> <ol> <li>Web service: Application serves HTTP on port 5000</li> <li>Startup script: Service launcher at <code>/skaha/startup.sh</code></li> <li>Container labelling: Tagged appropriately for discovery as contributed.</li> </ol> <p>Examples include Marimo (reactive notebooks) and VSCode (browser IDE).</p>"},{"location":"platform/containers/#storage-mounting-and-integration","title":"\ud83d\udcbe Storage Mounting and Integration","text":""},{"location":"platform/containers/#canfar-storage-integration","title":"CANFAR Storage Integration","text":"<p>CANFAR automatically mounts storage systems into your container at runtime, providing seamless access to persistent data:</p> <pre><code>graph LR\n    Container[Running Container] --&gt; ArcMount[\"/arc\"]\n\n    ArcMount --&gt; Home[\"/arc/home/[user]\"]\n    ArcMount --&gt; Projects[\"/arc/projects/[project]\"]\n\n    Container --&gt; Scratch[\"/scratch\"]\n\n    Home --&gt; HomeData[\"Personal Data 10GB Quota\"]\n    Projects --&gt; ProjectData[\"Shared Project Data Variable Quota\"]\n    Scratch --&gt; FastStorage[\"Fast Temporary Storage Node-local\"]</code></pre>"},{"location":"platform/containers/#storage-hierarchy","title":"Storage Hierarchy","text":"Mount Point Purpose Persistence Quota Sharing <code>/arc/home/[user]</code> Personal files, notebooks, configs Permanent 10GB Private <code>/arc/projects/[project]</code> Research data, collaboration Permanent Variable Team-based <code>/scratch</code> High-speed processing Session only Node-dependent Private"},{"location":"platform/containers/#storage-best-practices","title":"Storage Best Practices","text":"<p>Personal Development (<code>/arc/home</code>):</p> <pre><code>/arc/home/[user]/\n\u251c\u2500\u2500 notebooks/              # Jupyter notebooks\n\u251c\u2500\u2500 scripts/               # Analysis scripts  \n\u251c\u2500\u2500 .local/               # pip install --user packages\n\u251c\u2500\u2500 .config/              # Application configurations\n\u2514\u2500\u2500 small_datasets/       # Personal research data\n</code></pre> <p>Project Collaboration (<code>/arc/projects</code>):</p> <pre><code>/arc/projects/[project]/\n\u251c\u2500\u2500 raw_data/             # Input datasets\n\u251c\u2500\u2500 processed/            # Reduced data products\n\u251c\u2500\u2500 scripts/              # Shared analysis code\n\u251c\u2500\u2500 docs/                 # Project documentation\n\u2514\u2500\u2500 results/              # Final outputs\n</code></pre> <p>Temporary Processing (<code>/scratch</code>):</p> <pre><code># Copy large datasets to fast storage for processing\ncp /arc/projects/[project]/large_data.fits /scratch/\nprocess_data /scratch/large_data.fits /scratch/output.fits\ncp /scratch/output.fits /arc/projects/[project]/results/\n</code></pre>"},{"location":"platform/containers/#user-context-and-permissions","title":"User Context and Permissions","text":"<p>Containers run with your CADC user identity, not as root or container-defined users:</p> <pre><code># Inside any CANFAR container\nwhoami                    # [user]\nid                        # uid=1234([user]) gid=1234([user]) groups=[user](and-all-your-CADC-groups)\necho $HOME                # /arc/home/[user]\ngroups                    # Shows your CANFAR project group memberships\n</code></pre> <p>Security model:</p> <ul> <li>No root access: Containers cannot perform system administration at runtime.</li> <li>File permissions: Respect standard Unix permissions on <code>/arc</code></li> <li>Group membership: Access to <code>/arc/projects</code> based on CANFAR group membership</li> <li>Network isolation: Containers have internet access but cannot access other users' sessions</li> </ul>"},{"location":"platform/containers/#canfar-supported-containers","title":"\ud83d\udd27 CANFAR-Supported Containers","text":"<p>The CANFAR team maintains several core containers that cover most astronomy research needs in the <code>skaha</code> namespace:</p> Container Description base Basic UNIX tools, conda, CADC packages astroml Many astro (STILTS, astropy ecosystem), data sciences (pandas, pyarrow,...), machine learning (sklearn, pytorch) packages. JupyterLab, xterm. marimo Same as astroml stack, with marimo notebook as web interface vscode Same as astroml, with VSCode on browser as interface *-cuda Same as all above containers, with CUDA-enabled improc Image processing tools (SWarp, SExtractor, SourceExtractor++, IRAF, CASUTools...) casa CASA installations"},{"location":"platform/containers/#visualisation-containers","title":"Visualisation Containers","text":""},{"location":"platform/containers/#carta-radio-astronomy-visualisation","title":"<code>carta</code> - Radio Astronomy Visualisation","text":"<p>Purpose: Interactive visualisation of radio astronomy data</p> <p>Features:</p> <ul> <li>CARTA application: Cube Analysis and Rendering Tool for Astronomy</li> <li>Multi-dimensional data: Spectral cubes, moment maps, polarisation</li> <li>Interactive analysis: Region statistics, profile extraction</li> <li>Collaboration support: Session sharing capabilities</li> </ul>"},{"location":"platform/containers/#firefly-catalogue-data-analysis","title":"<code>firefly</code> - Catalogue Data Analysis","text":"<p>Purpose: Advanced catalogue queries and visualisation</p> <p>Features:</p> <ul> <li>Multi-mission support: LSST, Spitzer, WISE, 2MASS, ...</li> <li>Interactive catalogues: Source overlays and cross-matching</li> <li>Multi-wavelength workflows: RGB composites and band comparisons</li> <li>Large dataset handling: Efficient rendering of survey-scale data</li> </ul>"},{"location":"platform/containers/#development-and-desktop-containers","title":"Development and Desktop Containers","text":""},{"location":"platform/containers/#desktop-ubuntu-environment","title":"<code>desktop</code> - Ubuntu Environment","text":"<p>Purpose: Complete Linux desktop for GUI applications and legacy software. Astronomy software applications will each run on dedicated nodes.</p> <p>Features:</p> <ul> <li>Ubuntu: Linux environment</li> <li>Desktop environment: Full GNOME-based interface</li> <li>Applications: Firefox, file managers, terminals, editors</li> <li>X11 forwarding: Support for launching astronomy GUI applications</li> </ul>"},{"location":"platform/containers/#notebook-jupyter-environment","title":"<code>notebook</code> - Jupyter Environment","text":"<p>Purpose: Minimal Jupyter environment for basic Python work</p> <p>Features:</p> <ul> <li>Jupyter Lab: Web-based notebook interface</li> <li>Extensible: Foundation for custom development</li> <li>Fast startup: Minimal software for quick sessions</li> </ul>"},{"location":"platform/containers/#container-selection-guide","title":"Container Selection Guide","text":"Workflow Type Recommended Container Session Type Typical Resources Python core <code>base</code> Headless 1 core, 1GB Python data analysis <code>astroml</code> Notebook 2-4 cores, 8-16GB GPU machine learning <code>astroml-cuda</code> Notebook 4-8 cores, 16-32GB, 1 GPU Radio interferometry <code>casa</code> Notebook/Desktop 4-8 cores, 16-32GB Data visualisation <code>carta</code> CARTA session 2-4 cores, 8-16GB Catalogue analysis <code>firefly</code> Firefly session 2-4 cores, 8-16GB GUI applications <code>desktop</code> Desktop 2-4 cores, 8-16GB Legacy software <code>desktop</code> Desktop Variable Batch processing <code>astroml</code> or <code>casa</code> Headless Variable <p>Container Selection Strategy</p> <p>Start with <code>astroml</code> for most astronomy work. It includes comprehensive libraries and is actively maintained. Use specialised containers (<code>casa</code>, <code>carta</code>, <code>firefly</code>) only when you need their specific tools.</p>"},{"location":"platform/containers/#version-management","title":"Version Management","text":"<p>CANFAR containers follow semantic versioning:</p> <ul> <li><code>:latest</code> - Current stable release (recommended for most work)</li> <li><code>:YY.MM</code> - Monthly snapshots for reproducibility</li> <li><code>:commit-hash</code> - Specific builds for exact reproducibility</li> </ul> <pre><code># Use latest stable version (recommended)\nimages.canfar.net/skaha/astroml:latest\n\n# Use specific monthly snapshot for reproducible research\nimages.canfar.net/skaha/astroml:25.09\n\n# Use exact commit for critical reproducibility\nimages.canfar.net/skaha/astroml:a1b2c3d4\n</code></pre>"},{"location":"platform/containers/#container-updates-and-maintenance","title":"Container Updates and Maintenance","text":"<p>CANFAR containers receive regular updates:</p> <p>Monthly releases: Security patches, library updates, new features Quarterly reviews: Major version updates, new software additions Community feedback: Feature requests and bug reports incorporated</p> <p>Update notifications:</p> <ul> <li>Science Portal notifications for major changes</li> </ul> <p>Version Pinning for Reproducibility</p> <p>For published research, specify exact container versions (monthly tags or commit hashes) to ensure long-term reproducibility of your analysis.</p>"},{"location":"platform/containers/build/","title":"Building Custom Containers","text":"<p>Creating your own astronomy software environments for CANFAR - from development through deployment and maintenance.</p> <p>\ud83c\udfaf Container Building Overview</p> <p>Master custom container development:</p> <ul> <li>Development Setup: Local environments for container building and testing</li> <li>CANFAR Requirements: Platform-specific configurations and best practices</li> <li>Testing &amp; Debugging: Ensuring containers work correctly in CANFAR sessions</li> <li>Harbor Registry: Publishing and maintaining custom containers</li> </ul> <p>Building custom containers becomes necessary when existing CANFAR containers don't meet your specific software requirements or when creating standardised environments for research teams. This guide covers the complete development workflow from initial setup through deployment and maintenance.</p>"},{"location":"platform/containers/build/#when-to-build-custom-containers","title":"\ud83d\udccb When to Build Custom Containers","text":""},{"location":"platform/containers/build/#scenarios-requiring-custom-containers","title":"Scenarios Requiring Custom Containers","text":"<p>Missing Software Packages: - Proprietary or licensed software not available in public containers - Cutting-edge research tools not yet in CANFAR containers - Specific versions of software required for reproducibility - Legacy software with complex dependency requirements</p> <p>Team Standardization: - Consistent environments across research groups - Custom analysis pipelines and workflows - Institutional software licensing requirements - Project-specific data processing tools</p> <p>Performance Optimization: - GPU-optimised builds for specific hardware - Memory-efficient configurations for large datasets - Custom compilation flags for scientific software - Minimized container size for batch processing</p>"},{"location":"platform/containers/build/#alternatives-to-consider-first","title":"Alternatives to Consider First","text":"<p>Before building custom containers, consider these alternatives:</p> <pre><code># Runtime package installation (temporary)\npip install --user new-package          # Installs to /arc/home/[user]/.local/\nmamba install -c conda-forge package    # If mamba or conda is available\n\n# Development in existing containers\n# Use astroml as base and install packages per session\n# Keep development scripts in /arc/home/ or /arc/projects/\n</code></pre> <p>Start Simple</p> <p>Try adding software to existing containers at runtime first. Only build custom containers when you need permanent, reproducible environments or when runtime installation isn't feasible.</p>"},{"location":"platform/containers/build/#development-environment-setup","title":"\ud83d\udee0\ufe0f Development Environment Setup","text":""},{"location":"platform/containers/build/#local-development-prerequisites","title":"Local Development Prerequisites","text":"<p>Required software: - Docker Desktop or Docker Engine - Git for version control - Text editor or IDE (VS Code recommended for Dockerfile support) - Terminal access for command-line operations</p> <p>CANFAR-specific requirements: - Harbor registry access (images.canfar.net) - Understanding of CANFAR storage mounting (<code>/arc/</code>, <code>/scratch/</code>) - Knowledge of target session types (notebook, desktop-app, headless)</p>"},{"location":"platform/containers/build/#development-workflow-setup","title":"Development Workflow Setup","text":"<p>Create a structured development environment:</p> <pre><code># Set up development directory\nmkdir ~/canfar-containers\ncd ~/canfar-containers\n\n# Create container project\nmkdir my-analysis-container\ncd my-analysis-container\n\n# Initialize version control\ngit init\ngit remote add origin https://github.com/myteam/my-analysis-container.git\n\n# Create basic structure\ntouch Dockerfile\ntouch README.md\nmkdir scripts/\nmkdir tests/\nmkdir docs/\n\n# Create test data directories (for local testing)\nmkdir test-data/\nmkdir test-home/\n</code></pre> <p>Recommended project structure: <pre><code>my-analysis-container/\n\u251c\u2500\u2500 Dockerfile              # Container definition\n\u251c\u2500\u2500 README.md               # Documentation and usage\n\u251c\u2500\u2500 requirements.txt        # Python dependencies\n\u251c\u2500\u2500 environment.yml         # Conda environment (if using)\n\u251c\u2500\u2500 scripts/               # Custom scripts to include\n\u251c\u2500\u2500 tests/                 # Container functionality tests\n\u251c\u2500\u2500 docs/                  # Additional documentation\n\u251c\u2500\u2500 test-data/             # Sample data for testing\n\u251c\u2500\u2500 test-home/             # Mock user home for testing\n\u2514\u2500\u2500 .github/workflows/     # CI/CD automation (optional)\n</code></pre></p>"},{"location":"platform/containers/build/#container-development-process","title":"\ud83c\udfd7\ufe0f Container Development Process","text":""},{"location":"platform/containers/build/#starting-from-canfar-base-images","title":"Starting from CANFAR Base Images","text":"<p>Always extend existing CANFAR base images rather than starting from scratch:</p> <pre><code># For general astronomy work\nFROM images.canfar.net/skaha/astroml:latest\n\n# For radio astronomy\nFROM images.canfar.net/skaha/casa:[version]\n\n# For minimal environments\nFROM images.canfar.net/skaha/base:latest\n</code></pre>"},{"location":"platform/containers/build/#basic-dockerfile-patterns","title":"Basic Dockerfile Patterns","text":""},{"location":"platform/containers/build/#notebook-container-extension","title":"Notebook Container Extension","text":"<pre><code>FROM images.canfar.net/skaha/astroml:latest\n\n# Container metadata\nLABEL maintainer=\"research-team@university.edu\"\nLABEL description=\"Custom astronomy analysis environment with X-ray tools\"\nLABEL version=\"1.0.0\"\n\n# Install system dependencies as root\nUSER root\n\n\n# Install specialized X-ray analysis tools\nRUN pip install --no-cache-dir \\\n    xspec-models-cxc \\\n    pyxspec \\\n    sherpa\n\n# Install custom analysis tools from source\nRUN git clone https://github.com/myteam/xray-analysis-tools.git /tmp/tools &amp;&amp; \\\n    cd /tmp/tools &amp;&amp; \\\n    pip install --no-cache-dir -e . &amp;&amp; \\\n    rm -rf /tmp/tools\n\n# Set up environment variables\nENV XRAY_TOOLS_PATH=/opt/custom-tools\nENV PYTHONPATH=${PYTHONPATH}:/opt/custom-tools\n</code></pre>"},{"location":"platform/containers/build/#desktop-app-container","title":"Desktop-App Container","text":"<pre><code>FROM ubuntu:24.04\n\n# Avoid prompts during installation\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    # X11 and GUI libraries\n    libx11-6 \\\n    libxext6 \\\n    libxrender1 \\\n    libxtst6 \\\n    libxrandr2 \\\n    libxss1 \\\n    libgtk-3-0 \\\n    saods9 \\\n    xterm \\\n    wget \\\n    curl \\\n    vim \\\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Create startup script for CANFAR desktop integration\nRUN mkdir -p /skaha\n\n# Create the startup script\nCOPY startup.sh /skaha/ \n\n# Make startup script executable\nRUN chmod +x /skaha/startup.sh\n\n# Set the startup script as entrypoint\nENTRYPOINT [\"/skaha/startup.sh\"]\n</code></pre> <p>with the <code>startup.sh</code> being:</p> <pre><code>#!/bin/bash\n\n# Set up X11 environment\nexport DISPLAY=${DISPLAY:-:1}\n\n# Navigate to user home directory\ncd /arc/home/$USER || cd /tmp\n\n# Launch the application\nexec ds9 -title \"Custom DS9 - $USER\" &amp;\n\n# Keep container running\nwait\n</code></pre>"},{"location":"platform/containers/build/#advanced-container-features","title":"Advanced Container Features","text":""},{"location":"platform/containers/build/#multi-stage-builds-for-complex-software","title":"Multi-stage Builds for Complex Software","text":"<pre><code># Build stage for compiling software\nFROM ubuntu:24.04 AS builder\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    cmake \\\n    git \\\n    libfftw3-dev \\\n    libcfitsio-dev\n\n# Clone and build complex software\nRUN git clone https://github.com/radio-astro/complex-software.git /src\nWORKDIR /src\nRUN cmake . &amp;&amp; make -j$(nproc) &amp;&amp; make install\n\n# Production stage\nFROM images.canfar.net/skaha/astroml:latest\n\n# Copy only the built binaries\nCOPY --from=builder /usr/local/bin/complex-software /usr/local/bin/\nCOPY --from=builder /usr/local/lib/libcomplex* /usr/local/lib/\n\n# Update library cache\nUSER root\nRUN ldconfig\n</code></pre>"},{"location":"platform/containers/build/#gpu-enabled-containers","title":"GPU-Enabled Containers","text":"<pre><code>FROM images.canfar.net/skaha/astroml-cuda:latest\n\n# Install additional GPU-accelerated packages\nRUN pip install --no-cache-dir \\\n    # GPU-accelerated arrays\n    cupy-cuda11x \\\n    # GPU machine learning\n    rapids-singlecell \\\n    # GPU image processing\n    cucim \\\n    # GPU signal processing\n    cusignal\n\nRUN cd /opt/cuda_kernels &amp;&amp; \\\n    nvcc -o gpu_analysis analysis.cu -lcufft -lcublas\n</code></pre>"},{"location":"platform/containers/build/#testing-and-debugging","title":"\ud83e\uddea Testing and Debugging","text":""},{"location":"platform/containers/build/#local-testing-strategy","title":"Local Testing Strategy","text":"<p>Test containers thoroughly before deploying to CANFAR:</p> <pre><code># Build container locally\ndocker build -t myteam/analysis-env:test .\n\n# Test basic functionality\ndocker run --rm myteam/analysis-env:test python -c \"import astropy; print('Astropy works!')\"\n\n# Test with mounted directories (simulate CANFAR environment)\ndocker run -it --rm \\\n  -v $(pwd)/test-data:/arc/projects/test \\\n  -v $(pwd)/test-home:/arc/home/testuser \\\n  -e USER=testuser \\\n  -e HOME=/arc/home/testuser \\\n  myteam/analysis-env:test \\\n  /bin/bash\n</code></pre>"},{"location":"platform/containers/build/#testing-notebook-containers","title":"Testing Notebook Containers","text":"<pre><code># Test Jupyter startup\ndocker run -it --rm \\\n  -p 8888:8888 \\\n  -v $(pwd)/test-notebooks:/arc/home/testuser \\\n  -e USER=testuser \\\n  myteam/analysis-env:test \\\n  jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n</code></pre>"},{"location":"platform/containers/build/#testing-desktop-app-containers","title":"Testing Desktop-App Containers","text":"<pre><code># Test X11 application (requires X11 forwarding setup)\ndocker run -it --rm \\\n  -e DISPLAY=${DISPLAY} \\\n  -v /tmp/.X11-unix:/tmp/.X11-unix \\\n  myteam/desktop-app:test \\\n  xterm\n</code></pre>"},{"location":"platform/containers/build/#automated-testing-framework","title":"Automated Testing Framework","text":"<p>Create test scripts to validate container functionality:</p> <pre><code># tests/test_container.py\nimport subprocess\nimport pytest\n\ndef test_python_packages():\n    \"\"\"Test that required Python packages are installed.\"\"\"\n    packages = ['astropy', 'numpy', 'scipy', 'matplotlib']\n\n    for package in packages:\n        result = subprocess.run([\n            'docker', 'run', '--rm', 'myteam/analysis-env:test',\n            'python', '-c', f'import {package}; print(f\"{package} version: {{package.__version__}}\")'\n        ], capture_output=True, text=True)\n\n        assert result.returncode == 0, f\"Package {package} not available\"\n        print(result.stdout)\n\ndef test_custom_scripts():\n    \"\"\"Test that custom analysis scripts work.\"\"\"\n    result = subprocess.run([\n        'docker', 'run', '--rm', 'myteam/analysis-env:test',\n        'python', '/opt/custom-tools/test_analysis.py'\n    ], capture_output=True, text=True)\n\n    assert result.returncode == 0, \"Custom analysis script failed\"\n    assert \"Analysis completed\" in result.stdout\n\ndef test_file_permissions():\n    \"\"\"Test that file permissions work correctly.\"\"\"\n    result = subprocess.run([\n        'docker', 'run', '--rm',\n        '-v', '$(pwd)/test-data:/arc/projects/test',\n        'myteam/analysis-env:test',\n        'ls', '-la', '/arc/projects/test'\n    ], capture_output=True, text=True)\n\n    assert result.returncode == 0, \"Cannot access mounted directories\"\n\nif __name__ == \"__main__\":\n    pytest.main([__file__])\n</code></pre>"},{"location":"platform/containers/build/#debugging-common-issues","title":"Debugging Common Issues","text":""},{"location":"platform/containers/build/#package-installation-failures","title":"Package Installation Failures","text":"<pre><code># Clean package caches to reduce image size and avoid corruption\nRUN apt-get update &amp;&amp; apt-get install -y package1 package2 \\\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/* /var/cache/apt/* /var/tmp/*\n\n# if necessary, you can pin exact package versions to avoid conflicts\nRUN pip install --no-cache-dir \\\n    astropy==5.3.4 \\\n    numpy==1.24.3 \\\n    scipy==1.10.1\n</code></pre>"},{"location":"platform/containers/build/#container-size-issues","title":"Container Size Issues","text":"<pre><code># Use multi-stage builds\nFROM ubuntu:24.04 AS builder\n# ... build software ...\n\nFROM images.canfar.net/skaha/astroml:latest\nCOPY --from=builder /output /final-location\n\n# Minimize layers\nRUN apt-get update &amp;&amp; apt-get install -y pkg1 pkg2 pkg3 &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n# Instead of:\n# RUN apt-get update\n# RUN apt-get install -y pkg1\n# RUN apt-get install -y pkg2\n</code></pre>"},{"location":"platform/containers/build/#building-and-optimization","title":"\ud83d\udce6 Building and Optimization","text":""},{"location":"platform/containers/build/#efficient-docker-practices","title":"Efficient Docker Practices","text":""},{"location":"platform/containers/build/#layer-optimization","title":"Layer Optimization","text":"<pre><code># Good: Combine related operations\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    package1 \\\n    package2 \\\n    package3 \\\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Good: Order layers by change frequency\nFROM base-image\n# System packages (change rarely)\nRUN apt-get update &amp;&amp; apt-get install -y system-packages\n# Python packages (change occasionally)  \nRUN pip install stable-packages\n# Custom code (changes frequently)\nCOPY . /app/\n</code></pre>"},{"location":"platform/containers/build/#size-minimization","title":"Size Minimization","text":"<pre><code># Use .dockerignore to exclude unnecessary files\n# .dockerignore contents:\n# .git\n# *.md\n# tests/\n# docs/\n# .DS_Store\n# __pycache__\n\n# Clean up in same layer\nRUN apt-get update &amp;&amp; apt-get install -y packages \\\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/* \\\n    &amp;&amp; rm -rf /tmp/* /var/tmp/*\n\n# Use --no-cache-dir for pip\nRUN pip install --no-cache-dir package-name\n</code></pre>"},{"location":"platform/containers/build/#performance-optimization","title":"Performance Optimization","text":""},{"location":"platform/containers/build/#parallel-builds","title":"Parallel Builds","text":"<pre><code># Build with multiple cores\ndocker build --build-arg MAKEFLAGS=-j$(nproc) .\n\n# Use BuildKit for faster builds\nexport DOCKER_BUILDKIT=1\ndocker build .\n</code></pre>"},{"location":"platform/containers/build/#build-arguments-for-flexibility","title":"Build Arguments for Flexibility","text":"<pre><code># Flexible package versions\nARG PYTHON_VERSION=3.11\nARG ASTROPY_VERSION=5.3.4\n\nFROM python:${PYTHON_VERSION}-slim\n\nRUN pip install --no-cache-dir astropy==${ASTROPY_VERSION}\n</code></pre> <pre><code># Build with custom arguments\ndocker build --build-arg PYTHON_VERSION=3.10 --build-arg ASTROPY_VERSION=5.2.0 .\n</code></pre>"},{"location":"platform/containers/build/#version-management-and-tagging","title":"Version Management and Tagging","text":"<pre><code># Build with specific tags\ndocker build -t myteam/analysis-env:latest .\ndocker build -t myteam/analysis-env:v1.2.3 .\ndocker build -t myteam/analysis-env:2024.03 .\n\n# Tag for Harbor registry\ndocker tag myteam/analysis-env:latest images.canfar.net/myteam/analysis-env:latest\ndocker tag myteam/analysis-env:v1.2.3 images.canfar.net/myteam/analysis-env:v1.2.3\n</code></pre>"},{"location":"platform/containers/build/#publishing-to-harbor-registry","title":"\ud83d\ude80 Publishing to Harbor Registry","text":""},{"location":"platform/containers/build/#registry-authentication","title":"Registry Authentication","text":"<pre><code># Login to CANFAR Harbor registry\ndocker login images.canfar.net\n\n# Or use credentials directly\necho \"your-harbor-password\" | docker login images.canfar.net -u your-harbor-username --password-stdin\n</code></pre>"},{"location":"platform/containers/build/#pushing-images","title":"Pushing Images","text":"<pre><code># Push specific version\ndocker push images.canfar.net/myteam/analysis-env:v1.2.3\n\n# Push latest\ndocker push images.canfar.net/myteam/analysis-env:latest\n\n# Push all tags\ndocker push --all-tags images.canfar.net/myteam/analysis-env\n</code></pre>"},{"location":"platform/containers/registry/","title":"Harbor Container Registry","text":"<p>Managing container images on CANFAR's Harbor-based registry for secure distribution and deployment.</p> <p>\ud83c\udfaf Registry Management Overview</p> <p>Master Harbor registry operations:</p> <ul> <li>Harbor Platform: Understanding CANFAR's enterprise container registry features</li> <li>Access Control: Projects, repositories, and role-based permissions</li> <li>Image Management: Tagging, versioning, and metadata organization</li> <li>Security Features: Vulnerability scanning and compliance monitoring</li> </ul> <p>Harbor serves as CANFAR's container registry, providing a secure, feature-rich platform for storing, managing, and distributing container images. Built on Docker Registry v2, Harbor adds enterprise features like role-based access control, vulnerability scanning, and image replication.</p>"},{"location":"platform/containers/registry/#harbor-registry-overview","title":"\ud83d\udccb Harbor Registry Overview","text":""},{"location":"platform/containers/registry/#what-is-harbor","title":"What is Harbor?","text":"<p>Harbor is an open-source container registry that provides:</p> <ul> <li>Secure storage: Role-based access control and authentication integration</li> <li>Image management: Tagging, versioning, and metadata handling</li> <li>Vulnerability scanning: Automated security analysis of container images</li> <li>Replication: Multi-site synchronisation and backup capabilities</li> <li>Web interface: User-friendly management portal</li> <li>API access: Programmatic integration with development workflows</li> </ul>"},{"location":"platform/containers/registry/#canfar-harbor-instance","title":"CANFAR Harbor Instance","text":"<p>Registry URL: https://images.canfar.net</p> <p>Key features on CANFAR:</p> <ul> <li>Integration with CADC authentication system</li> <li>Project-based organization for research teams</li> <li>Automated vulnerability scanning for public containers</li> <li>Role-based permissions aligned with CANFAR groups</li> <li>API access for automated workflows</li> </ul> <pre><code>graph TB\n    Users[CANFAR Users] --&gt; Harbor[Harbor Registry]\n    Harbor --&gt; Projects[Projects/Organizations]\n\n    Projects --&gt; Public[Public Projects]\n    Projects --&gt; Private[Private Projects]\n\n    Public --&gt; CADCContainers[CADC/Official Containers]\n    Public --&gt; CommunityContainers[Community Containers]\n\n    Private --&gt; TeamProjects[Research Team Projects]\n    Private --&gt; PersonalProjects[Individual Projects]\n\n    Harbor --&gt; Features[Harbor Features]\n    Features --&gt; RBAC[Role-Based Access]\n    Features --&gt; Scanning[Vulnerability Scanning]\n    Features --&gt; Replication[Image Replication]\n    Features --&gt; API[REST API]</code></pre>"},{"location":"platform/containers/registry/#projects-and-organization","title":"\ud83c\udfd7\ufe0f Projects and Organization","text":""},{"location":"platform/containers/registry/#project-structure","title":"Project Structure","text":"<p>Harbor organizes containers into projects, which serve as top-level namespaces:</p> <pre><code>images.canfar.net/\n\u251c\u2500\u2500 skaha/                    # Official CANFAR containers\n\u2502   \u251c\u2500\u2500 astroml:latest\n\u2502   \u251c\u2500\u2500 casa:latest\n\u2502   \u2514\u2500\u2500 desktop:latest\n\u251c\u2500\u2500 [project]/                # Research team project\n\u2502   \u251c\u2500\u2500 custom-pipeline:latest\n\u2502   \u2514\u2500\u2500 analysis-env:v2.1\n\u2514\u2500\u2500 [user]/                   # Personal project\n    \u251c\u2500\u2500 development:latest\n    \u2514\u2500\u2500 testing:experimental\n</code></pre>"},{"location":"platform/containers/registry/#project-types","title":"Project Types","text":""},{"location":"platform/containers/registry/#public-projects","title":"Public Projects","text":"<p>Characteristics:</p> <ul> <li>Visible to all CANFAR users</li> <li>Images can be pulled without authentication</li> <li>Suitable for community-shared tools</li> <li>Used for official CANFAR containers</li> </ul> <p>Examples:</p> <ul> <li><code>skaha/</code> - Core CANFAR containers</li> <li><code>lsst/</code> - LSST Community-contributed containers</li> </ul>"},{"location":"platform/containers/registry/#private-projects","title":"Private Projects","text":"<p>Characteristics:</p> <ul> <li>Access restricted to project members</li> <li>Require authentication for all operations</li> <li>Support proprietary or sensitive software</li> <li>Can be shared with specific research teams</li> </ul> <p>Examples:</p> <ul> <li><code>myuniversity-xray/</code> - Institutional X-ray analysis tools</li> <li><code>survey-collaboration/</code> - Multi-institutional survey project</li> <li><code>proprietary-software/</code> - Licensed commercial software</li> </ul>"},{"location":"platform/containers/registry/#project-creation-and-management","title":"Project Creation and Management","text":""},{"location":"platform/containers/registry/#requesting-new-projects","title":"Requesting New Projects","text":"<p>Contact support@canfar.net to create new projects:</p> <p>Required information:</p> <ul> <li>Project name (must be unique, lowercase, alphanumeric)</li> <li>Description and purpose</li> <li>Visibility (public/private)</li> <li>Initial project members and their roles</li> <li>Resource requirements (storage quota)</li> </ul> <p>Naming conventions:</p> <pre><code># Good project names\nmyteam-radio-analysis\nsurvey-processing-tools\nxray-spectroscopy\n\n# Avoid\nMyTeam_Radio_Analysis    # Mixed case, underscores\nmy team radio            # Spaces\nspecial-chars-@#$        # Special characters\n</code></pre>"},{"location":"platform/containers/registry/#project-membership-management","title":"Project Membership Management","text":"<p>Project owners can manage membership through the Harbor web interface:</p> <p>Role levels:</p> <ul> <li>Guest: Pull images only</li> <li>Developer: Pull and push images, manage tags</li> <li>Master: Full project management, member administration</li> <li>ProjectAdmin: Complete project control including deletion</li> </ul>"},{"location":"platform/containers/registry/#repository-management","title":"\ud83d\uddc2\ufe0f Repository Management","text":""},{"location":"platform/containers/registry/#understanding-repositories","title":"Understanding Repositories","text":"<p>Within each project, repositories contain the actual container images:</p> <pre><code>[project]/                       # Project\n\u251c\u2500\u2500 analysis-pipeline/        # Repository\n\u2502   \u251c\u2500\u2500 latest               # Tag\n\u2502   \u251c\u2500\u2500 v1.0.0               # Tag\n\u2502   \u2514\u2500\u2500 2024.03              # Tag\n\u2514\u2500\u2500 visualization-tools/      # Repository\n    \u251c\u2500\u2500 latest               # Tag\n    \u2514\u2500\u2500 beta                 # Tag\n</code></pre>"},{"location":"platform/containers/registry/#repository-naming","title":"Repository Naming","text":"<p>Follow consistent naming conventions for repositories:</p> <pre><code># Good repository names\nanalysis-pipeline\ndata-processing-tools\nvisualization-suite\nradio-astronomy-env\n\n# Specific use cases\nsurvey-reduction-v2\nxray-spectral-analysis\nmachine-learning-gpu\n</code></pre>"},{"location":"platform/containers/registry/#tagging-strategy","title":"Tagging Strategy","text":"<p>Implement systematic tagging for version control:</p>"},{"location":"platform/containers/registry/#semantic-versioning","title":"Semantic Versioning","text":"<pre><code># Major.Minor.Patch format\nmyteam/analysis-env:1.0.0     # Initial release\nmyteam/analysis-env:1.1.0     # New features\nmyteam/analysis-env:1.1.1     # Bug fixes\nmyteam/analysis-env:2.0.0     # Breaking changes\n</code></pre>"},{"location":"platform/containers/registry/#date-based-versioning","title":"Date-Based Versioning","text":"<pre><code># Monthly releases\nmyteam/analysis-env:2024.03   # March 2024 release\nmyteam/analysis-env:2024.04   # April 2024 release\n\n# Daily builds (development)\nmyteam/analysis-env:2024.03.15\nmyteam/analysis-env:2024.03.16\n</code></pre>"},{"location":"platform/containers/registry/#feature-and-environment-tags","title":"Feature and Environment Tags","text":"<pre><code># Environment-specific\nmyteam/analysis-env:production\nmyteam/analysis-env:development\nmyteam/analysis-env:testing\n\n# Feature branches\nmyteam/analysis-env:feature-gpu-support\nmyteam/analysis-env:experimental-ml\n\n# Special purpose\nmyteam/analysis-env:conference-demo\nmyteam/analysis-env:paper-reproduction\n</code></pre>"},{"location":"platform/containers/registry/#managing-image-metadata","title":"Managing Image Metadata","text":"<p>Harbor stores rich metadata for each image:</p> <pre><code>{\n  \"name\": \"analysis-env\",\n  \"version\": \"v1.2.0\", \n  \"description\": \"Custom astronomy analysis environment\",\n  \"created\": \"2024-03-15T10:30:00Z\",\n  \"size\": \"2.1GB\",\n  \"labels\": {\n    \"maintainer\": \"research-team@university.edu\",\n    \"version\": \"1.2.0\",\n    \"description\": \"X-ray astronomy analysis with XSPEC\",\n    \"ca.nrc.cadc.skaha.type\": \"notebook\"\n  },\n  \"vulnerabilities\": \"scanned\",\n  \"signature\": \"verified\"\n}\n</code></pre>"},{"location":"platform/containers/registry/#access-control-and-security","title":"\ud83d\udd10 Access Control and Security","text":""},{"location":"platform/containers/registry/#authentication-methods","title":"Authentication Methods","text":""},{"location":"platform/containers/registry/#web-interface-access","title":"Web Interface Access","text":"<ol> <li>Visit: https://images.canfar.net</li> <li>Login: Use your CADC credentials</li> <li>Navigate: Browse projects and repositories</li> <li>Manage: Create, tag, and delete images (with appropriate permissions)</li> </ol>"},{"location":"platform/containers/registry/#docker-cli-authentication","title":"Docker CLI Authentication","text":"<pre><code># Login to Harbor registry\ndocker login images.canfar.net\n# Enter CADC username and password when prompted\n# Paste the Habor CLI Secret copied from the images.canfar.net User Profile-&gt;\n\n# Verify authentication\ndocker info | grep -A 5 \"Registry Mirrors\"\n</code></pre>"},{"location":"platform/containers/registry/#api-access","title":"API Access","text":"<pre><code># Get authentication token\ncurl -X POST \"https://images.canfar.net/api/v2.0/users/current\" \\\n  -u \"username:password\" \\\n  -H \"accept: application/json\"\n\n# Use token for API calls\ncurl -X GET \"https://images.canfar.net/api/v2.0/projects\" \\\n  -H \"Authorization: Bearer YOUR_TOKEN\" \\\n  -H \"accept: application/json\"\n</code></pre>"},{"location":"platform/containers/registry/#permission-matrix","title":"Permission Matrix","text":"Action Guest Developer Master ProjectAdmin View public projects \u2705 \u2705 \u2705 \u2705 View private projects \u274c \u2705* \u2705* \u2705* Pull images \u2705** \u2705 \u2705 \u2705 Push images \u274c \u2705 \u2705 \u2705 Delete images \u274c \u274c \u2705 \u2705 Manage tags \u274c \u2705 \u2705 \u2705 Scan images \u274c \u2705 \u2705 \u2705 Add/remove members \u274c \u274c \u2705 \u2705 Delete project \u274c \u274c \u274c \u2705 <p>*Only if member of project **Public images only</p>"},{"location":"platform/containers/registry/#harbor-web-interface","title":"\ud83d\udd0d Harbor Web Interface","text":""},{"location":"platform/containers/registry/#navigation-and-features","title":"Navigation and Features","text":""},{"location":"platform/containers/registry/#project-dashboard","title":"Project Dashboard","text":"<p>The project dashboard provides an overview of:</p> <ul> <li>Repositories: List of container repositories</li> <li>Members: Project access control</li> <li>Logs: Activity and audit trail  </li> <li>Configuration: Project settings and policies</li> <li>Summary: Storage usage and statistics</li> </ul>"},{"location":"platform/containers/registry/#repository-view","title":"Repository View","text":"<p>For each repository, you can:</p> <ul> <li>Browse tags: View all available versions</li> <li>Inspect images: Examine layers, metadata, and vulnerabilities</li> <li>Manage artifacts: Add/remove tags, delete images</li> <li>View history: Track changes and updates</li> <li>Configure policies: Set retention and scanning rules</li> </ul>"},{"location":"platform/containers/registry/#image-details","title":"Image Details","text":"<p>Each image provides detailed information:</p> <pre><code>Image Information:\n  Digest: sha256:a1b2c3d4e5f6...\n  Size: 2.1 GB\n  Created: 2024-03-15 10:30:00 UTC\n  OS/Arch: linux/amd64\n\nLabels:\n  maintainer: research-team@university.edu\n  version: 1.2.0\n  ca.nrc.cadc.skaha.type: notebook\n\nVulnerabilities:\n  Critical: 0\n  High: 1\n  Medium: 3\n  Low: 12\n\nBuild Information:\n  Dockerfile: Available\n  Build Args: Recorded\n  Layers: 15 layers, optimized\n</code></pre>"},{"location":"platform/containers/registry/#vulnerability-scanning","title":"Vulnerability Scanning","text":"<p>Harbor automatically scans container images for security vulnerabilities:</p>"},{"location":"platform/containers/registry/#scanning-process","title":"Scanning Process","text":"<ol> <li>Automatic scanning: Public containers scanned on push</li> <li>Manual scanning: Trigger scans for private repositories</li> <li>Scheduled scanning: Regular updates with latest vulnerability database</li> <li>Policy enforcement: Block pulls based on vulnerability thresholds</li> </ol>"},{"location":"platform/containers/registry/#vulnerability-reports","title":"Vulnerability Reports","text":"<pre><code>Vulnerability Report:\n  Scanner: Trivy\n  Scan Time: 2024-03-15 11:00:00 UTC\n  Database Version: 2024-03-14\n\nCritical Vulnerabilities:\n  - None found\n\nHigh Vulnerabilities:\n  - CVE-2024-1234: OpenSSL vulnerability\n    Severity: High\n    Package: openssl 3.0.1\n    Fix: Upgrade to openssl 3.0.2\n\nMedium Vulnerabilities:\n  - CVE-2024-5678: Python vulnerability\n    Severity: Medium  \n    Package: python 3.11.1\n    Fix: Upgrade to python 3.11.2\n</code></pre>"},{"location":"platform/containers/registry/#addressing-vulnerabilities","title":"Addressing Vulnerabilities","text":"<pre><code># Update base image to address vulnerabilities\nFROM images.canfar.net/skaha/astroml:latest\n\n# Update system packages\nUSER root\nRUN apt-get update &amp;&amp; apt-get upgrade -y \\\n    &amp;&amp; apt-get clean \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Update Python packages\nRUN pip install --upgrade package-with-vulnerability\n</code></pre>"},{"location":"platform/containers/registry/#cli-and-api-usage","title":"\ud83d\udee0\ufe0f CLI and API Usage","text":""},{"location":"platform/containers/registry/#harbor-cli-operations","title":"Harbor CLI Operations","text":""},{"location":"platform/containers/registry/#basic-image-operations","title":"Basic Image Operations","text":"<pre><code># List repositories in a project\ncurl -X GET \"https://images.canfar.net/api/v2.0/projects/myteam/repositories\" \\\n  -H \"Authorization: Basic $(echo -n username:password | base64)\"\n\n# Get repository information\ncurl -X GET \"https://images.canfar.net/api/v2.0/projects/myteam/repositories/analysis-env\" \\\n  -H \"Authorization: Basic $(echo -n username:password | base64)\"\n\n# List tags for a repository\ncurl -X GET \"https://images.canfar.net/api/v2.0/projects/myteam/repositories/analysis-env/artifacts\" \\\n  -H \"Authorization: Basic $(echo -n username:password | base64)\"\n</code></pre>"},{"location":"platform/containers/registry/#docker-registry-v2-api","title":"Docker Registry v2 API","text":"<pre><code># Get manifest for specific tag\ncurl -X GET \"https://images.canfar.net/v2/myteam/analysis-env/manifests/latest\" \\\n  -H \"Accept: application/vnd.docker.distribution.manifest.v2+json\" \\\n  -H \"Authorization: Basic $(echo -n username:password | base64)\"\n\n# Get blob (layer) information\ncurl -X GET \"https://images.canfar.net/v2/myteam/analysis-env/blobs/sha256:digest\" \\\n  -H \"Authorization: Basic $(echo -n username:password | base64)\"\n</code></pre>"},{"location":"platform/containers/registry/#automated-workflows","title":"Automated Workflows","text":""},{"location":"platform/containers/registry/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># GitHub Actions example\nname: Build and Push to Harbor\n\non:\n  push:\n    branches: [main]\n    tags: ['v*']\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Login to Harbor\n      uses: docker/login-action@v3\n      with:\n        registry: images.canfar.net\n        username: ${{ secrets.HARBOR_USERNAME }}\n        password: ${{ secrets.HARBOR_PASSWORD }}\n\n    - name: Extract metadata\n      id: meta\n      uses: docker/metadata-action@v5\n      with:\n        images: images.canfar.net/myteam/analysis-env\n        tags: |\n          type=ref,event=branch\n          type=semver,pattern={{version}}\n          type=raw,value=latest,enable={{is_default_branch}}\n\n    - name: Build and push\n      uses: docker/build-push-action@v5\n      with:\n        push: true\n        tags: ${{ steps.meta.outputs.tags }}\n        labels: ${{ steps.meta.outputs.labels }}\n</code></pre>"},{"location":"platform/containers/registry/#automated-scanning-and-deployment","title":"Automated Scanning and Deployment","text":"<pre><code>#!/usr/bin/env python3\n# automated_scan_deploy.py\n\nimport requests\nimport time\nimport sys\nfrom base64 import b64encode\n\nclass HarborManager:\n    def __init__(self, registry_url, username, password):\n        self.registry_url = registry_url\n        self.auth_header = self._create_auth_header(username, password)\n\n    def _create_auth_header(self, username, password):\n        credentials = f\"{username}:{password}\"\n        encoded_credentials = b64encode(credentials.encode()).decode()\n        return {\"Authorization\": f\"Basic {encoded_credentials}\"}\n\n    def scan_repository(self, project, repository):\n        \"\"\"Trigger vulnerability scan for repository.\"\"\"\n        url = f\"{self.registry_url}/api/v2.0/projects/{project}/repositories/{repository}/artifacts\"\n\n        response = requests.get(url, headers=self.auth_header)\n        if response.status_code != 200:\n            print(f\"Failed to get artifacts: {response.status_code}\")\n            return False\n\n        artifacts = response.json()\n        for artifact in artifacts:\n            scan_url = f\"{url}/{artifact['digest']}/scan\"\n            scan_response = requests.post(scan_url, headers=self.auth_header)\n\n            if scan_response.status_code == 202:\n                print(f\"Scan triggered for {artifact['digest'][:12]}\")\n            else:\n                print(f\"Scan failed for {artifact['digest'][:12]}: {scan_response.status_code}\")\n\n        return True\n\n    def get_vulnerability_report(self, project, repository, tag=\"latest\"):\n        \"\"\"Get vulnerability scan results.\"\"\"\n        url = f\"{self.registry_url}/api/v2.0/projects/{project}/repositories/{repository}/artifacts/{tag}/scan/overview\"\n\n        response = requests.get(url, headers=self.auth_header)\n        if response.status_code == 200:\n            return response.json()\n        else:\n            print(f\"Failed to get scan results: {response.status_code}\")\n            return None\n\n    def check_vulnerabilities(self, scan_results, max_critical=0, max_high=5):\n        \"\"\"Check if vulnerability levels are within acceptable limits.\"\"\"\n        if not scan_results:\n            return False\n\n        # Extract vulnerability counts\n        summary = scan_results.get('vulnerabilities', {})\n        critical = summary.get('critical', 0)\n        high = summary.get('high', 0)\n\n        print(f\"Vulnerabilities found - Critical: {critical}, High: {high}\")\n\n        return critical &lt;= max_critical and high &lt;= max_high\n\n# Usage example\ndef main():\n    harbor = HarborManager(\n        \"https://images.canfar.net\",\n        \"your-username\", \n        \"your-password\"\n    )\n\n    project = \"myteam\"\n    repository = \"analysis-env\"\n\n    # Trigger scan\n    if harbor.scan_repository(project, repository):\n        print(\"Scan triggered, waiting for completion...\")\n        time.sleep(60)  # Wait for scan to complete\n\n        # Check results\n        results = harbor.get_vulnerability_report(project, repository)\n        if harbor.check_vulnerabilities(results):\n            print(\"\u2705 Vulnerability check passed\")\n            sys.exit(0)\n        else:\n            print(\"\u274c Vulnerability check failed\")\n            sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"platform/containers/registry/#registry-maintenance","title":"\ud83d\udcca Registry Maintenance","text":""},{"location":"platform/containers/registry/#storage-management","title":"Storage Management","text":""},{"location":"platform/containers/registry/#repository-cleanup","title":"Repository Cleanup","text":"<pre><code># Remove old tags (manual approach)\n# List all tags first\ndocker images images.canfar.net/myteam/analysis-env\n\n# Remove specific old tags\ndocker rmi images.canfar.net/myteam/analysis-env:old-tag\n\n# Remove all untagged images\ndocker image prune -f\n\n# Remove all unused images\ndocker system prune -a -f\n</code></pre>"},{"location":"platform/containers/registry/#automated-retention-policies","title":"Automated Retention Policies","text":"<p>Configure Harbor retention policies through web interface:</p> <p>Policy examples:</p> <ul> <li>Keep latest 10 versions</li> <li>Retain images pushed within last 30 days</li> <li>Preserve all tagged releases (v*..)</li> <li>Delete untagged artifacts after 7 days</li> </ul>"},{"location":"platform/containers/registry/#monitoring-and-analytics","title":"Monitoring and Analytics","text":""},{"location":"platform/containers/registry/#usage-statistics","title":"Usage Statistics","text":"<p>Harbor provides metrics on:</p> <ul> <li>Storage usage per project</li> <li>Pull/push activity</li> <li>Popular repositories</li> <li>User access patterns</li> <li>Vulnerability trends</li> </ul>"},{"location":"platform/containers/registry/#audit-logging","title":"Audit Logging","text":"<p>Track all registry activities:</p> <pre><code>Audit Log Entry:\n  Timestamp: 2024-03-15T10:30:00Z\n  User: research-user\n  Action: PUSH\n  Resource: myteam/analysis-env:v1.2.0\n  IP Address: 192.168.1.100\n  User Agent: docker/24.0.0\n  Status: SUCCESS\n</code></pre>"},{"location":"platform/containers/registry/#backup-and-disaster-recovery","title":"Backup and Disaster Recovery","text":""},{"location":"platform/containers/registry/#exportimport-procedures","title":"Export/Import Procedures","text":"<pre><code># Export project (including images)\n# Contact CANFAR support for full project exports\n\n# Export image metadata only\ncurl -X GET \"https://images.canfar.net/api/v2.0/projects/myteam/repositories/analysis-env/artifacts\" \\\n  -H \"Authorization: Basic $(echo -n username:password | base64)\" \\\n  &gt; repository-metadata.json\n\n# Backup individual images\ndocker pull images.canfar.net/myteam/analysis-env:v1.2.0\ndocker save images.canfar.net/myteam/analysis-env:v1.2.0 &gt; analysis-env-v1.2.0.tar\n\n# Restore from backup\ndocker load &lt; analysis-env-v1.2.0.tar\ndocker push images.canfar.net/myteam/analysis-env:v1.2.0\n</code></pre>"},{"location":"platform/containers/registry/#best-practices","title":"\ud83d\ude80 Best Practices","text":""},{"location":"platform/containers/registry/#registry-best-practices","title":"Registry Best Practices","text":""},{"location":"platform/containers/registry/#registry-organization","title":"Registry Organization","text":"<pre><code># Recommended project organization\nuniversity-astronomy/        # Institution-wide project\n\u251c\u2500\u2500 public-tools/            # Publicly available tools\n\u251c\u2500\u2500 course-materials/        # Educational containers\n\u2514\u2500\u2500 research-environments/   # General research tools\n\nsurvey-collaboration/        # Multi-institutional project\n\u251c\u2500\u2500 data-processing/         # Survey data pipeline\n\u251c\u2500\u2500 analysis-tools/          # Shared analysis software\n\u2514\u2500\u2500 visualization/           # Survey-specific viz tools\n\npersonal-research/           # Individual development\n\u251c\u2500\u2500 experimental/            # Development and testing\n\u251c\u2500\u2500 paper-environments/      # Publication reproducibility\n\u2514\u2500\u2500 conference-demos/        # Presentation materials\n</code></pre>"},{"location":"platform/containers/registry/#naming-conventions","title":"Naming Conventions","text":"<pre><code># Clear, descriptive names\nradio-interferometry-pipeline\noptical-photometry-tools\nxray-spectral-analysis\n\n# Version-specific names for major releases\nsurvey-pipeline-v2\nanalysis-environment-2024\n\n# Environment-specific variants\nml-environment-gpu\nprocessing-tools-cpu\nanalysis-suite-minimal\n</code></pre>"},{"location":"platform/containers/registry/#performance-optimization","title":"Performance Optimization","text":""},{"location":"platform/containers/registry/#registry-performance","title":"Registry Performance","text":"<pre><code># Use layer caching effectively\n# Order Dockerfile instructions by change frequency\n\n# Use multi-stage builds to reduce final image size\nFROM ubuntu:24.04 AS builder\n# ... build dependencies ...\nFROM images.canfar.net/skaha/astroml:latest\nCOPY --from=builder /app/binary /usr/local/bin/\n\n# Optimize layer sizes\nRUN apt-get update &amp;&amp; apt-get install -y pkg1 pkg2 pkg3 \\\n    &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n# Instead of multiple RUN commands\n</code></pre>"},{"location":"platform/containers/registry/#network-optimization","title":"Network Optimization","text":"<pre><code># Use Harbor proximity\n# CANFAR Harbor is optimized for Canadian academic networks\n\n# Leverage local caching\n# Images are cached at compute nodes for faster session startup\n\n# Consider image size for session startup time\n# Smaller images start faster, especially for batch jobs\n</code></pre>"},{"location":"platform/containers/registry/#integration-with-canfar-services","title":"\ud83d\udd17 Integration with CANFAR Services","text":""},{"location":"platform/containers/registry/#storage-integration","title":"Storage Integration","text":"<p>Containers automatically receive CANFAR storage mounts:</p> <pre><code># Inside any Harbor-deployed container on CANFAR\nls /arc/home/[user]/        # Personal storage\nls /arc/projects/[project]/     # Project storage  \nls /scratch/                  # Temporary storage\n</code></pre>"},{"location":"platform/containers/registry/#authentication-integration","title":"Authentication Integration","text":"<p>Harbor integrates with CADC authentication:</p> <ul> <li>Single sign-on with CADC credentials</li> <li>Group membership determines project access</li> <li>API tokens respect CADC account policies</li> <li>Audit logs integrate with CADC security monitoring</li> </ul>"},{"location":"platform/containers/registry/#troubleshooting","title":"\ud83c\udd98 Troubleshooting","text":""},{"location":"platform/containers/registry/#common-issues","title":"Common Issues","text":""},{"location":"platform/containers/registry/#authentication-problems","title":"Authentication Problems","text":"<pre><code># Check Docker login status\ndocker system info | grep -A 5 \"Registry\"\n\n# Clear cached credentials\ndocker logout images.canfar.net\n\n# Login with explicit credentials\ndocker login images.canfar.net -u username\n\n# Test authentication\ndocker pull images.canfar.net/skaha/astroml:latest\n</code></pre>"},{"location":"platform/containers/registry/#pushpull-failures","title":"Push/Pull Failures","text":"<pre><code># Check repository permissions\ncurl -X GET \"https://images.canfar.net/api/v2.0/projects/myteam\" \\\n  -H \"Authorization: Basic $(echo -n username:password | base64)\"\n\n# Verify image format\ndocker inspect local-image:tag\n\n# Check Harbor service status\ncurl -X GET \"https://images.canfar.net/api/v2.0/systeminfo\"\n</code></pre>"},{"location":"platform/containers/registry/#vulnerability-scan-issues","title":"Vulnerability Scan Issues","text":"<pre><code># Manually trigger scan\ncurl -X POST \"https://images.canfar.net/api/v2.0/projects/myteam/repositories/image/artifacts/tag/scan\" \\\n  -H \"Authorization: Basic $(echo -n username:password | base64)\"\n\n# Check scan status\ncurl -X GET \"https://images.canfar.net/api/v2.0/projects/myteam/repositories/image/artifacts/tag/scan/overview\" \\\n  -H \"Authorization: Basic $(echo -n username:password | base64)\"\n</code></pre>"},{"location":"platform/containers/registry/#performance-issues","title":"Performance Issues","text":""},{"location":"platform/containers/registry/#slow-pushpull-operations","title":"Slow Push/Pull Operations","text":"<pre><code># Check network connectivity\nping images.canfar.net\n\n# Use Docker BuildKit for faster builds\nexport DOCKER_BUILDKIT=1\ndocker build .\n\n# Enable Docker layer caching\ndocker build --cache-from=images.canfar.net/myteam/image:latest .\n</code></pre>"},{"location":"platform/containers/registry/#large-image-size","title":"Large Image Size","text":"<p><pre><code># Analyze image layers\ndocker history images.canfar.net/myteam/image:latest\n\n# Use dive tool for detailed analysis\ndive images.canfar.net/myteam/image:latest\n\n# Optimize Dockerfile layers\n# Combine RUN commands\n# Remove package caches\n# Use multi-stage builds\n</code></pre> ```</p>"},{"location":"platform/sessions/","title":"Interactive Sessions","text":"<p>CANFAR computing environments for astronomical research - Jupyter notebooks, and non-interactive applications.</p> <p>\ud83c\udfaf Session Types Overview</p> <p>Choose the right interface for your research:</p> <ul> <li>Jupyter Notebooks: Interactive data analysis and visualisation</li> <li>Desktop Environment: Full Linux desktop with GUI applications  </li> <li>CARTA Viewer: Radio astronomy visualisation and analysis</li> <li>Firefly Viewer: Table and image viewing for surveys</li> <li>Contributed Apps: Specialised community applications</li> <li>Batch Processing: Automated and large-scale workflows</li> </ul>"},{"location":"platform/sessions/#session-fundamentals","title":"\ud83d\ude80 Session Fundamentals","text":""},{"location":"platform/sessions/#what-are-interactive-sessions","title":"What are Interactive Sessions?","text":"<p>Interactive sessions provide on-demand access to pre-configured computing environments running in containers. Each session type offers different interfaces optimised for specific astronomical workflows.</p>"},{"location":"platform/sessions/#key-benefits","title":"Key Benefits","text":"No Installation Required Access complex astronomy software through your web browser without local installation or configuration. Pre-Configured Environments Containers include popular astronomy packages like AstroPy, CASA, and scientific Python libraries ready to use. Persistent Data Access All sessions automatically connect to your ARC storage and can access VOSpace for long-term data management. Scalable Resources Choose flexible or fixed resource allocation based on your computational requirements."},{"location":"platform/sessions/#session-type-comparison","title":"\ud83d\udcca Session Type Comparison","text":"Session Type Interface Best For GUI Support Notebook JupyterLab Data analysis, prototyping, documentation \u2705 Web-based Desktop Full Linux desktop CASA, image viewers, traditional software \u2705 Desktop GUI CARTA CARTA interface Radio astronomy visualisation \u2705 Specialised Firefly Firefly viewer Catalogue analysis, image display \u2705 Web-based Contributed Various Specialised applications \u26a0\ufe0f Varies Batch None (headless) Large-scale processing \u274c Headless"},{"location":"platform/sessions/#session-management","title":"\ud83d\udd27 Session Management","text":""},{"location":"platform/sessions/#creating-sessions","title":"Creating Sessions","text":"Via Science Portal: Launch sessions through the CANFAR Science Portal web interface with point-and-click simplicity. Via Command Line: Use the CANFAR CLI for scripted session creation and automation. Via Python API: Integrate session management into custom workflows using the CANFAR Python Client."},{"location":"platform/sessions/#session-lifecycle","title":"Session Lifecycle","text":"Creation (30 seconds - 3 minutes) Container download (first time) and startup with storage mounting Active Use Full access to computing resources and storage systems Idle Management Sessions automatically suspend after periods of inactivity to conserve resources Termination Container deletion with data preserved in persistent storage <p>Data Persistence</p> <p>Important: Session containers are temporary. Always save important work to <code>/arc/</code> storage or VOSpace before ending sessions.</p>"},{"location":"platform/sessions/#resource-allocation","title":"\ud83d\udcc8 Resource Allocation","text":""},{"location":"platform/sessions/#flexible-allocation-default","title":"Flexible Allocation (Default)","text":"<p>Advantages: - Faster session startup - Can burst to higher resource usage when available - Optimal for interactive work and development</p> <p>Best For: - Data exploration and analysis - Development and testing - Educational workshops</p>"},{"location":"platform/sessions/#fixed-allocation","title":"Fixed Allocation","text":"<p>Advantages: - Guaranteed consistent performance - Predictable resource availability - Better for production workloads</p> <p>Best For: - Large-scale processing - Performance-critical analysis - Time-sensitive computations</p>"},{"location":"platform/sessions/#resource-selection-guide","title":"Resource Selection Guide","text":"Workflow Type Recommended Mode CPU/Memory Duration Interactive Analysis Flexible 2-4 CPU, 4-8GB Hours Large Dataset Processing Fixed 4-8 CPU, 16-32GB Hours-Days Development &amp; Testing Flexible 1-2 CPU, 2-4GB Hours Production Pipelines Fixed Varies by workload Days"},{"location":"platform/sessions/#integration-with-platform-services","title":"\ud83d\udd17 Integration with Platform Services","text":""},{"location":"platform/sessions/#storage-integration","title":"Storage Integration","text":"<p>All interactive sessions automatically mount:</p> <ul> <li>ARC Home (<code>/arc/home/[user]/</code>): Personal configurations and scripts</li> <li>ARC Projects (<code>/arc/projects/[project]/</code>): Shared research data and results</li> <li>Scratch (<code>/scratch/</code>): High-speed temporary storage for processing</li> </ul> <p>Additional storage accessible via API: - VOSpace (<code>vos:</code>): Long-term archives and data sharing</p>"},{"location":"platform/sessions/#container-environments","title":"Container Environments","text":"<p>Sessions run in container environments that include:</p> <ul> <li>Operating system (typically Ubuntu Linux)</li> <li>Astronomy software packages (AstroPy, CASA, etc.)</li> <li>Scientific computing libraries (NumPy, SciPy, Matplotlib)</li> <li>Development tools and utilities</li> </ul>"},{"location":"platform/sessions/#authentication-permissions","title":"Authentication &amp; Permissions","text":"<p>Sessions inherit your CANFAR permissions:</p> <ul> <li>Automatic access to your group projects</li> <li>Secure integration with CADC services</li> <li>API access for automated workflows</li> </ul>"},{"location":"platform/sessions/#choosing-your-session-type","title":"\ud83c\udfaf Choosing Your Session Type","text":""},{"location":"platform/sessions/#for-data-analysis","title":"For Data Analysis","text":"New to CANFAR? \u2192 Start with Jupyter Notebooks Familiar interface combining code, documentation, and visualisation Need GUI Applications? \u2192 Use Desktop Sessions Full Linux desktop for CASA, image viewers, and traditional software"},{"location":"platform/sessions/#for-astronomy-specialisations","title":"For Astronomy Specialisations","text":"Radio Astronomy \u2192 CARTA Viewer Optimised for radio interferometry data visualisation and analysis Survey Data \u2192 Firefly Viewer Efficient table and image viewing for large astronomical catalogues Specialised Tools \u2192 Contributed Applications Community-maintained applications for specific research domains"},{"location":"platform/sessions/#for-production-work","title":"For Production Work","text":"Large-Scale Processing \u2192 Batch Sessions Automated workflows for processing large datasets without interactive interfaces"},{"location":"platform/sessions/batch/","title":"Batch Processing","text":"<p>Headless container execution for automated workflows and large-scale batch processing</p> <p>\ud83c\udfaf What You'll Learn</p> <ul> <li>How to run containers without interactive interfaces (\"headless\" batch mode)</li> <li>Submitting and managing batch jobs through the Science Portal</li> <li>Using the REST API for programmatic job control and automation</li> <li>Best practices for resource allocation, job scheduling, and monitoring</li> <li>Advanced workflows including parameter sweeps and pipeline automation</li> </ul> <p>Batch processing on CANFAR enables you to run computational workflows without interactive interfaces, perfect for automated data processing, parameter sweeps, and production pipelines. The same containers that power interactive sessions can run in headless mode, executing your scripts and analyses automatically while you focus on other work.</p>"},{"location":"platform/sessions/batch/#overview","title":"\ud83d\udccb Overview","text":"<p>Batch processing provides several key advantages for astronomical research:</p> <ul> <li>Unattended execution: Jobs run without requiring user interaction</li> <li>Resource efficiency: Optimal resource allocation for long-running tasks</li> <li>Scalability: Process large datasets or parameter sweeps systematically</li> <li>Automation: Integrate with existing workflows and pipelines</li> <li>Cost effectiveness: Run jobs during off-peak hours when resources are available</li> </ul>"},{"location":"platform/sessions/batch/#choosing-your-session-type","title":"\ud83c\udf9b\ufe0f Choosing Your Session Type","text":"<p>CANFAR offers two types of resource allocation for batch jobs:</p> <p>Flexible Sessions</p> <p>Optimal for interactive work and development</p> <ul> <li>Faster session startup: Begins immediately with minimal initial resources</li> <li>Can burst to higher resource usage: Auto-scales up to 8 cores and 32GB RAM as needed</li> <li>Resource efficient: Only uses what your workload actually requires</li> <li>Best for: Data exploration, development, testing, educational workshops</li> </ul> <p>Fixed Sessions</p> <p>Better for production workloads</p> <ul> <li>Guaranteed consistent performance: Gets exactly what you request for the entire session</li> <li>Predictable resource availability: No variation in available CPU/memory during execution</li> <li>Better for production workloads: Suitable for performance-critical analysis and time-sensitive computations</li> <li>Best for: Large-scale processing, production pipelines, longer duration jobs</li> </ul> <p>Quick Links</p> <ul> <li>Container Development</li> <li>Storage Guide</li> <li>CANFAR Python Client</li> <li>API Reference</li> <li>Support</li> </ul>"},{"location":"platform/sessions/batch/#1-api-based-execution","title":"1. API-Based Execution","text":"<p>Execute containers programmatically using the <code>canfar</code> command-line client:</p> <pre><code># Ensure you are logged in first\ncanfar auth login\n\n# Submit a flexible session job (default - auto-scaling resources)\ncanfar launch -n data-reduction headless skaha/astroml:latest -- python /arc/projects/[project]/scripts/reduce_data.py\n\n# Submit a fixed session job (guaranteed resources)\ncanfar launch --name large-simulation --cpu 16 --memory 64 headless skaha/astroml:latest -- python /arc/projects/[project]/scripts/simulation.py\n</code></pre>"},{"location":"platform/sessions/batch/#2-job-submission-scripts","title":"2. Job Submission Scripts","text":"<p>Create shell scripts for common workflows using the <code>canfar</code> client:</p> <pre><code>#!/bin/bash\n# submit_reduction.sh\n\n# Set job parameters\nJOB_NAME=\"nightly-reduction-$(date +%Y%m%d)\"\nIMAGE=\"images.canfar.net/skaha/casa:6.5\"\nCMD=\"python /arc/projects/[project]/pipelines/reduce_night.py /arc/projects/survey/data/$(date +%Y%m%d)\"\n\n# Submit job\ncanfar launch \\\n  --name \"$JOB_NAME\" \\\n  --image \"$IMAGE\" \\\n  --cores 8 \\\n  --ram 32 \\\n  --cmd \"$CMD\"\n</code></pre> <p>Or using the Python <code>canfar</code> client:</p> <pre><code>#!/usr/bin/env python\n# submit_reduction.py - Python client-based submission\n\nfrom canfar.sessions import Session\nfrom datetime import datetime\n\n# Initialize session manager\nsession = Session()\n\n# Set job parameters\njob_name = f\"nightly-reduction-{datetime.now().strftime('%Y%m%d')}\"\nimage = \"images.canfar.net/skaha/casa:6.5\"\nproject=\"/arc/projects/[project]\"\ndata_path = f\"{project}/data/{datetime.now().strftime('%Y%m%d')}\"\n\n# Submit flexible job (default - auto-scaling)\njob_ids = session.create(\n    name=job_name,\n    image=image,\n    cmd=\"python\",\n    args=[f\"{project}/pipelines/reduce_night.py\", data_path]\n)\n\n# Or submit fixed job (guaranteed resources by specifying cores/ram)\njob_ids = session.create(\n    name=job_name,\n    image=image,\n    cores=8,\n    ram=32,  # Having cores/ram makes it a fixed session\n    cmd=\"python\",\n    args=[f\"{project}/pipelines/reduce_night.py\", data_path]\n)\n\nprint(f\"Submitted job(s): {job_ids}\")\n</code></pre>"},{"location":"platform/sessions/batch/#performance-optimization","title":"Performance Optimization","text":"<p>Advanced: Resource Monitoring</p> <ul> <li>Use <code>canfar stats [session-id]</code> and <code>canfar info [session-id]</code> to monitor job resource usage.</li> <li>For parallel workloads, see Distributed Computing for strategies.</li> </ul>"},{"location":"platform/sessions/batch/#resource-allocation-strategy","title":"Resource Allocation Strategy","text":"<p>Right-sizing your jobs is crucial for performance and queue times:</p> <pre><code># Start small and scale up based on monitoring\n# Test job with minimal resources first\ncanfar launch \\\n  --name \"test-small\" \\\n  --cores 2 \\\n  --ram 4 \\\n  --image \"images.canfar.net/skaha/astroml:latest\" \\\n  --kind \"headless\" \\\n  --cmd \"python /arc/projects/[project]/test_script.py\"\n\n# Monitor resource usage in the job logs\n# Scale up for production runs if needed\n</code></pre> <p>Memory Optimization:</p> <pre><code># Memory-efficient data processing patterns\nimport numpy as np\nfrom astropy.io import fits\n\ndef process_large_cube(filename):\n    \"\"\"Process large data cube efficiently\"\"\"\n\n    # Memory-map large files instead of loading fully\n    with fits.open(filename, memmap=True) as hdul:\n        data = hdul[0].data\n\n        # Process in chunks to control memory usage\n        chunk_size = 100  # Adjust based on available RAM\n        results = []\n\n        for i in range(0, data.shape[0], chunk_size):\n            chunk = data[i:i+chunk_size]\n            # Process chunk and collect lightweight results\n            result = np.mean(chunk, axis=(1,2))  # Example operation\n            results.append(result)\n\n            # Explicit cleanup for large chunks\n            del chunk\n\n        return np.concatenate(results)\n</code></pre> <p>Storage Performance:</p> <pre><code># Use /scratch/ for I/O intensive operations\n#!/bin/bash\nset -e\nPROJECT=\"/arc/projects/[project]\"\n\n# Copy data to fast scratch storage\necho \"Copying data to scratch...\"\nrsync -av $PROJECT/large_dataset/ /scratch/working/\n\n# Process on fast storage\ncd /scratch/working\npython intensive_processing.py\n\n# Save results back to permanent storage\necho \"Saving results...\"\nmkdir -p $PROJECT/results/$(date +%Y%m%d)\ncp *.fits $PROJECT/results/$(date +%Y%m%d)/\ncp *.log $PROJECT/logs/\n\necho \"Processing complete\"\n</code></pre>"},{"location":"platform/sessions/batch/#parallel-processing","title":"Parallel Processing","text":"<p>Multi-core CPU Usage:</p> <pre><code>from multiprocessing import Pool, cpu_count\nimport numpy as np\nfrom functools import partial\n\ndef process_file(filename, parameters):\n    \"\"\"Process a single file\"\"\"\n    # Your processing logic here\n    return result\n\ndef parallel_processing():\n    \"\"\"Process multiple files in parallel\"\"\"\n\n    # Get available CPU cores (leave 1 for system)\n    n_cores = max(1, cpu_count() - 1)\n\n    files = glob.glob('/scratch/input/*.fits')\n    parameters = {'param1': value1, 'param2': value2}\n\n    # Create partial function with fixed parameters\n    process_func = partial(process_file, parameters=parameters)\n\n    # Process files in parallel\n    with Pool(n_cores) as pool:\n        results = pool.map(process_func, files)\n\n    return results\n</code></pre> <p>GPU Acceleration (when available):</p> <pre><code>import numpy as np\ntry:\n    import cupy as cp  # GPU arrays\n    gpu_available = True\nexcept ImportError:\n    import numpy as cp  # Fallback to CPU\n    gpu_available = False\n\ndef gpu_accelerated_processing(data):\n    \"\"\"Use GPU acceleration when available\"\"\"\n\n    if gpu_available:\n        print(f\"Using GPU acceleration\")\n        # Convert to GPU array\n        gpu_data = cp.asarray(data)\n\n        # GPU-accelerated operations\n        result = cp.fft.fft2(gpu_data)\n        result = cp.abs(result)\n\n        # Convert back to CPU for saving\n        return cp.asnumpy(result)\n    else:\n        print(\"Using CPU fallback\")\n        # CPU-only operations\n        return np.abs(np.fft.fft2(data))\n</code></pre>"},{"location":"platform/sessions/batch/#job-monitoring-and-logging","title":"Job Monitoring and Logging","text":"<p>Comprehensive Logging:</p> <pre><code>import logging\nimport psutil\nimport time\nfrom datetime import datetime\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('/arc/projects/[project]/logs/processing.log'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\ndef log_system_status():\n    \"\"\"Log current system resource usage\"\"\"\n    cpu_percent = psutil.cpu_percent(interval=1)\n    memory = psutil.virtual_memory()\n    disk = psutil.disk_usage('/scratch')\n\n    logger.info(f\"CPU: {cpu_percent:.1f}%, \"\n                f\"Memory: {memory.percent:.1f}% \"\n                f\"({memory.used//1024**3:.1f}GB used), \"\n                f\"Scratch: {disk.percent:.1f}% used\")\n\ndef timed_processing(func, *args, **kwargs):\n    \"\"\"Wrapper to time and log function execution\"\"\"\n    start_time = time.time()\n    logger.info(f\"Starting {func.__name__}\")\n    log_system_status()\n\n    try:\n        result = func(*args, **kwargs)\n        elapsed = time.time() - start_time\n        logger.info(f\"Completed {func.__name__} in {elapsed:.2f} seconds\")\n        return result\n    except Exception as e:\n        elapsed = time.time() - start_time\n        logger.error(f\"Failed {func.__name__} after {elapsed:.2f} seconds: {e}\")\n        raise\n</code></pre> <p>See Also</p> <ul> <li>Container Development</li> <li>Storage Guide</li> <li>CANFAR Python Client</li> <li>API Reference</li> <li>Support</li> </ul>"},{"location":"platform/sessions/batch/#job-sizing-guidelines","title":"Job Sizing Guidelines","text":"<p>Choose appropriate resources based on your workload:</p> Job Type Cores Memory Storage Duration Single image reduction 1-2 4-8GB 10GB 5-30 min Survey night processing 4-8 16-32GB 100GB 1-4 hours Catalog cross-matching 2-4 8-16GB 50GB 30min-2hr ML model training 8-16 32-64GB 200GB 4-24 hours Large simulations 16-32 64-128GB 1TB Days-weeks <p>Queue Optimization</p> <ul> <li>Small jobs (\u22644 cores, \u226416GB) start faster</li> <li>Large jobs (\u226516 cores, \u226564GB) may queue longer  </li> <li>Off-peak hours (evenings, weekends) often have shorter wait times</li> <li>Resource requests should match actual usage to avoid waste</li> <li>For advanced queue management, see CANFAR Python Client</li> </ul>"},{"location":"platform/sessions/batch/#queue-management","title":"Queue Management","text":"<p>Understand job priorities and scheduling:</p> <ul> <li>Small jobs (&lt;4 cores, &lt;16GB): Higher priority, faster start</li> <li>Large jobs (16+ cores, 64GB+): Lower priority, may queue longer</li> <li>Off-peak hours: Better resource availability (evenings, weekends)</li> <li>Resource limits: Per-user and per-group limits apply</li> </ul>"},{"location":"platform/sessions/batch/#api-access","title":"API Reference","text":"<p>Legacy Client</p> <p>The <code>skaha</code> Python client is deprecated and has been replaced by the <code>canfar</code> client. The following examples use the modern <code>canfar</code> client. For more examples, see Client Examples.</p>"},{"location":"platform/sessions/batch/#method-1-canfar-command-line-client","title":"Method 1: <code>canfar</code> Command-Line Client","text":""},{"location":"platform/sessions/batch/#submit-job","title":"Submit Job","text":"<pre><code>canfar launch \\\n  --name \"my-analysis-job\" \\\n  --image \"images.canfar.net/skaha/astroml:latest\" \\\n  --cores 4 \\\n  --ram 16 \\\n  --cmd \"python /arc/projects/myproject/analysis.py\"\n</code></pre>"},{"location":"platform/sessions/batch/#list-jobs","title":"List Jobs","text":"<pre><code>canfar ps\n</code></pre>"},{"location":"platform/sessions/batch/#get-job-status","title":"Get Job Status","text":"<pre><code>canfar info [session-id]\n</code></pre>"},{"location":"platform/sessions/batch/#cancel-job","title":"Cancel Job","text":"<pre><code>canfar delete [session-id]\n</code></pre>"},{"location":"platform/sessions/batch/#get-job-logs","title":"Get Job Logs","text":"<pre><code>canfar logs [session-id]\n</code></pre>"},{"location":"platform/sessions/batch/#get-resource-usage","title":"Get Resource Usage","text":"<pre><code>canfar stats [session-id]\n</code></pre>"},{"location":"platform/sessions/batch/#method-2-canfar-python-api","title":"Method 2: <code>canfar</code> Python API","text":"<p>The <code>canfar</code> Python client provides a convenient interface for batch job management and automation.</p>"},{"location":"platform/sessions/batch/#installation","title":"Installation","text":"<pre><code>pip install canfar\n</code></pre>"},{"location":"platform/sessions/batch/#basic-python-api-usage","title":"Basic Python API Usage","text":"<pre><code>from canfar.sessions import Session\n\n# Initialize session manager\nsession = Session()\n\n# Simple job submission\njob_ids = session.create(\n    name=\"python-analysis\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    kind=\"headless\",\n    cmd=\"python\",\n    args=[\"/arc/projects/[project]/analysis.py\"]\n)\n\nprint(f\"Submitted job(s): {job_ids}\")\n</code></pre>"},{"location":"platform/sessions/batch/#advanced-job-submission","title":"Advanced Job Submission","text":"<pre><code>from canfar.sessions import Session\n\nsession = Session()\n\n# Job with custom resources and environment\njob_ids = session.create(\n    name=\"heavy-computation\",\n    image=\"images.canfar.net/[project]/processor:latest\", \n    kind=\"headless\",\n    cores=8,\n    ram=32,\n    cmd=\"/opt/scripts/heavy_process.sh\",\n    args=[\"/arc/projects/[project]/data/large_dataset.h5\", \"/arc/projects/results/\"],\n    env={\n        \"PROCESSING_THREADS\": \"8\",\n        \"OUTPUT_FORMAT\": \"hdf5\",\n        \"VERBOSE\": \"true\"\n    }\n)\n</code></pre>"},{"location":"platform/sessions/batch/#private-image-authentication","title":"Private Image Authentication","text":"<p>To use private images, you first need to configure the client with your registry credentials. See the registry guide for details.</p>"},{"location":"platform/sessions/batch/#job-monitoring-and-management","title":"Job Monitoring and Management","text":"<pre><code>import time\nfrom canfar.sessions import Session\n\nsession = Session()\n\n# List all your sessions\nsessions = session.fetch()\nprint(f\"Active sessions: {len(sessions)}\")\n\n# Create a job to monitor\njob_ids = session.create(\n    name=\"monitored-job\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    kind=\"headless\",\n    cmd=\"sleep 60\"\n)\njob_id = job_ids[0]\n\n# Get session details\nsession_info = session.info(ids=job_id)\nprint(f\"Status: {session_info[0]['status']}\")\nprint(f\"Start time: {session_info[0]['startTime']}\")\n\n# Wait for completion\nwhile True:\n    status = session.info(ids=job_id)[0]['status']\n    if status in ['Succeeded', 'Failed', 'Terminated']:\n        print(f\"Job completed with status: {status}\")\n        break\n    time.sleep(10)\n\n# Get logs\nlogs = session.logs(ids=job_id)\nprint(\"Job output:\")\nprint(logs[job_id])\n\n# Clean up\nsession.destroy(ids=job_id)\n</code></pre>"},{"location":"platform/sessions/batch/#bulk-job-management","title":"Bulk Job Management","text":"<pre><code>from canfar.sessions import Session\n\nsession = Session()\n\n# Submit multiple related jobs\njob_ids = session.create(\n    name=\"parameter-study\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    kind=\"headless\",\n    cmd=\"python /arc/projects/[project]/scripts/analyze.py\",\n    replicas=10 # Creates 10 jobs named parameter-study-1, parameter-study-2, etc.\n)\nprint(f\"Submitted jobs: {job_ids}\")\n\n# Monitor all jobs\n# ... (see single job monitoring example)\n</code></pre>"},{"location":"platform/sessions/batch/#monitoring-and-debugging","title":"Monitoring and Debugging","text":"<p>Advanced: Debugging Batch Jobs</p> <ul> <li>Use <code>canfar logs [session-id]</code> and <code>canfar stats [session-id]</code> for troubleshooting.</li> <li>For persistent issues, see FAQ and Support.</li> </ul>"},{"location":"platform/sessions/batch/#log-analysis","title":"Log Analysis","text":"<p>Monitor job progress through logs:</p> <pre><code># Real-time log monitoring\ncanfar logs -f [session-id]\n\n# Search for errors\ncanfar logs &lt;[session-id] | grep -i error\n</code></pre>"},{"location":"platform/sessions/batch/#resource-monitoring","title":"Resource Monitoring","text":"<p>Track resource usage:</p> <pre><code># Get session statistics\ncanfar stats [session-id]\n</code></pre>"},{"location":"platform/sessions/batch/#common-issues","title":"Common Issues","text":"<p>Job fails to start: - Check resource availability - Verify container image exists - Check command syntax</p> <p>Job crashes: - Review logs for error messages - Check memory usage patterns - Verify input file accessibility</p> <p>Job hangs: - Monitor CPU usage - Check for infinite loops - Verify network connectivity</p>"},{"location":"platform/sessions/batch/#best-practices","title":"Best Practices","text":"<p>See Also</p> <ul> <li>Container Development</li> <li>Storage Guide</li> <li>CANFAR Python Client</li> <li>API Reference</li> <li>Support</li> </ul>"},{"location":"platform/sessions/batch/#script-design","title":"Script Design","text":"<ul> <li>Error handling: Use try-catch blocks and meaningful error messages</li> <li>Logging: Include progress indicators and debugging information</li> <li>Checkpointing: Save intermediate results for long-running jobs</li> <li>Resource monitoring: Track memory and CPU usage</li> </ul>"},{"location":"platform/sessions/batch/#data-management","title":"Data Management","text":"<ul> <li>Input validation: Check file existence and format before processing</li> <li>Output organisation: Use consistent naming and directory structures</li> <li>Cleanup: Remove temporary files to save storage</li> <li>Metadata: Include processing parameters in output headers</li> </ul> <p>Persistence Reminder</p> <p>Headless containers do not persist changes to the container filesystem. Always write outputs to <code>/arc/projects/</code> or <code>/arc/home/</code>. For data management strategies, see Storage Guide.</p>"},{"location":"platform/sessions/carta/","title":"CARTA Sessions","text":"<p>CARTA (Cube Analysis and Rendering Tool for Astronomy) for astronomy data visualisation</p> <p>\ud83c\udfaf What You'll Learn</p> <ul> <li>How to launch CARTA sessions and choose the right version</li> <li>Loading data from CANFAR storage and working with radio data cubes</li> <li>Key features for spectral analysis, region analysis, and animations</li> <li>Performance tips and troubleshooting guidance</li> </ul> <p>CARTA is a specialised image visualisation and analysis tool designed specifically for radio astronomy data. It excels at handling multi-dimensional data cubes, providing powerful tools for spectral analysis, and enabling real-time collaborative workflows.</p>"},{"location":"platform/sessions/carta/#overview","title":"\ud83d\udccb Overview","text":"<p>CARTA provides specialised capabilities for:</p>"},{"location":"platform/sessions/carta/#key-features","title":"Key Features","text":"Feature Capability Image Visualisation Multi-dimensional data cube exploration with WCS support Spectral Analysis Line profiles, moment maps, and velocity analysis Region Analysis Statistical analysis of user-defined image regions Animation Time-series and frequency animations through data cubes Collaboration Real-time session sharing with multiple users Performance Optimised rendering for large astronomical datasets"},{"location":"platform/sessions/carta/#data-format-support","title":"Data Format Support","text":"<ul> <li>FITS files: Standard astronomical format with full WCS support</li> <li>HDF5 files: High-performance format for large datasets</li> <li>CASA images: Native support for CASA image formats</li> <li>Compressed formats: Automatic handling of gzipped files</li> </ul>"},{"location":"platform/sessions/carta/#creating-a-carta-session","title":"\ud83d\ude80 Creating a CARTA Session","text":""},{"location":"platform/sessions/carta/#step-1-select-session-type","title":"Step 1: Select Session Type","text":"<p>From the Science Portal dashboard, click the plus sign (+) to create a new session, then select carta as your session type.</p>"},{"location":"platform/sessions/carta/#step-2-choose-container-version","title":"Step 2: Choose Container Version","text":"<p>Note that the menu options update automatically after your session type selection. Choose the CARTA version that meets your needs:</p>"},{"location":"platform/sessions/carta/#available-versions","title":"Available Versions","text":"<ul> <li>CARTA 4.0 (recommended): Latest features and bug fixes</li> <li>CARTA 3.x: Previous stable releases for compatibility</li> </ul> <p>Version Selection</p> <p>Use the latest version (4.0+) unless you specifically need compatibility with older workflows. New versions include performance improvements and additional features.</p>"},{"location":"platform/sessions/carta/#step-3-configure-session","title":"Step 3: Configure Session","text":""},{"location":"platform/sessions/carta/#session-name","title":"Session Name","text":"<p>Choose a descriptive session name to help you identify it later:</p> <p>Good session names: - <code>m87-analysis</code> - <code>ngc-1300-cube</code> - <code>alma-co-line-study</code> - <code>vla-continuum-imaging</code></p>"},{"location":"platform/sessions/carta/#resource-allocation","title":"Resource Allocation","text":"<p>Start with a \"flexible\" session for most analyses. Switch to a fixed resource allocation if you need guaranteed performance for demanding visualisations.</p> <p>Resource Guidelines: - Flexible: Good for most CARTA workflows - Fixed: Use for large datasets (&gt;1GB) or guaranteed performance</p>"},{"location":"platform/sessions/carta/#step-4-launch-session","title":"Step 4: Launch Session","text":"<p>Click the Launch button and wait for your session to initialise. CARTA sessions typically start within 30-60 seconds.</p>"},{"location":"platform/sessions/carta/#using-carta","title":"\ud83e\udded Using CARTA","text":""},{"location":"platform/sessions/carta/#first-steps","title":"First Steps","text":"<p>Once connected to your CARTA session:</p> <ol> <li>File Menu: Use \"Open Image\" to load your data</li> <li>File Browser: Navigate to <code>/arc/projects/[project]/</code> or <code>/arc/home/[user]/</code></li> <li>Load Data: Select FITS or HDF5 files to visualise</li> </ol>"},{"location":"platform/sessions/carta/#data-loading","title":"Data Loading","text":""},{"location":"platform/sessions/carta/#from-canfar-storage","title":"From CANFAR Storage","text":"<pre><code># CARTA can access files from:\n/arc/home/[user]/                    # Your personal data\n/arc/projects/[project]/             # Shared project data\n/scratch/                            # Temporary high-speed storage\n</code></pre>"},{"location":"platform/sessions/carta/#supported-file-paths","title":"Supported File Paths","text":"<ul> <li>Local files: Any file accessible in the session filesystem</li> <li>Remote files: HTTP/HTTPS URLs (limited support)</li> <li>Archive data: Files downloaded to CANFAR storage</li> </ul>"},{"location":"platform/sessions/carta/#interface-overview","title":"Interface Overview","text":""},{"location":"platform/sessions/carta/#main-components","title":"Main Components","text":"<ul> <li>Image Viewer: Central panel showing the astronomical image</li> <li>File Browser: Left panel for navigating and opening files</li> <li>Region List: Panel for managing analysis regions</li> <li>Statistics: Real-time statistics for selected regions</li> <li>Spectral Profiler: Panel for line profile analysis</li> <li>Animation: Controls for cycling through cube slices</li> </ul>"},{"location":"platform/sessions/carta/#essential-controls","title":"Essential Controls","text":"Control Function Mouse wheel Zoom in/out Click + drag Pan around image Right-click Context menu with additional options Keyboard shortcuts See Help menu for complete list"},{"location":"platform/sessions/carta/#analysis-features","title":"\ud83d\udd2c Analysis Features","text":""},{"location":"platform/sessions/carta/#spectral-analysis","title":"Spectral Analysis","text":""},{"location":"platform/sessions/carta/#line-profiles","title":"Line Profiles","text":"<ol> <li>Draw regions on the image</li> <li>Open Spectral Profiler panel</li> <li>Select region to view spectrum</li> <li>Analyse lines with built-in fitting tools</li> </ol>"},{"location":"platform/sessions/carta/#moment-maps","title":"Moment Maps","text":"<p>CARTA can generate:</p> <ul> <li>Moment 0: Integrated intensity</li> <li>Moment 1: Velocity field  </li> <li>Moment 2: Velocity dispersion</li> </ul>"},{"location":"platform/sessions/carta/#region-analysis","title":"Region Analysis","text":""},{"location":"platform/sessions/carta/#creating-regions","title":"Creating Regions","text":"<ol> <li>Select region tool from toolbar</li> <li>Draw on image: Rectangle, ellipse, polygon, or point</li> <li>View statistics in the Statistics panel</li> <li>Export regions in DS9 or CRTF format</li> </ol>"},{"location":"platform/sessions/carta/#statistical-analysis","title":"Statistical Analysis","text":"<p>CARTA automatically computes: - Sum, mean, RMS within regions - Min/max values and positions - Flux measurements with proper units - Histogram analysis of pixel values</p>"},{"location":"platform/sessions/carta/#animation-and-navigation","title":"Animation and Navigation","text":""},{"location":"platform/sessions/carta/#data-cube-navigation","title":"Data Cube Navigation","text":"<ul> <li>Slider controls: Navigate through spectral channels or Stokes parameters</li> <li>Animation playback: Automatic cycling through cube slices</li> <li>Frame rate control: Adjust animation speed</li> <li>Custom ranges: Focus on specific velocity ranges</li> </ul>"},{"location":"platform/sessions/carta/#multi-panel-views","title":"Multi-Panel Views","text":"<ul> <li>Compare datasets: Load multiple images simultaneously</li> <li>Linked panels: Synchronise zoom, pan, and navigation</li> <li>Layout control: Arrange panels as needed</li> </ul>"},{"location":"platform/sessions/carta/#performance-optimisation","title":"\u26a1 Performance Optimisation","text":""},{"location":"platform/sessions/carta/#large-dataset-handling","title":"Large Dataset Handling","text":""},{"location":"platform/sessions/carta/#memory-management","title":"Memory Management","text":"<pre><code># Monitor session resources\nhtop                    # Check memory usage\ndf -h                   # Check disk space\n</code></pre>"},{"location":"platform/sessions/carta/#optimisation-tips","title":"Optimisation Tips","text":"<ul> <li>Use /scratch for large files: Copy data to high-speed storage</li> <li>Close unused files: Reduce memory consumption</li> <li>Reduce image resolution: For initial exploration</li> <li>Use data subsets: Work with spatial/spectral sub-cubes</li> </ul>"},{"location":"platform/sessions/carta/#network-performance","title":"Network Performance","text":""},{"location":"platform/sessions/carta/#for-remote-access","title":"For Remote Access","text":"<ul> <li>Stable connection: CARTA requires consistent network connectivity</li> <li>Bandwidth: Higher bandwidth improves responsiveness</li> <li>Close other applications: Reduce network competition</li> </ul>"},{"location":"platform/sessions/carta/#collaboration-features","title":"\ud83e\udd1d Collaboration Features","text":""},{"location":"platform/sessions/carta/#real-time-sharing","title":"Real-Time Sharing","text":"<p>CARTA supports collaborative analysis:</p> <ol> <li>Share session URL with team members</li> <li>Simultaneous access: Multiple users can connect</li> <li>Synchronised views: All users see the same state</li> <li>Coordinate activities: Communicate to avoid conflicts</li> </ol>"},{"location":"platform/sessions/carta/#best-practices-for-collaboration","title":"Best Practices for Collaboration","text":"<ul> <li>Designate a lead: Have one person control navigation</li> <li>Use voice/chat: Coordinate complex operations</li> <li>Save work frequently: Export regions and analysis results</li> <li>Plan sessions: Organise collaborative time in advance</li> </ul>"},{"location":"platform/sessions/carta/#advanced-features","title":"\ud83d\udd27 Advanced Features","text":""},{"location":"platform/sessions/carta/#scripting-and-automation","title":"Scripting and Automation","text":""},{"location":"platform/sessions/carta/#export-capabilities","title":"Export Capabilities","text":"<ul> <li>Image exports: PNG, JPEG, PDF formats</li> <li>Region files: DS9 or CRTF format for other tools</li> <li>Spectral data: CSV format for further analysis</li> <li>Session state: Save and restore CARTA configurations</li> </ul>"},{"location":"platform/sessions/carta/#integration-with-other-tools","title":"Integration with Other Tools","text":"<pre><code># Load CARTA regions in Python\nfrom astropy.io import fits\nfrom regions import Regions\n\n# Read CARTA-exported region file\nregions = Regions.read('carta_regions.crtf', format='crtf')\n\n# Use with other astronomy software\n</code></pre>"},{"location":"platform/sessions/carta/#custom-colour-maps","title":"Custom Colour Maps","text":"<ul> <li>Built-in maps: Scientific colour schemes</li> <li>Custom maps: Import your own colour tables</li> <li>Accessibility: Colour-blind friendly options</li> <li>Publication quality: High-contrast options for papers</li> </ul>"},{"location":"platform/sessions/carta/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"platform/sessions/carta/#common-issues","title":"Common Issues","text":""},{"location":"platform/sessions/carta/#session-wont-load-data","title":"Session Won't Load Data","text":"<p>Problem: CARTA cannot open FITS files</p> <p>Solutions:</p> <ol> <li>Check file permissions and location</li> <li>Verify file format is supported</li> <li>Try copying file to <code>/scratch/</code> first</li> <li>Check file isn't corrupted</li> </ol>"},{"location":"platform/sessions/carta/#slow-performance","title":"Slow Performance","text":"<p>Problem: CARTA responds slowly to interactions</p> <p>Solutions:</p> <ol> <li>Check available memory with <code>htop</code></li> <li>Close other browser tabs/applications</li> <li>Reduce image size or use sub-cubes</li> <li>Restart session if memory is exhausted</li> </ol>"},{"location":"platform/sessions/carta/#connection-issues","title":"Connection Issues","text":"<p>Problem: Lost connection to CARTA session</p> <p>Solutions:</p> <ol> <li>Refresh browser page</li> <li>Check internet connection stability</li> <li>Clear browser cache if persistent</li> <li>Try different browser</li> </ol>"},{"location":"platform/sessions/carta/#display-problems","title":"Display Problems","text":"<p>Problem: Images don't render correctly</p> <p>Solutions:</p> <ol> <li>Try different browser (Chrome/Firefox recommended)</li> <li>Update browser to latest version</li> <li>Disable browser extensions temporarily</li> <li>Check graphics drivers on local machine</li> </ol>"},{"location":"platform/sessions/contributed/","title":"Contributed Applications","text":"<p>Contributed Web-based Applications on CANFAR</p> <p>\ud83c\udfaf What You'll Learn</p> <ul> <li>How to launch contributed applications on CANFAR</li> <li>Where your data is stored and how to save results</li> <li>How to contribute your own web application</li> <li>Troubleshooting common issues</li> </ul> <p>Contributed applications are specialised, community-developed web tools that expand CANFAR's capabilities. They integrate with CANFAR storage and authentication, are web-based for easy collaboration, and require no local installation. The catalogue of available applications evolves as the community contributes new tools.</p> <p>Suggest New Applications</p> <p>Have an idea for a new application? Jump on discord, or contact support@canfar.net to discuss it.</p>"},{"location":"platform/sessions/contributed/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ol> <li>Log into the CANFAR Science Portal, click + to create a new session, and select <code>contributed</code>.</li> <li>Choose an application from the dropdown menu.</li> <li>Give your session a descriptive name and click \"Launch.\"</li> </ol> <p>Your application will start in 30-90 seconds.</p> <p>Data Persistence</p> <p>Contributed applications can access your files at <code>/arc/projects/[project]/</code> and <code>/arc/home/[user]/</code>. Always save important results to these paths, as other locations may not persist.</p> <p>Currently Available Applications:</p> <ul> <li>marimo (<code>skaha/marimo:latest</code>): Reactive Python notebooks for reproducible analysis.</li> <li>VSCode on Browser (<code>skaha/vscode:latest</code>): A browser-based development environment for collaborative projects (based on Visual Studio Code).</li> </ul>"},{"location":"platform/sessions/contributed/#contributing-your-app","title":"\ud83e\uddd1\u200d\ud83d\udcbb Contributing Your App","text":"<p>If you have a containerized web application, you can contribute it to the platform. The main requirements are that your application must expose a web interface on port 5000 and include a startup script at <code>/skaha/startup.sh</code>.</p> <p>For detailed instructions, see the Container Development guide. We recommend contacting support@canfar.net to discuss your idea before you start.</p>"},{"location":"platform/sessions/contributed/#troubleshooting","title":"\ud83c\udd98 Troubleshooting","text":"<ul> <li>Application doesn't load: If your session doesn't start after 90 seconds, try a hard refresh of your browser page.</li> <li>Data access issues: These usually stem from incorrect file paths or permissions. Verify you are using the correct paths and have the necessary permissions.</li> </ul>"},{"location":"platform/sessions/contributed/#whats-next","title":"\ud83d\udd17 What's Next?","text":"<p>To make the most of contributed applications, match the right tool to your workflow. Explore each application's capabilities and consider combining them with other CANFAR services like Batch Processing for more powerful analysis. The Storage Guide will help you effectively manage your data.</p>"},{"location":"platform/sessions/desktop/","title":"Desktop Sessions","text":"<p>Linux graphical environment in your browser with astronomy software</p> <p>\ud83c\udfaf What You'll Learn</p> <ul> <li>How to launch, connect, and configure desktop sessions</li> <li>Available software and how to launch astronomy applications</li> <li>Managing files and storage within desktop sessions</li> <li>Tips for collaboration, performance, and troubleshooting</li> </ul> <p>Desktop sessions on CANFAR provide a full Linux graphical environment directly in your browser, with access to CANFAR storage. Most astronomy software runs in dedicated containers on separate worker nodes and connects to your browser session using X11 protocols. This provides a familiar desktop experience for GUI applications and traditional workflows.</p>"},{"location":"platform/sessions/desktop/#overview","title":"\ud83d\udccb Overview","text":"<p>Desktop sessions offer:</p> <ul> <li>Full Linux desktop: Accessed in your browser, with CANFAR storage integration</li> <li>Multi-application workflow: Run multiple programs and containers simultaneously</li> <li>Traditional interfaces: Use graphical astronomy software and desktop tools</li> <li>File management: Visual file browser and management tools</li> <li>Session persistence: Resume work exactly where you left off</li> </ul>"},{"location":"platform/sessions/desktop/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Running astronomy software in containers: DS9, Aladin, TOPCAT, CASA, etc.</li> <li>Multi-step workflows: Combine several applications in sequence</li> <li>Teaching and demonstrations: Share desktop for educational purposes</li> <li>Legacy software: Applications requiring a desktop environment</li> <li>Visual file management: Organise data with graphical tools</li> </ul>"},{"location":"platform/sessions/desktop/#how-desktop-sessions-work","title":"How Desktop Sessions Work","text":"<pre><code>graph TB\n    Browser[Your Browser] --&gt; Desktop[Desktop Session]\n    Desktop --&gt; FileManager[File Manager]\n    Desktop --&gt; Terminal[Terminal]\n    Desktop --&gt; Shortcuts[Application Shortcuts]\n\n    Shortcuts --&gt; DS9[DS9 Container]\n    Shortcuts --&gt; TOPCAT[TOPCAT Container]\n    Shortcuts --&gt; Aladin[Aladin Container]\n\n    Desktop --&gt; AstroMenu[Astro Software Menu]\n    AstroMenu --&gt; CASA[CASA Container]\n    AstroMenu --&gt; UserApps[User-Contributed Apps]\n\n    Desktop --&gt; Storage[CANFAR Storage]\n    Storage --&gt; ArcHome[/arc/home/user/]\n    Storage --&gt; ArcProjects[/arc/projects/project/]\n    Storage --&gt; Scratch[/scratch/]</code></pre> <p>Desktop Runtime: Provides the graphical environment in your browser Application Containers: Most astronomy software runs in separate containers X11 Forwarding: Applications display through the desktop session Storage Integration: Direct access to CANFAR filesystems</p>"},{"location":"platform/sessions/desktop/#creating-a-desktop-session","title":"\ud83d\ude80 Creating a Desktop Session","text":""},{"location":"platform/sessions/desktop/#step-1-select-session-type-and-name","title":"Step 1: Select Session Type and Name","text":"<p>From the Science Portal dashboard, click the plus sign (+) to create a new session, then select desktop as your session type.</p> <p>Choose a descriptive session name to help you identify it later:</p> <p>Good session names: - <code>data-reduction</code> - <code>teaching-session</code> - <code>multi-instrument-analysis</code> - <code>collaborative-work</code></p>"},{"location":"platform/sessions/desktop/#step-2-configure-resources","title":"Step 2: Configure Resources","text":"<p>Desktop sessions use the default container and resource allocation. For most desktop work, the default settings are appropriate.</p> <p>Resource Guidelines: - Memory: 16GB is typically sufficient for most desktop workflows - CPU: 2-4 cores handle most desktop applications well - Storage: Use persistent storage in <code>/arc/</code> for important work</p>"},{"location":"platform/sessions/desktop/#step-3-launch-session","title":"Step 3: Launch Session","text":"<p>Click the Launch button and wait for your session to initialise:</p> <p>Desktop sessions may take slightly longer to start than other session types as they need to set up the full graphical environment.</p> <p>Your session will appear on the Science Portal dashboard.</p> <p>Connection Timing</p> <p>Sometimes it takes a few seconds for the session link to work properly. If you see a \"Bad gateway\" error, wait a moment and try again.</p>"},{"location":"platform/sessions/desktop/#connecting-to-your-desktop","title":"\ud83d\udda5\ufe0f Connecting to Your Desktop","text":""},{"location":"platform/sessions/desktop/#initial-connection","title":"Initial Connection","text":"<p>Click the desktop icon to access the connection page, then click Connect to access your desktop environment.</p>"},{"location":"platform/sessions/desktop/#desktop-environment","title":"Desktop Environment","text":"<p>When you connect, you'll see a full Linux desktop in your browser with the following features:</p>"},{"location":"platform/sessions/desktop/#key-desktop-features","title":"Key Desktop Features","text":"<ul> <li>Taskbar: Application launcher and system controls at bottom</li> <li>File Manager: Browse CANFAR storage and manage files</li> <li>Terminal: Command-line access for advanced operations</li> <li>Application Shortcuts: Quick launch icons for common tools</li> <li>System Menu: Access to additional applications and settings</li> </ul>"},{"location":"platform/sessions/desktop/#session-persistence","title":"Session Persistence","text":"<p>When your session becomes inactive, you'll be returned to the connection page. Click Connect again to resume exactly where you left off - all your applications and work remain open.</p> <p>Session Persistence Features: - Open applications remain running - File locations and window positions preserved - Terminal sessions maintain history - Application states saved automatically</p>"},{"location":"platform/sessions/desktop/#available-software","title":"\ud83d\udee0\ufe0f Available Software","text":""},{"location":"platform/sessions/desktop/#desktop-architecture","title":"Desktop Architecture","text":"<p>The desktop session provides access to astronomy software in two main ways:</p>"},{"location":"platform/sessions/desktop/#1-desktop-shortcuts","title":"1. Desktop Shortcuts","text":"<p>Quick access icons available directly on the desktop: - DS9: FITS image viewer and analysis - Aladin: Interactive sky atlas and visualisation - TOPCAT: Tool for Operations on Catalogues And Tables - Firefox: Web browser for documentation and online tools</p>"},{"location":"platform/sessions/desktop/#2-astro-software-menu","title":"2. Astro Software Menu","text":"<p>Access CANFAR-supported and user-contributed astronomy containers:</p> <ol> <li>Click Applications menu in the taskbar</li> <li>Select Astro Software to browse available containers</li> <li>Choose your desired application to launch in a dedicated container</li> </ol>"},{"location":"platform/sessions/desktop/#available-applications","title":"Available Applications","text":"Application Type Best For DS9 Image Viewer FITS file display, region analysis Aladin Sky Atlas Multi-survey visualisation, catalog overlay TOPCAT Table Tool Catalogue analysis, cross-matching CASA Radio Astronomy Interferometry data reduction astroml Analysis python software stack for astronomy and ML"},{"location":"platform/sessions/desktop/#native-vs-container-applications","title":"Native vs Container Applications","text":"<p>Native Applications (few): - Basic file manager and terminal - Simple text editors - System utilities</p> <p>Container Applications (most astronomy software): - Run in dedicated containers on worker nodes - Connect via X11 forwarding to your desktop - Provide full functionality with isolated environments - Include DS9, CASA, TOPCAT, Aladin, and contributed applications</p> <p>Application Launch Method</p> <p>You cannot start astronomy applications by simply running commands like <code>ds9 &amp;</code> in a terminal. These applications must be launched through desktop shortcuts or the Astro Software menu, as they run in separate containers.</p>"},{"location":"platform/sessions/desktop/#working-with-applications","title":"\ud83e\udded Working with Applications","text":""},{"location":"platform/sessions/desktop/#launching-applications","title":"Launching Applications","text":""},{"location":"platform/sessions/desktop/#method-1-desktop-shortcuts","title":"Method 1: Desktop Shortcuts","text":"<p>Click the shortcut icon on the desktop for immediate access to: - DS9 (FITS viewer) - Aladin (sky atlas) - TOPCAT (table analysis) - Firefox (web browser)</p>"},{"location":"platform/sessions/desktop/#method-2-astro-software-menu","title":"Method 2: Astro Software Menu","text":"<ol> <li>Click Applications in the taskbar</li> <li>Navigate to Astro Software</li> <li>Select the application or container you need</li> <li>Application launches in a new window</li> </ol>"},{"location":"platform/sessions/desktop/#method-3-file-association","title":"Method 3: File Association","text":"<ul> <li>Double-click FITS files to open in DS9 (if available)</li> <li>Right-click files for \"Open with\" options</li> <li>File manager remembers your preferred applications</li> </ul>"},{"location":"platform/sessions/desktop/#example-multi-application-workflow","title":"Example Multi-Application Workflow","text":"<p>Optical Astronomy Analysis:</p> <ol> <li>File Management: Organise data using the graphical file manager</li> <li>Image Display: Open FITS files in DS9 for visual inspection</li> <li>Catalogue Analysis: Load source lists in TOPCAT for analysis</li> <li>Cross-matching: Use TOPCAT to cross-match with online catalogues</li> <li>Documentation: Use Firefox to access documentation and references</li> <li>Scripting: Open terminal for command-line operations as needed</li> </ol>"},{"location":"platform/sessions/desktop/#casa-desktop-usage","title":"CASA Desktop Usage","text":"<p>To use CASA with its graphical interface:</p> <ol> <li>Launch CASA from the Astro Software menu</li> <li>Start CASA in the terminal by typing either <code>casa</code> or <code>casa --pipeline</code> as appropriate. </li> <li>Run CASA tasks as usual via command line or scripts (e.g., calibration or imaging). </li> <li>Access CASA's plotting and visualization tools (e.g., plotms or interactive clean).</li> </ol>"},{"location":"platform/sessions/desktop/#desktop-session-features","title":"\ud83d\udd27 Desktop Session Features","text":""},{"location":"platform/sessions/desktop/#copy-paste-between-containers","title":"Copy &amp; Paste Between Containers","text":"<p>Since different containers may run on separate remote computers, text transfer between applications requires the Clipboard application.</p>"},{"location":"platform/sessions/desktop/#accessing-the-clipboard","title":"Accessing the Clipboard","text":"<ol> <li>Click the arrow at the far left of the desktop taskbar</li> <li>Find \"Clipboard\" in the application menu (middle of list)</li> <li>Click to open the Clipboard application</li> </ol>"},{"location":"platform/sessions/desktop/#using-the-clipboard-for-text-transfer","title":"Using the Clipboard for Text Transfer","text":"<p>The Clipboard functions as an intermediary for text transfer:</p> <p>Transfer Process:</p> <ol> <li>Copy text: Highlight text in source application, use <code>Ctrl+Shift+C</code></li> <li>Transfer to Clipboard: Text appears in the Clipboard application</li> <li>Select in Clipboard: Highlight the text and copy with <code>Ctrl+Shift+C</code></li> <li>Paste to target: Click in destination application, use <code>Ctrl+Shift+V</code></li> </ol> <p>Keyboard Shortcuts</p> <ul> <li>Copy: <code>Ctrl+Shift+C</code></li> <li>Paste: <code>Ctrl+Shift+V</code></li> <li>These shortcuts work consistently across desktop containers</li> </ul>"},{"location":"platform/sessions/desktop/#font-size-adjustment","title":"Font Size Adjustment","text":"<p>Desktop containers support adjustable font sizes for better readability:</p>"},{"location":"platform/sessions/desktop/#changing-terminal-font-size","title":"Changing Terminal Font Size","text":"<ol> <li>Access font menu: Hold <code>Ctrl</code> and right-click in a terminal window</li> <li>Select size: Choose from Small, Medium, or Large options</li> <li>Apply immediately: Font changes take effect instantly</li> </ol> <p>Compatible Applications: - Terminal windows - CASA command-line interface - Text-based applications</p> <p>Font Persistence</p> <p>Font size changes apply only to the current session. You'll need to readjust when starting new sessions.</p>"},{"location":"platform/sessions/desktop/#file-management","title":"\ud83d\udcbe File Management","text":""},{"location":"platform/sessions/desktop/#storage-access","title":"Storage Access","text":"<p>Your desktop session provides access to all CANFAR storage systems:</p> <pre><code>/arc/home/[user]/           # Personal persistent storage (10GB)\n/arc/projects/[project]/    # Shared project storage\n/scratch/                   # Temporary high-speed storage\n</code></pre>"},{"location":"platform/sessions/desktop/#file-operations","title":"File Operations","text":"<p>Use the graphical file manager for:</p> <ul> <li>Visual browsing: Navigate directories with point-and-click</li> <li>Drag-and-drop: Move files between directories easily</li> <li>Preview: View image thumbnails and file properties</li> <li>Batch operations: Select multiple files for operations</li> <li>Permissions: Set file and directory permissions graphically</li> </ul>"},{"location":"platform/sessions/desktop/#file-transfer","title":"File Transfer","text":"<p>Small Files: Drag and drop from your local computer to the file manager Large Files: Use data transfer methods Between Sessions: Files in <code>/arc/</code> are accessible from all session types</p> <p>Persistence Reminder</p> <p>Save important work to <code>/arc/projects/</code> or <code>/arc/home/</code>. Files in <code>/scratch/</code> will not persist after the session ends.</p>"},{"location":"platform/sessions/desktop/#collaboration-and-sharing","title":"\ud83e\udd1d Collaboration and Sharing","text":""},{"location":"platform/sessions/desktop/#session-sharing","title":"Session Sharing","text":"<p>Desktop sessions can be shared for collaborative work:</p> <ol> <li>Copy session URL from browser address bar</li> <li>Share with team members who have CANFAR accounts</li> <li>Coordinate activities to avoid conflicts</li> <li>Use shared storage in <code>/arc/projects/[project]/</code> for collaboration</li> </ol>"},{"location":"platform/sessions/desktop/#collaborative-workflows","title":"Collaborative Workflows","text":"<p>Teaching and Training: - Share desktop session URL with students - Demonstrate software usage in real-time - Students can follow along with same tools</p> <p>Team Analysis: - Multiple researchers access same desktop - Share applications and data simultaneously - Coordinate complex multi-step analyses</p>"},{"location":"platform/sessions/desktop/#best-practices-for-collaboration","title":"Best Practices for Collaboration","text":"<ul> <li>Communicate clearly about who is controlling what</li> <li>Use shared storage for data that everyone needs to access</li> <li>Plan ahead for resource-intensive operations</li> <li>Save work frequently to avoid conflicts</li> </ul>"},{"location":"platform/sessions/firefly/","title":"Firefly Sessions","text":"<p>The LSST table and image visualiser for astronomical data exploration</p> <p>\ud83c\udfaf What You'll Learn</p> <ul> <li>How to launch a Firefly session and choose the right version</li> <li>How to load images, tables, and access CANFAR storage</li> <li>How to perform catalogue overlays, plotting, and cutouts</li> <li>Performance tips for large surveys and troubleshooting guidance</li> </ul> <p>Firefly is a powerful web-based visualisation tool originally developed for the Rubin Observatory LSST. It provides advanced capabilities for viewing images, overlaying catalogues, and analysing tabular data - making it perfect for survey data analysis and multi-wavelength astronomy.</p>"},{"location":"platform/sessions/firefly/#what-is-firefly","title":"\ud83c\udfaf What is Firefly?","text":"<p>Firefly offers specialised tools for:</p> <ul> <li>Image visualisation with advanced stretch and colour controls</li> <li>Catalogue overlay and source analysis tools  </li> <li>Table viewer with filtering, plotting, and statistical tools</li> <li>Multi-wavelength data comparison and analysis</li> <li>Large survey datasets like LSST, HSC, and WISE</li> </ul>"},{"location":"platform/sessions/firefly/#key-features","title":"Key Features","text":"Feature Capability Image Display FITS images with WCS support, multiple panels Catalogue Overlay Plot sources on images, interactive selection Table Analysis Sort, filter, plot columns, statistical analysis Multi-band RGB colour composites, band switching Cutout Services Extract subimages from large surveys Coordinate Systems Support for all standard astronomical coordinates"},{"location":"platform/sessions/firefly/#launching-firefly","title":"\ud83d\ude80 Launching Firefly","text":""},{"location":"platform/sessions/firefly/#step-1-create-new-session","title":"Step 1: Create New Session","text":"<ol> <li>Login to the CANFAR Science Portal</li> <li>Click the plus sign (+) to create a new session</li> <li>Select <code>firefly</code> as your session type</li> </ol>"},{"location":"platform/sessions/firefly/#step-2-choose-container","title":"Step 2: Choose Container","text":"<p>The container selection updates automatically after choosing the session type. Select the Firefly container version you need:</p>"},{"location":"platform/sessions/firefly/#available-versions","title":"Available Versions","text":"<ul> <li>firefly:latest - Most recent stable version (recommended)</li> <li>firefly:X.X - Specific version for reproducible analysis</li> </ul> <p>Version Selection</p> <p>Use the latest version unless you need a specific version for reproducibility. The latest version includes performance improvements and new features.</p>"},{"location":"platform/sessions/firefly/#step-3-configure-session","title":"Step 3: Configure Session","text":""},{"location":"platform/sessions/firefly/#session-name","title":"Session Name","text":"<p>Choose a descriptive name that helps identify your work:</p> <p>Good session names: - <code>lsst-photometry</code> - <code>hsc-catalogue-analysis</code>  - <code>multiband-survey</code> - <code>gaia-cross-match</code></p>"},{"location":"platform/sessions/firefly/#memory-requirements","title":"Memory Requirements","text":"<p>If using a fixed resource session, select RAM based on your data size:</p> <ul> <li>8GB: Small catalogues, single images</li> <li>16GB: Default, suitable for most work</li> <li>32GB: Large catalogues, multiple images</li> <li>64GB: Very large survey datasets</li> </ul> <p>Memory Planning</p> <p>Large tables and multi-image layouts benefit from 32GB+ RAM. Start with 8GB and scale up if needed.</p>"},{"location":"platform/sessions/firefly/#cpu-cores","title":"CPU Cores","text":"<p>Most Firefly work is I/O bound rather than CPU intensive:</p> <ul> <li>2 cores: Default, sufficient for most visualisation tasks</li> <li>4 cores: Large table operations, complex filtering</li> </ul>"},{"location":"platform/sessions/firefly/#step-4-launch-session","title":"Step 4: Launch Session","text":"<ol> <li>Click \"Launch\" button</li> <li>Wait for container initialisation (~30-60 seconds)</li> <li>Session appears on your portal dashboard</li> <li>Click the session icon to access Firefly</li> </ol>"},{"location":"platform/sessions/firefly/#using-firefly","title":"\ud83d\udd25 Using Firefly","text":""},{"location":"platform/sessions/firefly/#interface-overview","title":"Interface Overview","text":"<p>Firefly's interface consists of several main areas:</p> <pre><code>graph TD\n    Interface[Firefly Interface]\n    Interface --&gt; Upload[File Upload Area]\n    Interface --&gt; Images[Image Display]\n    Interface --&gt; Tables[Table Viewer]\n    Interface --&gt; Tools[Analysis Tools]\n\n    Upload --&gt; Local[Local Files]\n    Upload --&gt; URLs[Remote URLs]\n    Upload --&gt; Storage[CANFAR Storage]\n\n    Images --&gt; Display[Image Canvas]\n    Images --&gt; Controls[Display Controls]\n    Images --&gt; Overlays[Catalogue Overlays]\n\n    Tables --&gt; Browse[Data Browser]\n    Tables --&gt; Filter[Filtering Tools]\n    Tables --&gt; Plot[Plotting Tools]</code></pre>"},{"location":"platform/sessions/firefly/#main-components","title":"Main Components","text":"<ul> <li>File Upload Area: Load local files, URLs, or access CANFAR storage</li> <li>Image Display: Multi-panel image viewer with WCS support</li> <li>Table Viewer: Advanced table browser with analysis tools</li> <li>Control Panels: Image display controls, colour maps, overlays</li> </ul>"},{"location":"platform/sessions/firefly/#loading-data","title":"Loading Data","text":""},{"location":"platform/sessions/firefly/#upload-local-files","title":"Upload Local Files","text":"<p>FITS Images:</p> <ol> <li>Click \"Images\" tab</li> <li>Select \"Upload\" </li> <li>Choose FITS file from your computer</li> <li>Image loads automatically with WCS if available</li> </ol> <p>Catalogue Tables:</p> <ol> <li>Click \"Tables\" tab</li> <li>Select \"Upload\"</li> <li>Choose CSV, FITS table, or VOTable</li> <li>Table opens in browser interface</li> </ol>"},{"location":"platform/sessions/firefly/#access-canfar-storage","title":"Access CANFAR Storage","text":"<p>From ARC Projects:</p> <pre><code># Files in your project directory are accessible via:\n/arc/projects/[project]/data/image.fits\n/arc/projects/[project]/data/image_sources.csv\n</code></pre> <p>From VOSpace:</p> <ol> <li>In Firefly, use \"File\" \u2192 \"Open\"</li> <li>Navigate to VOSpace URLs</li> <li>Access: <code>vos://cadc.nrc.ca~vault/[user]/</code></li> </ol>"},{"location":"platform/sessions/firefly/#remote-data-access","title":"Remote Data Access","text":"<p>Survey Archives:</p> <pre><code># Example URLs for Firefly\nhttps://archive.stsci.edu/hlsp/data.fits\nhttps://irsa.ipac.caltech.edu/data/WISE/cutouts/\n</code></pre> <p>Supported Formats:</p> <ul> <li>Images: FITS, JPEG, PNG</li> <li>Tables: CSV, FITS tables, VOTable, IPAC tables</li> <li>Archives: Gzipped files automatically handled</li> </ul>"},{"location":"platform/sessions/firefly/#image-analysis","title":"Image Analysis","text":""},{"location":"platform/sessions/firefly/#basic-image-display","title":"Basic Image Display","text":"<p>Display Controls:</p> <ol> <li>Load FITS image</li> <li>Adjust stretch (log, linear, sqrt)</li> <li>Set scale limits (min/max values)</li> <li>Choose colour table (heat, cool, rainbow)</li> </ol> <p>Navigation:</p> <ul> <li>Zoom: Mouse wheel or zoom controls</li> <li>Pan: Click and drag</li> <li>Centre: Double-click to centre</li> <li>Reset: Reset zoom and pan to default</li> </ul>"},{"location":"platform/sessions/firefly/#multi-band-rgb","title":"Multi-band RGB","text":"<p>Creating RGB Composites:</p> <ol> <li>Load three images (e.g., g, r, i bands)</li> <li>Select \"RGB\" mode</li> <li>Assign each image to R, G, or B channel</li> <li>Adjust relative scaling</li> <li>Fine-tune colour balance</li> </ol>"},{"location":"platform/sessions/firefly/#coordinate-systems","title":"Coordinate Systems","text":"<p>Firefly supports standard coordinate systems:</p> <ul> <li>Equatorial: RA/Dec (J2000, B1950)</li> <li>Galactic: Galactic longitude/latitude</li> <li>Ecliptic: Ecliptic coordinates  </li> <li>Pixel: Image pixel coordinates</li> </ul>"},{"location":"platform/sessions/firefly/#catalogue-analysis","title":"Catalogue Analysis","text":""},{"location":"platform/sessions/firefly/#table-operations","title":"Table Operations","text":"<p>Basic Navigation:</p> <ul> <li>Sort columns: Click headers to sort</li> <li>Filter rows: Use search box for text filtering</li> <li>Select rows: Click rows, Ctrl+click for multiple</li> <li>Pagination: Navigate large tables with page controls</li> </ul> <p>Advanced Filtering:</p> <pre><code>// Example filters (use in filter box):\nmagnitude &lt; 20.5                         // Bright sources\ncolour_g_r &gt; 0.5 &amp;&amp; colour_g_r &lt; 1.5     // Colour selection\ndistance &lt; 100                           // Distance constraint\nra &gt; 180 &amp;&amp; ra &lt; 200                     // RA range\n</code></pre>"},{"location":"platform/sessions/firefly/#statistical-analysis","title":"Statistical Analysis","text":"<p>Built-in Statistics:</p> <ul> <li>Column statistics: Mean, median, std deviation</li> <li>Histogram analysis: Distribution plots</li> <li>Cross-correlation: Compare columns</li> <li>Selection statistics: Stats on filtered data</li> </ul>"},{"location":"platform/sessions/firefly/#plotting-tools","title":"Plotting Tools","text":"<p>Column Plots:</p> <ol> <li>Select table columns for X and Y axes</li> <li>Choose plot type (scatter, histogram, line)</li> <li>Apply colour coding by third column</li> <li>Add error bars if available</li> <li>Customise symbols and colours</li> </ol> <p>Image-Catalogue Overlay:</p> <ol> <li>Load image and catalogue table</li> <li>Match coordinate columns (RA, Dec)</li> <li>Select overlay symbol (circle, cross, diamond)</li> <li>Adjust symbol size and colour</li> <li>Sources appear overlaid on image</li> </ol>"},{"location":"platform/sessions/firefly/#advanced-features","title":"Advanced Features","text":""},{"location":"platform/sessions/firefly/#cutout-services","title":"Cutout Services","text":"<p>Extract subimages from large surveys:</p> <p>Manual Cutouts:</p> <ol> <li>Right-click on image location</li> <li>Select \"Create Cutout\"</li> <li>Specify size (arcmin)</li> <li>Choose format (FITS, JPEG, PNG)</li> <li>Download or save to CANFAR storage</li> </ol> <p>Programmatic Cutouts:</p> <pre><code># Example using Python and Firefly\nimport requests\n\nurl = \"https://irsa.ipac.caltech.edu/cgi-bin/Cutouts/nph-cutouts\"\nparams = {\n    'mission': 'wise',\n    'locstr': '10.68 +41.27',\n    'sizeX': '300',\n    'sizeY': '300'\n}\nresponse = requests.get(url, params=params)\n</code></pre>"},{"location":"platform/sessions/firefly/#multi-wavelength-analysis","title":"Multi-wavelength Analysis","text":"<p>Cross-band Analysis:</p> <ol> <li>Load images in different bands</li> <li>Use \"Blink\" mode to compare</li> <li>Create RGB composite</li> <li>Overlay catalogue with colour-magnitude selection</li> <li>Identify sources across wavelengths</li> </ol> <p>Spectral Energy Distributions:</p> <ol> <li>Load multi-band photometry table</li> <li>Select source of interest</li> <li>Plot flux vs wavelength</li> <li>Fit SED models if available</li> </ol>"},{"location":"platform/sessions/firefly/#data-export","title":"Data Export","text":"<p>Save Results:</p> <ul> <li>Modified tables: CSV, FITS, VOTable formats</li> <li>Image displays: PNG, PDF for publications  </li> <li>Analysis plots: Vector formats for papers</li> <li>Session state: Save/restore workspace</li> </ul> <p>Export Options:</p> <pre><code>File \u2192 Export \u2192 [Format]\n- Tables: CSV, FITS table, VOTable\n- Images: FITS, PNG, JPEG, PDF\n- Plots: PNG, PDF, SVG\n- Session: Save current state\n</code></pre>"},{"location":"platform/sessions/firefly/#common-workflows","title":"\ud83d\udee0\ufe0f Common Workflows","text":""},{"location":"platform/sessions/firefly/#survey-photometry","title":"Survey Photometry","text":"<p>HSC/LSST Photometry Workflow:</p> <ol> <li>Load survey image (HSC, LSST, etc.)</li> <li>Upload photometric catalogue</li> <li>Overlay sources on image</li> <li>Filter by magnitude and colour</li> <li>Create colour-magnitude diagram</li> <li>Export selected sources</li> </ol> <pre><code>// Example: Filter for main sequence stars\n// In Firefly filter box:\n(g_mag - r_mag) &gt; 0.2 &amp;&amp; (g_mag - r_mag) &lt; 1.0 &amp;&amp; r_mag &lt; 22\n</code></pre>"},{"location":"platform/sessions/firefly/#multi-object-analysis","title":"Multi-object Analysis","text":"<p>Target List Processing:</p> <ol> <li>Load target list (CSV with coordinates)</li> <li>Create cutouts around each target</li> <li>Measure properties in each cutout</li> <li>Compile results in table</li> <li>Plot trends and correlations</li> <li>Save analysis products</li> </ol>"},{"location":"platform/sessions/firefly/#cross-matching-catalogues","title":"Cross-matching Catalogues","text":"<p>Gaia Cross-match Example:</p> <ol> <li>Load your source catalogue</li> <li>Load Gaia reference catalogue</li> <li>Perform spatial cross-match</li> <li>Analyse proper motions and parallaxes</li> <li>Create clean stellar sample</li> <li>Export matched catalogue</li> </ol>"},{"location":"platform/sessions/firefly/#time-series-visualisation","title":"Time Series Visualisation","text":"<p>Light Curve Analysis:</p> <ol> <li>Load time-series table (time, magnitude, error)</li> <li>Create light curve plot</li> <li>Apply period folding if needed</li> <li>Identify outliers and trends</li> <li>Export cleaned data</li> </ol>"},{"location":"platform/sessions/firefly/#integration-with-canfar","title":"\ud83d\udd27 Integration with CANFAR","text":""},{"location":"platform/sessions/firefly/#storage-access","title":"Storage Access","text":"<p>ARC Projects:</p> <pre><code># Your project data appears in Firefly file browser\n/arc/projects/[project]/\n\u251c\u2500\u2500 images/           # FITS images\n\u251c\u2500\u2500 catalogues/       # Source tables  \n\u251c\u2500\u2500 results/          # Analysis products\n\u2514\u2500\u2500 plots/            # Exported figures\n</code></pre> <p>VOSpace Integration:</p> <pre><code># Access archived data\nvos://cadc.nrc.ca~vault/[user]/\n\u251c\u2500\u2500 published_data/   # Public datasets\n\u251c\u2500\u2500 working_data/     # Analysis in progress\n\u2514\u2500\u2500 final_products/   # Paper-ready results\n</code></pre>"},{"location":"platform/sessions/firefly/#collaborative-features","title":"Collaborative Features","text":"<p>Session Sharing:</p> <ol> <li>Copy Firefly session URL</li> <li>Share with team members (same CANFAR group)</li> <li>Collaborate on analysis in real-time</li> <li>Each user sees same data and visualisations</li> </ol> <p>Data Sharing:</p> <ol> <li>Save analysis results to shared project space</li> <li>Export publication-quality figures</li> <li>Share VOSpace links for external collaborators</li> <li>Version control important datasets</li> </ol>"},{"location":"platform/sessions/firefly/#working-with-other-canfar-tools","title":"Working with Other CANFAR Tools","text":"<p>Integration Patterns:</p> <ul> <li>Notebooks \u2192 Firefly: Prepare data in Python, visualise in Firefly</li> <li>Firefly \u2192 Desktop: Export results for further analysis in CASA/DS9</li> <li>Batch \u2192 Firefly: Process large datasets, visualise results</li> <li>CARTA \u2192 Firefly: Radio analysis in CARTA, optical follow-up in Firefly</li> </ul>"},{"location":"platform/sessions/notebook/","title":"Notebook Sessions","text":"<p>Interactive Jupyter Lab sessions for data analysis and computational astronomy</p> <p>\ud83c\udfaf What You'll Learn</p> <ul> <li>How to launch and configure Jupyter notebook sessions</li> <li>Available containers and when to use each</li> <li>File management, uploads, and storage integration</li> <li>Performance tips, collaboration, and troubleshooting</li> </ul> <p>Jupyter notebooks combine code execution, rich text documentation, and inline visualisations in a single interface. CANFAR's notebook sessions include pre-configured astronomy software stacks, persistent storage access, and collaborative sharing capabilities.</p>"},{"location":"platform/sessions/notebook/#overview","title":"\ud83d\udccb Overview","text":"<p>Notebook sessions provide:</p> <ul> <li>Jupyter Lab: Full-featured development environment with file browser, terminal, and extensions</li> <li>Pre-configured containers: Astronomy-specific software stacks with popular libraries</li> <li>Persistent storage: Direct access to your <code>/arc/home/</code> and <code>/arc/projects/</code> data</li> <li>Terminal access: Built-in terminal for command-line operations</li> <li>File transfers: Upload/download capabilities for data management</li> </ul>"},{"location":"platform/sessions/notebook/#creating-a-notebook-session","title":"\ud83d\ude80 Creating a Notebook Session","text":""},{"location":"platform/sessions/notebook/#step-1-access-session-creation","title":"Step 1: Access Session Creation","text":"<p>From the Science Portal dashboard, click the plus sign (+) to create a new session, then select notebook as your session type.</p>"},{"location":"platform/sessions/notebook/#step-2-choose-your-container","title":"Step 2: Choose Your Container","text":"<p>Select a container image that includes the software you need. Each container comes pre-configured with specific tools and libraries:</p>"},{"location":"platform/sessions/notebook/#available-containers","title":"Available Containers","text":"<p>There are quite a few containers available, some from the CANFAR team, and the community. Some examples:</p> Container Contents Best For astroml \u2b50 Modern Python astronomy stack (<code>astropy</code>, <code>numpy</code>, <code>scipy</code>, <code>matplotlib</code>, <code>pandas</code>) General astronomy analysis, data science casa-notebook CASA + Python stack Radio astronomy data reduction <p>Container Selection</p> <p>Start with astroml for most astronomy workflows. It includes the latest astronomy libraries and is actively maintained. Use CASA containers only when you specifically need CASA functionality.</p>"},{"location":"platform/sessions/notebook/#step-3-configure-session-resources","title":"Step 3: Configure Session Resources","text":""},{"location":"platform/sessions/notebook/#session-name","title":"Session Name","text":"<p>Choose a descriptive name that helps you identify this session later:</p> <p>Good session names: - <code>galaxy-photometry</code> - <code>pulsar-analysis</code> - <code>alma-data-reduction</code> - <code>exoplanet-search</code></p>"},{"location":"platform/sessions/notebook/#memory-allocation","title":"Memory Allocation","text":"<p>Select the maximum amount of RAM you anticipate requiring:</p> <p>Memory Guidelines: - 4GB: Basic analysis, small datasets - 16GB: Standard workflows, moderate datasets (recommended default) - 32GB: Large datasets, memory-intensive operations - 64GB+: Very large datasets, specialized workflows</p> <p>Resource Sharing</p> <p>Choose the smallest value reasonable for your needs. Computing resources are shared amongst all users. Excessive requests may slow or prevent session launch.</p>"},{"location":"platform/sessions/notebook/#cpu-cores","title":"CPU Cores","text":"<p>Select the maximum number of computing cores you anticipate requiring:</p> <p>CPU Guidelines: - 1-2 cores: Most single-threaded analysis (recommended default) - 4-8 cores: Parallel processing, multi-threaded libraries - 16+ cores: Highly parallel workflows</p>"},{"location":"platform/sessions/notebook/#step-4-launch-session","title":"Step 4: Launch Session","text":"<p>Click the Launch button to start your notebook session.</p> <p>Wait until a notebook icon appears on your dashboard, then click it to access your session:</p>"},{"location":"platform/sessions/notebook/#using-jupyter-lab","title":"\ud83e\udded Using Jupyter Lab","text":""},{"location":"platform/sessions/notebook/#interface-overview","title":"Interface Overview","text":"<p>Once connected, you'll see the Jupyter Lab interface with several key areas:</p> <ul> <li>File Browser (left): Navigate your filesystem and open files</li> <li>Main Work Area (centre): Notebooks, terminals, and file editors</li> <li>Launcher: Create new notebooks, terminals, and files</li> <li>Menu Bar: File operations, edit functions, and view options</li> </ul>"},{"location":"platform/sessions/notebook/#starting-your-first-notebook","title":"Starting Your First Notebook","text":"<ol> <li>Click the Python 3 (ipykernel) notebook icon in the launcher</li> <li>Start coding in the first cell</li> <li>Run cells with <code>Shift+Enter</code></li> </ol>"},{"location":"platform/sessions/notebook/#file-management","title":"File Management","text":""},{"location":"platform/sessions/notebook/#persistent-storage-locations","title":"Persistent Storage Locations","text":"<p>Your notebook session has access to:</p> <pre><code>/arc/home/[user]/           # Your personal 10GB space\n/arc/projects/[project]/    # Shared project spaces\n/scratch/                   # Temporary high-speed storage\n</code></pre>"},{"location":"platform/sessions/notebook/#uploading-files","title":"Uploading Files","text":"<p>Method 1: Direct Upload (&lt; 100MB)</p> <ol> <li>Navigate to your target directory in the file browser</li> <li>Click the upload arrow in the top menu bar</li> <li>Select files and click \"Open\"</li> <li>Files appear in the browser</li> </ol> <p>Method 2: Copy-Paste Text</p> <p>For code snippets or small text files:</p> <ol> <li>Open a terminal by double-clicking the terminal icon</li> <li>Create/edit files using command-line editors</li> <li>Copy text from your local computer</li> <li>Paste into the editor</li> </ol>"},{"location":"platform/sessions/notebook/#working-with-casa","title":"Working with CASA","text":"<p>If using a CASA container, you can run CASA commands directly in notebook cells:</p> <pre><code># Import CASA tasks\nimport casatasks as casa\n\n# Example: Import UV data\ncasa.importuvfits(fitsfile='data.uvfits', vis='data.ms')\n\n# List measurement set contents\ncasa.listobs(vis='data.ms')\n</code></pre>"},{"location":"platform/sessions/notebook/#advanced-features","title":"\ud83d\udd27 Advanced Features","text":""},{"location":"platform/sessions/notebook/#terminal-access","title":"Terminal Access","text":"<p>Access the built-in terminal for command-line operations:</p> <ol> <li>Click the terminal icon in the launcher</li> <li>Run commands as you would in any Linux terminal</li> <li>Install packages with pip or conda (where permissions allow)</li> </ol> <pre><code># Example terminal commands\nls /arc/projects/[project]/\npython script.py\ngit clone https://github.com/username/repo.git\n</code></pre>"},{"location":"platform/sessions/notebook/#jupyter-extensions","title":"Jupyter Extensions","text":"<p>Many useful extensions are pre-installed:</p> <ul> <li>Variable Inspector: View variable contents</li> <li>Table of Contents: Navigate large notebooks</li> <li>Git Integration: Version control directly in Jupyter</li> </ul>"},{"location":"platform/sessions/notebook/#python-package-management","title":"Python Package Management","text":""},{"location":"platform/sessions/notebook/#installing-additional-packages","title":"Installing Additional Packages","text":"<pre><code># Prefer python -m pip for clarity; installs to user site if needed\npython -m pip install --user package-name\n\n# Check installed packages\npython -m pip list | less\n</code></pre>"},{"location":"platform/sessions/notebook/#collaboration","title":"\ud83e\udd1d Collaboration","text":"<p>Focus collaboration on shared project storage and version control, not session URL sharing.</p>"},{"location":"platform/sessions/notebook/#best-practices-for-collaboration","title":"Best Practices for Collaboration","text":"<ul> <li>Use descriptive cell comments for clarity</li> <li>Save frequently to persistent storage</li> <li>Use version control (git) for important work</li> <li>Coordinate changes via pull requests or issue tracking</li> </ul>"},{"location":"platform/sessions/notebook/#sharing-notebooks","title":"Sharing Notebooks","text":"<pre><code># Save notebook to shared location\ncp my-analysis.ipynb /arc/projects/[project]/notebooks/\n\n# Share via git repository\ngit add my-analysis.ipynb\ngit commit -m \"Add analysis notebook\"\ngit push origin main\n</code></pre>"},{"location":"platform/sessions/notebook/#performance-optimisation","title":"\u26a1 Performance Optimisation","text":""},{"location":"platform/sessions/notebook/#memory-management","title":"Memory Management","text":"<pre><code>import psutil\nprint(f\"Memory usage: {psutil.virtual_memory().percent}%\")\n\n# Free up memory by deleting large variables\nif 'large_array' in globals():\n    del large_array\nimport gc\ngc.collect()\n</code></pre>"},{"location":"platform/sessions/notebook/#storage-performance","title":"Storage Performance","text":"<pre><code># Use /scratch for intensive I/O operations\nimport shutil, pathlib\n\nsource = '/arc/projects/[project]/large_file.fits'\ntarget = '/scratch/large_file.fits'\nshutil.copy(source, target)\n\n# ... your analysis code ...\n\n# Copy results back\npathlib.Path('/arc/projects/[project]/outputs/').mkdir(parents=True, exist_ok=True)\nshutil.copy('/scratch/results.fits', '/arc/projects/[project]/outputs/results.fits')\n</code></pre>"},{"location":"platform/sessions/notebook/#efficient-data-loading","title":"Efficient Data Loading","text":"<pre><code>from astropy.io import fits\n\n# Efficient FITS access with context manager and memmap\nwith fits.open('huge_file.fits', memmap=True) as hdul:\n    header = hdul[0].header          # Primary header\n    data_section = hdul[1].data      # Access required extension lazily\n\n# If only header needed\nfrom astropy.io.fits import getheader\nprimary_header = getheader('large_file.fits')\n</code></pre>"},{"location":"platform/sessions/notebook/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"platform/sessions/notebook/#common-issues","title":"Common Issues","text":""},{"location":"platform/sessions/notebook/#kernel-not-starting","title":"Kernel Not Starting","text":"<p>Problem: Python kernel fails to start</p> <p>Solutions: 1. Restart the kernel: <code>Kernel</code> \u2192 <code>Restart Kernel</code> 2. Clear output: <code>Cell</code> \u2192 <code>All Output</code> \u2192 <code>Clear</code> 3. Check memory usage and restart session if needed</p>"},{"location":"platform/sessions/notebook/#out-of-memory-errors","title":"Out of Memory Errors","text":"<p>Problem: <code>MemoryError</code> or kernel crashes</p> <p>Solutions: 1. Restart kernel and clear variables 2. Process data in smaller chunks 3. Use more memory-efficient data types 4. Launch session with more RAM</p>"},{"location":"platform/sessions/notebook/#slow-performance","title":"Slow Performance","text":"<p>Problem: Notebooks running slowly</p> <p>Solutions: 1. Check system resources with <code>htop</code> in terminal 2. Close unused notebooks and terminals 3. Clear notebook output: <code>Cell</code> \u2192 <code>All Output</code> \u2192 <code>Clear</code> 4. Use <code>/scratch</code> for temporary files</p>"},{"location":"platform/sessions/notebook/#file-upload-issues","title":"File Upload Issues","text":"<p>Problem: Cannot upload files or uploads fail</p> <p>Solutions: 1. Check file size (&lt; 100MB for web upload) 2. Use command-line tools for larger files 3. Check available disk space 4. Try uploading to <code>/scratch</code> first, then moving</p>"},{"location":"platform/storage/","title":"CANFAR Storage Systems","text":"<p>A guide to choosing the right storage, understanding how sessions interact with storage, and optimizing your data workflows on the CANFAR platform.</p> <p>CANFAR provides four distinct storage systems, each optimized for different stages of the research lifecycle. Understanding how these systems work together with CANFAR sessions is essential for efficient data management and analysis.</p> <p>Storage Guides</p> <ul> <li>Filesystem Access: ARC storage, SSHFS mounting, and permissions.</li> <li>Data Transfers: Moving data between systems and external sources.</li> <li>VOSpace Guide: Long-term storage, sharing, and archival.</li> </ul>"},{"location":"platform/storage/#storage-options-overview","title":"Storage Options Overview","text":"Storage Path/URI Access (Session/External) Speed Persistence &amp; Backup Default Quota Best For Scratch <code>/scratch</code> Direct FS / N/A Fastest (local SSD) Ephemeral (no backup) ~100GB (per session) High-speed temporary processing, staging I/O. ARC Home <code>/arc/home/[user]</code> Direct FS / SSHFS Fast (CephFS) Permanent (daily snapshots) 10GB Personal configs, scripts, small files. ARC Projects <code>/arc/projects/[project]</code> Direct FS / SSHFS Fast (CephFS) Permanent (daily snapshots) 200GB Active collaborative research data and results. Vault <code>vos:[project|user]</code> API / Web UI Medium Permanent (geo-redundant) Project-dependent Long-term archives, sharing, publication."},{"location":"platform/storage/#checking-quotas-and-requesting-more-space","title":"Checking Quotas and Requesting More Space","text":"<p>You can monitor your storage usage with the following commands:</p> <pre><code># Check ARC storage usage\ndf -h /arc/home/[user]/\ndf -h /arc/projects/[project]/\n\n# For a detailed breakdown of a project directory\ndu -sh /arc/projects/[project]/*\n</code></pre> <p>Vault usage can be monitored via the web interface.</p> <p>To request a quota increase, email <code>support@canfar.net</code> with the project name, current usage, requested space, and a brief justification.</p>"},{"location":"platform/storage/#storage-in-a-session","title":"Storage in a Session","text":"<p>When you start a CANFAR session (like a Notebook or Desktop), the storage systems are integrated seamlessly.</p> <ul> <li>ARC Home and Projects are automatically mounted as standard directories. You can interact with them just like any other folder on a Linux system.</li> <li>Scratch space is provided as a temporary, high-speed directory at <code>/scratch</code>.</li> </ul> <p>This setup allows for a simple and powerful workflow:</p> <pre><code>graph TD\n    Start([Session Starts]) --&gt; Mounts[\"/arc/home &amp; /arc/projects mounted\"]\n    Mounts --&gt; Scratch[\"Empty /scratch created\"]\n    Scratch --&gt; DataWork[\"Analyze data, using /scratch for temporary files\"]\n    DataWork --&gt; Save[\"Save results to /arc/projects\"]\n    Save --&gt; End([Session Ends])\n    End --&gt; Cleanup[\"/scratch is wiped clean\"]</code></pre> <p>Scratch is Temporary</p> <p>Any data left in <code>/scratch</code> is permanently deleted when your session ends. Always copy important files to <code>/arc</code> or <code>vos:</code> before stopping a session.</p>"},{"location":"platform/storage/#storage-strategy-and-performance","title":"Storage Strategy and Performance","text":"<p>Choosing the right storage for each task is key to an efficient workflow. The general principle is to move data to the fastest storage for processing.</p> <p>Storage Speed Hierarchy: 1. Fastest: <code>/scratch</code> (local SSD) 2. Medium: <code>/arc/projects</code> &amp; <code>/arc/home</code> (Shared network filesystem) 3. Slower: <code>vos:</code> (Vault) (optimized for archival)</p>"},{"location":"platform/storage/#common-workflows","title":"Common Workflows","text":""},{"location":"platform/storage/#interactive-analysis","title":"Interactive Analysis","text":"<ul> <li>Your data source: <code>/arc/projects/[project]</code></li> <li>For large files: Copy them to <code>/scratch</code> before processing.</li> <li>Save results to: <code>/arc/projects/[project]/results</code></li> </ul> <p>Example: <pre><code># 1. Copy data to fast, temporary storage\ncp /arc/projects/my_project/large_dataset.fits /scratch/\n\n# 2. Process the data in /scratch\nrun_analysis.py /scratch/large_dataset.fits\n\n# 3. Save the results back to permanent project storage\nmv /scratch/results.csv /arc/projects/my_project/\n</code></pre></p>"},{"location":"platform/storage/#batch-processing","title":"Batch Processing","text":"<ul> <li>Input: Stage data from Vault (<code>vos:</code>), ARC, or the internet into <code>/scratch</code>.</li> <li>Processing: Run your code on the data in <code>/scratch</code>.</li> <li>Output: Save results to ARC for collaboration or to Vault for long-term archival.</li> </ul>"},{"location":"platform/storage/#data-sharing-and-collaboration","title":"Data Sharing and Collaboration","text":"<ul> <li>Active collaboration: Use <code>/arc/projects</code> for shared data and code among team members.</li> <li>External sharing: Use Vault (<code>vos:</code>) to share data with collaborators outside of CANFAR, or for public data releases.</li> </ul>"},{"location":"platform/storage/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":"<p>\"No space left on device\" - This usually means your <code>/arc/home</code> or <code>/arc/projects</code> quota is full. - Use <code>du -sh /path/to/storage/*</code> to find large files and clean up anything you don't need.</p> <p>\"Can't access project directory\" - You may not be a member of the project's group. Contact the project PI to be added.</p> <p>\"Session is slow or unresponsive\" - If you are performing I/O-intensive operations directly in <code>/arc</code>, it can slow down your session. - For better performance, move large files to <code>/scratch</code> for processing.</p> <p>\"My files are gone!\" - You likely saved them to <code>/scratch</code> and the session ended. This data is not recoverable. - Always save important results to <code>/arc</code> or <code>vos:</code> before your session ends.</p>"},{"location":"platform/storage/filesystem/","title":"Filesystem Access","text":"<p>CANFAR's ARC storage systems as filesystems, SSHFS mounting from external computers, and permission management.</p> <p>\ud83c\udfaf Filesystem Guide Overview</p> <p>Master direct storage access:</p> <ul> <li>Session Access: How ARC storage appears within CANFAR computing sessions</li> <li>SSHFS Mounting: Accessing CANFAR storage from your local computer</li> <li>Access Control Lists: Fine-grained permissions for collaborative research  </li> <li>Performance Tips: Optimising filesystem operations and troubleshooting</li> </ul> <p>ARC storage (Home and Projects) can be accessed as standard Unix filesystems both within CANFAR sessions and from external computers via SSHFS. This provides familiar file operations and integrates seamlessly with existing tools and workflows.</p>"},{"location":"platform/storage/filesystem/#arc-storage-as-filesystems","title":"\ud83d\uddc2\ufe0f ARC Storage as Filesystems","text":""},{"location":"platform/storage/filesystem/#within-canfar-sessions","title":"Within CANFAR Sessions","text":"<p>When you start any CANFAR session (Notebook, Desktop, or batch job), ARC storage is automatically mounted as standard directories:</p> <pre><code># Automatic mounts in every session\n/arc/home/[user]/          # Your personal 10GB space\n/arc/projects/[project]/   # Shared project spaces (if member)\n/scratch/                  # Temporary session storage\n</code></pre>"},{"location":"platform/storage/filesystem/#directory-structure-and-conventions","title":"Directory Structure and Conventions","text":""},{"location":"platform/storage/filesystem/#arc-home-directory-archomeuser","title":"ARC Home Directory (<code>/arc/home/[user]/</code>)","text":"<p>Typically the home directory tree structure will be as follows:</p> <pre><code>/arc/home/[user]/\n\u251c\u2500\u2500 .ssh/                   # SSH keys and config\n\u2502   \u251c\u2500\u2500 authorized_keys     # Public keys for SSHFS access\n\u2502   \u2514\u2500\u2500 config              # SSH client configuration\n\u251c\u2500\u2500 .jupyter/               # Jupyter configuration\n\u251c\u2500\u2500 .bashrc                 # Shell configuration\n\u251c\u2500\u2500 .profile                # Environment setup\n\u251c\u2500\u2500 bin/                    # Personal scripts and tools\n\u251c\u2500\u2500 config/                 # Application configurations\n\u2514\u2500\u2500 work/                   # Personal analysis work\n</code></pre> <p>Recommended Use: - Configuration files and dotfiles - Personal code, scripts and utilities - SSH keys for external access - Small reference files and notes</p>"},{"location":"platform/storage/filesystem/#arc-projects-directory-arcprojectsproject","title":"ARC Projects Directory (<code>/arc/projects/[project]/</code>)","text":"<p>Used for team project use. For example, for a propcessing pipeline analysis: <pre><code>/arc/projects/[project]/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/                # Original datasets\n\u2502   \u251c\u2500\u2500 processed/          # Reduced/calibrated data\n\u2502   \u251c\u2500\u2500 catalogs/           # Reference catalogs\n\u2502   \u2514\u2500\u2500 archives/           # Archived datasets\n\u251c\u2500\u2500 code/\n\u2502   \u251c\u2500\u2500 pipelines/          # Data processing workflows  \n\u2502   \u251c\u2500\u2500 analysis/           # Analysis scripts\n\u2502   \u251c\u2500\u2500 notebooks/          # Jupyter notebooks\n\u2502   \u2514\u2500\u2500 tools/              # Project-specific utilities\n\u251c\u2500\u2500 results/\n\u2502   \u251c\u2500\u2500 plots/              # Figures and visualisations\n\u2502   \u251c\u2500\u2500 tables/             # Output catalogues and measurements\n\u2502   \u251c\u2500\u2500 papers/             # Manuscripts and drafts\n\u2502   \u2514\u2500\u2500 presentations/      # Conference materials\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 README.md           # Project documentation\n\u2502   \u251c\u2500\u2500 data_notes.md       # Dataset descriptions\n\u2502   \u2514\u2500\u2500 procedures.md       # Analysis procedures\n\u2514\u2500\u2500 scratch_archive/        # Backed up scratch work\n</code></pre></p>"},{"location":"platform/storage/filesystem/#direct-filesystem-access-within-sessions","title":"\ud83c\udfe0 Direct Filesystem Access (Within Sessions)","text":""},{"location":"platform/storage/filesystem/#basic-operations","title":"Basic Operations","text":"<p>All standard Unix filesystem commands work directly:</p> <pre><code># Navigation\ncd /arc/projects/[project]/\npwd\nls -la\n\n# File operations\ncp source.fits destination.fits\nmv old_name.fits new_name.fits\nrm unwanted_file.fits\n\n# Directory operations\nmkdir -p data/2024/observations/\nrmdir empty_directory/\nfind . -name \"*.fits\" -type f\n\n# Permissions\nchmod 644 data_file.fits          # Read/write owner, read others\nchmod 755 analysis_script.py      # Executable script\nchgrp projectgroup shared_data/   # Change group ownership\n</code></pre>"},{"location":"platform/storage/filesystem/#working-with-large-datasets","title":"Working with Large Datasets","text":"<pre><code># Check available space\ndf -h /arc/projects/[project]/\ndf -h /arc/home/[user]/\n\n# Monitor space usage\ndu -sh /arc/projects/[project]/*\ndu -h --max-depth=2 /arc/projects/[project]/\n\n# Efficient data movement\nrsync -avP /scratch/processed_data/ /arc/projects/[project]/results/\n\n# Archive old data\ntar -czf old_observations_2023.tar.gz data/2023/\nmv old_observations_2023.tar.gz archives/\n</code></pre>"},{"location":"platform/storage/filesystem/#linking-and-shortcuts","title":"Linking and Shortcuts","text":"<pre><code># Create symbolic links for easy access\nln -s /arc/projects/survey/data/master_catalogue.fits ~/current_catalogue.fits\nln -s /arc/projects/[project]/ ~/project\n\n# Hard links (same filesystem only)\nln /arc/projects/shared/reference.fits /arc/home/[user]/my_reference.fits\n\n# Quick navigation with variables\nexport PROJECT_DIR=\"/arc/projects/[project]\"\ncd $PROJECT_DIR/data\n</code></pre>"},{"location":"platform/storage/filesystem/#sshfs-remote-filesystem-access","title":"\ud83c\udf10 SSHFS: Remote Filesystem Access","text":"<p>SSHFS allows you to mount CANFAR's ARC storage on your local computer as if it were a local directory, enabling seamless integration with local tools and workflows.</p>"},{"location":"platform/storage/filesystem/#prerequisites","title":"Prerequisites","text":""},{"location":"platform/storage/filesystem/#local-computer-setup","title":"Local Computer Setup","text":"macOSLinux (Ubuntu/Debian)Linux (Fedora/RedHat) <pre><code># Install macFUSE and SSHFS\nbrew install --cask macfuse\nbrew install sshfs\n\n# Restart or logout/login after installation\n</code></pre> <pre><code># Install SSHFS\nsudo apt update\nsudo apt install sshfs\n\n# Add user to fuse group\nsudo usermod -a -G fuse $USER\n# Logout and login again\n</code></pre> <pre><code># Install SSHFS\nsudo dnf install sshfs\n\n# Add user to fuse group\nsudo usermod -a -G fuse $USER\n</code></pre>"},{"location":"platform/storage/filesystem/#canfar-side-setup","title":"CANFAR Side Setup","text":"<p>You need to set up SSH key authentication on your CANFAR account:</p> <ol> <li> <p>Create SSH key pair (on your local computer):    <pre><code>ssh-keygen -t rsa -b 4096 -f ~/.ssh/canfar_key\n# Enter passphrase when prompted (recommended)\n</code></pre></p> </li> <li> <p>Upload public key to CANFAR:</p> </li> </ol> <p>Method 1: Via Web Interface    - Navigate to ARC File Manager    - Go to your home directory    - Create <code>.ssh</code> folder if it doesn't exist    - Upload your <code>~/.ssh/canfar_key.pub</code> as <code>authorized_keys</code> (if it already exists, you will have to append to the end of the file)</p> <p>Method 2: Via existing session <pre><code># In a CANFAR session, copy your public key content to:\nmkdir -p /arc/home/[user]/.ssh\n# Paste your public key content into authorized_keys file\nnano /arc/home/[user]/.ssh/authorized_keys\nchmod 700 /arc/home/[user]/.ssh\nchmod 600 /arc/home/[user]/.ssh/authorized_keys\n</code></pre></p>"},{"location":"platform/storage/filesystem/#mounting-arc-storage","title":"Mounting ARC Storage","text":""},{"location":"platform/storage/filesystem/#basic-mount","title":"Basic Mount","text":"<pre><code># Create local mount point\nmkdir ~/canfar_arc\n\n# Mount ARC storage\nsshfs -p 64022 -i ~/.ssh/canfar_key \\\n      -o reconnect,ServerAliveInterval=15,ServerAliveCountMax=10 \\\n      [user]@ws-uv.canfar.net:/ ~/canfar_arc/\n\n# On macOS, you may have to add defer_permissions option:\nsshfs -p 64022 -i ~/.ssh/canfar_key \\\n      -o reconnect,ServerAliveInterval=15,ServerAliveCountMax=10,defer_permissions \\\n      [user]@ws-uv.canfar.net:/ ~/canfar_arc/\n</code></pre>"},{"location":"platform/storage/filesystem/#advanced-mount-options","title":"Advanced Mount Options","text":"<pre><code># Mount with optimizations for large files\nsshfs -p 64022 -i ~/.ssh/canfar_key \\\n      -o reconnect,ServerAliveInterval=15,ServerAliveCountMax=10 \\\n      -o cache=yes,kernel_cache,compression=yes \\\n      -o Ciphers=aes128-ctr \\\n      [user]@ws-uv.canfar.net:/ ~/canfar_arc/\n\n# Mount specific project only\nsshfs -p 64022 -i ~/.ssh/canfar_key \\\n      -o reconnect,ServerAliveInterval=15,ServerAliveCountMax=10 \\\n      [user]@ws-uv.canfar.net:/arc/projects/[project] ~/project_mount/\n</code></pre>"},{"location":"platform/storage/filesystem/#connection-configuration","title":"Connection Configuration","text":"<p>Create <code>~/.ssh/config</code> for easier connections:</p> <pre><code>Host canfar\n    HostName ws-uv.canfar.net\n    Port 64022\n    User [user]\n    IdentityFile ~/.ssh/canfar_key\n    ServerAliveInterval 15\n    ServerAliveCountMax 10\n    Compression yes\n</code></pre> <p>Then mount with simpler command: <pre><code>sshfs canfar:/ ~/canfar_arc/\n</code></pre></p>"},{"location":"platform/storage/filesystem/#using-mounted-storage","title":"Using Mounted Storage","text":"<p>Once mounted, use CANFAR storage like any local directory:</p> <pre><code># Navigate to your project\ncd ~/canfar_arc/arc/projects/[project]/\n\n# Copy files from local to CANFAR\ncp ~/local_analysis.py ~/canfar_arc/arc/projects/[project]/code/\n\n# Edit files with local editor\ncode ~/canfar_arc/arc/home/[user]/.bashrc\n\n# Run local tools on CANFAR data\npython analyze_data.py ~/canfar_arc/arc/projects/[project]/data/observations.fits\n\n# Sync directories\nrsync -avz ~/local_scripts/ ~/canfar_arc/arc/projects/[project]/code/\n</code></pre>"},{"location":"platform/storage/filesystem/#unmounting","title":"Unmounting","text":"<pre><code># Unmount when finished\numount ~/canfar_arc\n# or on macOS:\ndiskutil unmount ~/canfar_arc\n\n# Force unmount if needed\numount -f ~/canfar_arc\n# or\nfusermount -u ~/canfar_arc\n</code></pre>"},{"location":"platform/storage/filesystem/#access-control-and-permissions","title":"\ud83d\udd10 Access Control and Permissions","text":""},{"location":"platform/storage/filesystem/#understanding-arc-permissions","title":"Understanding ARC Permissions","text":"<p>ARC storage uses traditional Unix permissions combined with group-based access control:</p>"},{"location":"platform/storage/filesystem/#permission-types","title":"Permission Types","text":"<pre><code># View detailed permissions\nls -l /arc/projects/[project]/\n\n# Example output:\n# drwxrwxr--  projectgroup  data/\n# -rw-rw-r--  projectgroup  analysis.py\n# -rwx------  username      private_script.py\n\n# Permission breakdown:\n# d = directory, - = file\n# rwx = owner permissions (read/write/execute)\n# rwx = group permissions  \n# r-- = other permissions\n</code></pre>"},{"location":"platform/storage/filesystem/#user-and-group-information","title":"User and Group Information","text":"<pre><code># Check your user ID and groups\nid\nwhoami\ngroups\n\n# Check file ownership\nstat /arc/projects/[project]/somefile.fits\n\n# View group membership\ngetent group [project]\n</code></pre>"},{"location":"platform/storage/filesystem/#managing-permissions","title":"Managing Permissions","text":""},{"location":"platform/storage/filesystem/#setting-file-permissions","title":"Setting File Permissions","text":"<pre><code># Make file readable by group\nchmod g+r data_file.fits\n\n# Make script executable\nchmod +x analysis_script.py\n\n# Set specific permission modes\nchmod 664 shared_data.fits     # rw-rw-r--\nchmod 755 public_script.py     # rwxr-xr-x\nchmod 600 private_config.txt   # rw-------\n\n# Recursive permission changes\nchmod -R g+rw shared_directory/\n</code></pre>"},{"location":"platform/storage/filesystem/#group-management","title":"Group Management","text":"<p>Group membership is managed through CANFAR's Group Management system:</p> <ol> <li>Navigate to: Group Management</li> <li>Create or modify groups: Add/remove users from project groups</li> <li>Apply permissions: Use <code>chgrp</code> to assign files to groups</li> </ol> <pre><code># Change group ownership\nchgrp projectgroup /arc/projects/[project]/shared_data.fits\n\n# Change recursively\nchgrp -R projectgroup /arc/projects/[project]/shared_results/\n\n# Set default group for new files in directory\nchmod g+s /arc/projects/[project]/shared_directory/\n</code></pre>"},{"location":"platform/storage/filesystem/#access-control-lists-acls","title":"Access Control Lists (ACLs)","text":"<p>For fine-grained permissions beyond standard Unix permissions:</p> <pre><code># View current ACLs\ngetfacl /arc/projects/[project]/sensitive_data.fits\n\n# Set ACL for specific user\nsetfacl -m u:collaborator:r /arc/projects/[project]/data.fits\n\n# Set ACL for group\nsetfacl -m g:external_collaborators:r /arc/projects/[project]/\n\n# Remove ACL\nsetfacl -x u:former_collaborator /arc/projects/[project]/data.fits\n\n# Set default ACLs for directory\nsetfacl -d -m g:projectgroup:rw /arc/projects/[project]/shared/\n</code></pre>"},{"location":"platform/storage/filesystem/#optimization-and-best-practices","title":"\ud83d\udd27 Optimization and Best Practices","text":""},{"location":"platform/storage/filesystem/#performance-optimization","title":"Performance Optimization","text":""},{"location":"platform/storage/filesystem/#local-filesystem-operations","title":"Local Filesystem Operations","text":"<pre><code># Use rsync for efficient synchronization\nrsync -avz --progress ~/local_data/ /arc/projects/[project]/backup/\n\n# Monitor I/O performance\niostat -x 1    # Live I/O statistics\niotop          # Process I/O usage\n\n# Optimize for large files\n# Use /scratch/ for intensive processing\ncp /arc/projects/[project]/large_dataset.fits /scratch/\nprocess_data /scratch/large_dataset.fits\ncp /scratch/results.fits /arc/projects/[project]/outputs/\n</code></pre>"},{"location":"platform/storage/filesystem/#sshfs-performance-tips","title":"SSHFS Performance Tips","text":"<pre><code># Optimize SSHFS for different use cases\n\n# For frequent small file access:\nsshfs -o cache=yes,kernel_cache,attr_timeout=3600,entry_timeout=3600 \\\n      canfar:/ ~/canfar_arc/\n\n# For large file transfers:\nsshfs -o cache=no,compression=yes,Ciphers=aes128-ctr \\\n      canfar:/ ~/canfar_arc/\n\n# For read-only access (faster):\nsshfs -o ro,cache=yes,kernel_cache \\\n      canfar:/ ~/canfar_arc/\n</code></pre>"},{"location":"platform/storage/filesystem/#workflow-integration","title":"Workflow Integration","text":""},{"location":"platform/storage/filesystem/#local-development-with-canfar-data","title":"Local Development with CANFAR Data","text":"<pre><code># Create development environment\nmkdir ~/canfar_project/\ncd ~/canfar_project/\n\n# Mount CANFAR storage as subdirectory\nmkdir canfar_data\nsshfs canfar:/arc/projects/[project] canfar_data/\n\n# Create local working directory\nmkdir local_work\ncd local_work\n\n# Symlink to CANFAR data for easy access\nln -s ../canfar_data/data ./data\nln -s ../canfar_data/code ./shared_code\n\n# Work locally with CANFAR data\npython shared_code/analysis.py data/observations.fits\n</code></pre>"},{"location":"platform/storage/filesystem/#automated-backup-scripts","title":"Automated Backup Scripts","text":"<pre><code>#!/bin/bash\n# backup_to_canfar.sh - Automated backup script\n\nLOCAL_DIR=\"$HOME/important_work\"\nCANFAR_MOUNT=\"$HOME/canfar_arc\"\nBACKUP_DIR=\"$CANFAR_MOUNT/arc/home/[user]/backups\"\n\n# Check if CANFAR is mounted\nif ! mountpoint -q \"$CANFAR_MOUNT\"; then\n    echo \"Mounting CANFAR storage...\"\n    sshfs canfar:/ \"$CANFAR_MOUNT\"\nfi\n\n# Create backup with timestamp\nDATE=$(date +%Y%m%d_%H%M%S)\nBACKUP_PATH=\"$BACKUP_DIR/backup_$DATE\"\n\necho \"Creating backup: $BACKUP_PATH\"\nrsync -avz --progress \"$LOCAL_DIR/\" \"$BACKUP_PATH/\"\n\n# Keep only last 5 backups\ncd \"$BACKUP_DIR\"\nls -t | tail -n +6 | xargs rm -rf\n\necho \"Backup completed successfully\"\n</code></pre>"},{"location":"platform/storage/filesystem/#troubleshooting","title":"\ud83d\udee0\ufe0f Troubleshooting","text":""},{"location":"platform/storage/filesystem/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"platform/storage/filesystem/#sshfs-connection-problems","title":"SSHFS Connection Problems","text":"<pre><code># Debug SSHFS connection\nsshfs -d -o reconnect,ServerAliveInterval=15,ServerAliveCountMax=10,defer_permissions -p 64022 [user]@ws-uv.canfar.net:/ $HOME/canfar_arc\n# Check mount status\nmount | grep sshfs\ndf -h | grep sshfs\n</code></pre>"},{"location":"platform/storage/filesystem/#permission-denied-errors","title":"Permission Denied Errors","text":"<pre><code># Check your group membership\ngroups\nid\n\n# Verify file permissions\nls -la /arc/projects/[project]/problematic_file\n\n# Check directory execute permissions\nls -ld /arc/projects/[project]/\n</code></pre>"},{"location":"platform/storage/filesystem/#performance-issues","title":"Performance Issues","text":"<pre><code># Check filesystem I/O\niostat -x 1\n\n# Monitor network usage (for SSHFS)\nnetstat -i\niftop\n\n# Test SSHFS performance\ntime ls -la ~/canfar_arc/projects/[project]/\n\n# Remount with performance options\numount ~/canfar_arc\nsshfs -o cache=yes,compression=yes canfar:/ ~/canfar_arc/\n</code></pre>"},{"location":"platform/storage/filesystem/#storage-space-issues","title":"Storage Space Issues","text":"<pre><code># Check quota usage\ndf -h /arc/home/[user]/\ndf -h /arc/projects/[project]/\n\n# Find large files\nfind /arc/projects/[project]]/ -type f -size +100M -exec ls -lh {} \\;\n\n# Clean up space\ndu -sh /arc/projects/[project]/* | sort -hr\n# Remove or archive large unnecessary files\n</code></pre>"},{"location":"platform/storage/filesystem/#diagnostic-commands","title":"Diagnostic Commands","text":"<pre><code># System information\nuname -a\nmount | grep arc\ndf -h\n\n# Network connectivity\nping ws-uv.canfar.net\ntelnet ws-uv.canfar.net 64022\n\n# SSH key verification\nssh-keygen -lf ~/.ssh/canfar_key.pub\nssh-add -l\n\n# SSHFS troubleshooting\nfusermount -V\nsshfs --version\n\n# Permission debugging\ngetfacl /arc/projects/[project]/\nnamei -l /arc/projects/[project]/path/to/file\n</code></pre>"},{"location":"platform/storage/filesystem/#integration-examples","title":"\ud83d\udd17 Integration Examples","text":""},{"location":"platform/storage/filesystem/#ide-and-editor-integration","title":"IDE and Editor Integration","text":""},{"location":"platform/storage/filesystem/#vs-code-with-remote-filesystem","title":"VS Code with Remote Filesystem","text":"<pre><code>// .vscode/settings.json\n{\n    \"python.defaultInterpreterPath\": \"/usr/bin/python\",\n    \"files.watcherExclude\": {\n        \"**/canfar_arc/**\": true\n    },\n    \"search.exclude\": {\n        \"**/canfar_arc/**\": true\n    }\n}\n</code></pre>"},{"location":"platform/storage/filesystem/#jupyter-lab-with-sshfs","title":"Jupyter Lab with SSHFS","text":"<pre><code># In Jupyter Lab, access CANFAR data via mounted filesystem on your laptop\nimport pandas as pd\nfrom astropy.io import fits\n\n# Read data from mounted CANFAR storage\ndata_path = \"$HOME/canfar_arc/arc/projects/[project]/data/\"\ncatalog = pd.read_csv(f\"{data_path}/catalog.csv\")\n\n# Process and save results back to CANFAR\nresults = process_data(catalog)\nresults.to_csv(f\"{data_path}/processed_catalog.csv\")\n</code></pre>"},{"location":"platform/storage/filesystem/#automated-workflows","title":"Automated Workflows","text":""},{"location":"platform/storage/filesystem/#git-repository-sync","title":"Git Repository Sync","text":"<pre><code>#!/bin/bash\n# sync_code_to_canfar.sh\n\nLOCAL_REPO=\"$HOME/my_analysis_code\"\nCANFAR_CODE=\"$HOME/canfar_arc/arc/projects/[project]/code\"\n\ncd \"$LOCAL_REPO\"\n\n# Push local changes to git\ngit add .\ngit commit -m \"Update analysis code\"\ngit push origin main\n\n# Sync to CANFAR\nrsync -avz --exclude='.git' . \"$CANFAR_CODE/\"\n\necho \"Code synchronized to CANFAR\"\n</code></pre>"},{"location":"platform/storage/transfers/","title":"Data Transfers","text":"<p>Moving data between CANFAR storage systems, external sources, and your local computer.</p> <p>\ud83c\udfaf Transfer Methods Overview</p> <p>Efficient data movement strategies:</p> <ul> <li>Web Interfaces: Simple uploads and downloads for small files</li> <li>Command-Line Tools: Efficient transfers for large datasets</li> <li>Automated Workflows: Scripted transfers and synchronisation</li> <li>Performance Optimisation: Choosing the right method for your data size</li> </ul> <p>Efficient data transfer is essential for astronomy workflows. CANFAR provides multiple transfer methods optimised for different scenarios, from small file uploads to large dataset synchronisation.</p>"},{"location":"platform/storage/transfers/#transfer-overview","title":"\ud83d\udd04 Transfer Overview","text":""},{"location":"platform/storage/transfers/#transfer-types-by-method","title":"Transfer Types by Method","text":"Method Best For Speed Complexity Interactive Automated Web Upload/Download Small files (&lt;1GB) Slow Simple \u2705 \u274c Direct URLs Medium files, scripting Medium Simple \u26a0\ufe0f \u2705 VOSpace CLI All sizes, Vault access Medium Medium \u2705 \u2705 SSHFS Mount Local file operations Medium Medium \u2705 \u26a0\ufe0f rsync via SSHFS Large datasets, sync Fast Advanced \u26a0\ufe0f \u2705"},{"location":"platform/storage/transfers/#storage-system-access","title":"Storage System Access","text":"Source \u2192 Destination Method Command Example Local \u2192 ARC Projects SSHFS, Direct URL, VOSpace <code>vcp file.fits vos:/arc:projects/[project]/</code> Local \u2192 Vault VOSpace CLI, Web <code>vcp file.fits vos:[user]/</code> Local \u2192 Scratch Only during sessions <code>cp file.fits /scratch/</code> (within session) ARC \u2192 Vault VOSpace CLI <code>vcp /arc/projects/[project]/file.fits vos:[user]/</code> Vault \u2192 ARC VOSpace CLI <code>vcp vos:[user]/file.fits /arc/projects/[project]/</code> Scratch \u2194 ARC Direct copy <code>cp /scratch/file.fits /arc/projects/[project]/</code>"},{"location":"platform/storage/transfers/#upload-methods","title":"\ud83d\udce4 Upload Methods","text":""},{"location":"platform/storage/transfers/#small-files-1gb-web-interface","title":"Small Files (&lt;1GB): Web Interface","text":""},{"location":"platform/storage/transfers/#arc-projects-and-home","title":"ARC Projects and Home","text":"<ol> <li>Navigate to storage: ARC File Manager</li> <li>Select destination: Choose your home or project directory</li> <li>Upload files: Click \"Add\" \u2192 \"Upload Files\" </li> <li>Select files: Choose files from your computer</li> <li>Confirm upload: Click \"Upload\" then \"OK\"</li> </ol> <p>Note on a notebook session you can also use the JupyterLab Upload button.</p>"},{"location":"platform/storage/transfers/#vault-vospace","title":"Vault (VOSpace)","text":"<ol> <li>Navigate to Vault: VOSpace File Manager</li> <li>Select destination: Browse to your space</li> <li>Upload files: Same process as ARC storage</li> <li>Set permissions: Right-click \u2192 Properties to set sharing permissions</li> </ol>"},{"location":"platform/storage/transfers/#medium-files-1-100gb-command-line","title":"Medium Files (1-100GB): Command Line","text":""},{"location":"platform/storage/transfers/#using-direct-urls-arc-only","title":"Using Direct URLs (ARC only)","text":"<pre><code># Authenticate first\ncadc-get-cert -u [user]\n\n# Upload to ARC Home\ncurl -E ~/.ssl/cadcproxy.pem \\\n     -T myfile.fits \\\n     https://ws-uv.canfar.net/arc/files/home/[user]/myfile.fits\n\n# Upload to ARC Projects  \ncurl -E ~/.ssl/cadcproxy.pem \\\n     -T myfile.fits \\\n     https://ws-uv.canfar.net/arc/files/projects/[project]/myfile.fits\n</code></pre>"},{"location":"platform/storage/transfers/#using-vospace-cli","title":"Using VOSpace CLI","text":"<pre><code># Install VOS tools (if not already available)\npip install vos\n\n# Authenticate\ncadc-get-cert -u [user]\n\n# Upload to Vault\nvcp myfile.fits vos:[user]/data/\n\n# Upload to ARC via VOSpace API\nvcp myfile.fits arc:projects/[project]/data/\n\n# Upload with progress monitoring\nvcp --verbose myfile.fits vos:[user]/large_files/\n</code></pre>"},{"location":"platform/storage/transfers/#large-files-100gb-advanced-methods","title":"Large Files (&gt;100GB): Advanced Methods","text":""},{"location":"platform/storage/transfers/#sshfs-mount-rsync","title":"SSHFS Mount + rsync","text":"<pre><code># 1. Mount CANFAR storage locally\nmkdir ~/canfar_mount\nsshfs -p 64022 [user]@ws-uv.canfar.net:/ ~/canfar_mount\n\n# 2. Sync large datasets with rsync\nrsync -avzP --partial \\\n      ./large_dataset/ \\\n      ~/canfar_mount/arc/projects/[project]/data/\n\n# 3. Unmount when complete\numount ~/canfar_mount\n</code></pre>"},{"location":"platform/storage/transfers/#vospace-bulk-transfer","title":"VOSpace Bulk Transfer","text":"<pre><code># Sync entire directories\nvsync ./local_data/ vos:[user]/backup/\n\n# Parallel transfers (faster for many files)\nvcp --nstreams=4 large_file.tar vos:[user]/archives/\n</code></pre>"},{"location":"platform/storage/transfers/#download-methods","title":"\ud83d\udce5 Download Methods","text":""},{"location":"platform/storage/transfers/#from-arc-storage","title":"From ARC Storage","text":""},{"location":"platform/storage/transfers/#web-interface","title":"Web Interface","text":"<ol> <li>Navigate: ARC File Manager</li> <li>Select files: Check boxes next to desired files</li> <li>Download options:</li> <li>ZIP: Single archive (recommended for multiple files)</li> <li>URL List: Generate download links for scripting</li> <li>HTML List: Individual download links</li> </ol>"},{"location":"platform/storage/transfers/#command-line","title":"Command Line","text":"<pre><code># Direct URL download\ncurl -E ~/.ssl/cadcproxy.pem \\\n     https://ws-uv.canfar.net/arc/files/home/[user]/myfile.fits \\\n     -o myfile.fits\n\n# Via VOSpace API\nvcp arc:home/[user]/myfile.fits ./\n\n# Multiple files with wildcards\nvcp \"arc:projects/[project]/data/*.fits\" ./local_data/\n</code></pre>"},{"location":"platform/storage/transfers/#from-vault-vospace","title":"From Vault (VOSpace)","text":""},{"location":"platform/storage/transfers/#command-line_1","title":"Command Line","text":"<pre><code># Single file\nvcp vos:[user]/data.fits ./\n\n# Directory with all contents\nvcp vos:[user]/survey_data/ ./local_survey/\n</code></pre>"},{"location":"platform/storage/transfers/#python-api","title":"Python API","text":"<pre><code>import vos\n\nclient = vos.Client()\n\n# Download single file\nclient.copy(\"vos:[user]/data.fits\", \"./local_data.fits\")\n\n# Download with progress callback\ndef progress_callback(bytes_transferred, total_bytes):\n    percent = (bytes_transferred / total_bytes) * 100\n    print(f\"Progress: {percent:.1f}%\")\n\nclient.copy(\"vos:[user]/large_file.fits\", \n           \"./large_file.fits\", \n           callback=progress_callback)\n</code></pre>"},{"location":"platform/storage/transfers/#inter-storage-transfers","title":"\ud83d\udd04 Inter-Storage Transfers","text":""},{"location":"platform/storage/transfers/#moving-data-between-storage-systems","title":"Moving Data Between Storage Systems","text":""},{"location":"platform/storage/transfers/#scratch-to-arc-within-sessions","title":"Scratch to ARC (Within Sessions)","text":"<pre><code># Process data in scratch for speed\ncp /arc/projects/[project]/raw_data.fits /scratch/\npython reduce_data.py /scratch/raw_data.fits\n\n# Save results to permanent storage\ncp /scratch/processed_data.fits /arc/projects/[project]/results/\ncp /scratch/analysis_plots/ /arc/projects/[project]/figures/\n</code></pre>"},{"location":"platform/storage/transfers/#arc-to-vault-archival","title":"ARC to Vault (Archival)","text":"<pre><code># Archive completed project results\nvcp /arc/projects/[project]/final_results/ vos:[user]/archives/project2024/\n</code></pre>"},{"location":"platform/storage/transfers/#vault-to-arc-project-setup","title":"Vault to ARC (Project Setup)","text":"<pre><code># Import archived data for new analysis\nvcp vos:shared_project/calibrated_data/ /arc/projects/[project]/data/\n\n# Import specific datasets\nvcp \"vos:public_surveys/gaia_dr3/*.fits\" /arc/projects/[project]/catalogues/\n</code></pre>"},{"location":"platform/storage/transfers/#automated-workflow-example","title":"Automated Workflow Example","text":"<pre><code>#!/bin/bash\n# Complete data processing workflow\n\nset -e  # Exit on error\n\nPROJECT_DIR=\"/arc/projects/[project]\"\nSCRATCH_DIR=\"/scratch\"\n\necho \"Starting data processing pipeline...\"\n\n# 1. Download raw data from Vault to scratch\necho \"Downloading raw data...\"\nvcp vos:[user]/raw_observations/obs_*.fits ${SCRATCH_DIR}/\n\n# 2. Process data in scratch (fastest storage)\necho \"Processing data...\"\ncd ${SCRATCH_DIR}\nfor file in obs_*.fits; do\n    python calibrate.py \"$file\" \"cal_${file}\"\ndone\n\n# 3. Save intermediate results to ARC\necho \"Saving calibrated data...\"\nmkdir -p ${PROJECT_DIR}/calibrated/\ncp cal_*.fits ${PROJECT_DIR}/calibrated/\n\n# 4. Further analysis\necho \"Running analysis...\"\npython analyze_all.py ${PROJECT_DIR}/calibrated/ &gt; analysis_results.txt\n\n# 5. Save final results to ARC and archive to Vault\necho \"Saving final results...\"\ncp analysis_results.txt ${PROJECT_DIR}/results/\ncp final_plots/*.png ${PROJECT_DIR}/figures/\n\n# Archive to Vault\nvcp ${PROJECT_DIR}/results/ vos:[user]/completed_projects/$(date +%Y%m%d)/\n\necho \"Pipeline completed successfully!\"\n</code></pre>"},{"location":"platform/storage/transfers/#performance-optimization","title":"\ud83d\udcca Performance Optimization","text":""},{"location":"platform/storage/transfers/#transfer-speed-optimization","title":"Transfer Speed Optimization","text":""},{"location":"platform/storage/transfers/#for-many-small-files","title":"For Many Small Files","text":"<pre><code># Bundle small files into archives\ntar -czf analysis_scripts.tar.gz scripts/\nvcp analysis_scripts.tar.gz vos:[user]/code/\n\n# Use directory sync instead of individual copies\nvsync --nstreams=4 ./many_small_files/ vos:[user]/collection/\n</code></pre>"},{"location":"platform/storage/transfers/#network-performance-tips","title":"Network Performance Tips","text":""},{"location":"platform/storage/transfers/#optimal-transfer-times","title":"Optimal Transfer Times","text":"<ul> <li>Best performance: Off-peak hours (evenings, weekends)</li> <li>Avoid: Peak research hours (9 AM - 5 PM Pacific)</li> </ul>"},{"location":"platform/storage/transfers/#connection-optimization","title":"Connection Optimization","text":"<pre><code># Check network speed to CANFAR\nping ws-uv.canfar.net\n\n# Test transfer speed with small file\ntime vcp test_file.fits vos:[user]/speed_test/\n</code></pre>"},{"location":"platform/storage/transfers/#error-handling-and-recovery","title":"\ud83d\udea8 Error Handling and Recovery","text":""},{"location":"platform/storage/transfers/#common-transfer-issues","title":"Common Transfer Issues","text":""},{"location":"platform/storage/transfers/#authentication-errors","title":"Authentication Errors","text":"<pre><code># Certificate expired\nERROR:: Expired cert. Update by running cadc-get-cert\n\n# Solution: Refresh certificate\ncadc-get-cert -u [user]\n\n# Check certificate validity\ncadc-get-cert --days-valid\n</code></pre>"},{"location":"platform/storage/transfers/#network-timeouts","title":"Network Timeouts","text":"<pre><code># Retry with exponential backoff\nfor i in {1..3}; do\n    vcp file.fits vos:[user]/ &amp;&amp; break\n    sleep $((2**i))\ndone\n</code></pre>"},{"location":"platform/storage/transfers/#robust-transfer-script","title":"Robust Transfer Script","text":"<pre><code>#!/usr/bin/env python\n\"\"\"\nRobust file transfer with retry logic\n\"\"\"\nimport vos\nimport time\nimport sys\nfrom pathlib import Path\n\ndef robust_transfer(source, destination, max_retries=3):\n    \"\"\"Transfer file with retry logic\"\"\"\n    client = vos.Client()\n\n    for attempt in range(max_retries):\n        try:\n            print(f\"Transfer attempt {attempt + 1}: {source} \u2192 {destination}\")\n            client.copy(source, destination)\n            print(f\"\u2713 Transfer successful\")\n            return True\n\n        except Exception as e:\n            print(f\"\u2717 Attempt {attempt + 1} failed: {e}\")\n            if attempt &lt; max_retries - 1:\n                wait_time = 2 ** attempt  # Exponential backoff\n                print(f\"Waiting {wait_time} seconds before retry...\")\n                time.sleep(wait_time)\n            else:\n                print(f\"Transfer failed after {max_retries} attempts\")\n                return False\n\n# Usage\nif __name__ == \"__main__\":\n    if len(sys.argv) != 3:\n        print(\"Usage: python robust_transfer.py &lt;source&gt; &lt;destination&gt;\")\n        sys.exit(1)\n\n    source, destination = sys.argv[1], sys.argv[2]\n    success = robust_transfer(source, destination)\n    sys.exit(0 if success else 1)\n</code></pre>"},{"location":"platform/storage/transfers/#transfer-checklists","title":"\ud83d\udccb Transfer Checklists","text":""},{"location":"platform/storage/transfers/#pre-transfer-checklist","title":"Pre-Transfer Checklist","text":"<ul> <li> Authentication: Valid CADC certificate (<code>cadc-get-cert</code>)</li> <li> Permissions: Write access to destination directory</li> <li> Space: Sufficient quota in destination storage</li> <li> Network: Stable connection for large transfers</li> <li> Backup: Important data backed up before moving</li> </ul>"},{"location":"platform/storage/transfers/#post-transfer-verification","title":"Post-Transfer Verification","text":"<pre><code># Verify file integrity\nvls -l vos:[user]/transferred_file.fits  # Check size and timestamp\n\n# Compare checksums (if available)\nvcp --head vos:[user]/data.fits | grep MD5\n\n# Test file readability\npython -c \"from astropy.io import fits; fits.open('test_file.fits')\"\n</code></pre>"},{"location":"platform/storage/transfers/#transfer-planning-template","title":"Transfer Planning Template","text":"<pre><code>## Transfer Plan: [Project Name]\n\n**Data Description**: \n- Size: ___GB\n- File count: ___\n- Type: Raw/Processed/Results\n\n**Source**: _______________\n**Destination**: ___________\n**Method**: _______________\n\n**Timeline**:\n- Start: ____________\n- Estimated completion: ___________\n\n**Verification**:\n- [ ] File count matches\n- [ ] Total size matches  \n- [ ] Sample files readable\n- [ ] Permissions set correctly\n\n**Backup**: _______________\n</code></pre>"},{"location":"platform/storage/transfers/#integration-examples","title":"\ud83d\udd17 Integration Examples","text":""},{"location":"platform/storage/transfers/#jupyter-notebook-upload","title":"Jupyter Notebook Upload","text":"<p>Within a CANFAR Jupyter session:</p> <pre><code># Upload files using the Jupyter interface\n# 1. Click the \"Upload\" button in file browser\n# 2. Select files from your computer\n# 3. Files appear in current directory\n\n# Move uploaded files to appropriate storage\nimport shutil\nshutil.move('uploaded_data.fits', '/arc/projects/[project]/data/')\n\n# Or copy to scratch for processing\nshutil.copy('/arc/projects/[project]/data.fits', '/scratch/')\n</code></pre>"},{"location":"platform/storage/transfers/#batch-job-data-staging","title":"Batch Job Data Staging","text":"<pre><code>#!/bin/bash\n# Batch job with data staging\n\n# Download input data\nvcp vos:project/input_data.tar.gz /scratch/\ncd /scratch\ntar -xzf input_data.tar.gz\n\n# Process data\npython analysis.py input_data/\n\n# Upload results\ntar -czf results_$(date +%Y%m%d).tar.gz results/\nvcp results_*.tar.gz vos:[user]/job_outputs/\n\n# Cleanup\nrm -rf /scratch/*\n</code></pre>"},{"location":"platform/storage/transfers/#external-data-import","title":"External Data Import","text":"<pre><code># Download from astronomical archives\nwget -O survey_data.fits \"https://archive.eso.org/...\"\n\n# Upload to CANFAR\nvcp survey_data.fits vos:[user]/external_data/\n\n# Or direct to project space\ncurl -E ~/.ssl/cadcproxy.pem \\\n     -T survey_data.fits \\\n     https://ws-uv.canfar.net/arc/files/projects/[project]/survey_data.fits\n</code></pre>"},{"location":"platform/storage/vospace/","title":"VOSpace","text":"<p>\ud83c\udfaf VOSpace Guide Overview</p> <p>Master CANFAR's long-term storage system:</p> <ul> <li>VOSpace Concepts: Understanding IVOA standards and when to use Vault</li> <li>Web Interface: Browser-based file management and sharing</li> <li>Command-Line Tools: Efficient bulk operations and automation</li> <li>Python API: Programmatic access for workflows and integration</li> <li>Metadata &amp; Sharing: Rich data descriptions and collaborative access</li> </ul> <p>VOSpace is CANFAR's implementation of the International Virtual Observatory Alliance (IVOA) VOSpace standard, providing long-term, secure, and collaborative storage for astronomy data. It serves as both an archive and a data sharing platform.</p>"},{"location":"platform/storage/vospace/#vospace-overview","title":"\ud83c\udf10 VOSpace Overview","text":""},{"location":"platform/storage/vospace/#what-is-vospace","title":"What is VOSpace?","text":"<p>VOSpace is a distributed storage service that allows astronomers to:</p> <ul> <li>Store data persistently with geographic redundancy</li> <li>Share data with collaborators and the public</li> <li>Organize data with hierarchical directories and metadata</li> <li>Access data programmatically via standardized APIs</li> <li>Integrate with Virtual Observatory tools and services</li> </ul>"},{"location":"platform/storage/vospace/#vault-vospace-vs-arc-vospace-vs-scratch","title":"Vault VOSpace vs ARC VOSpace vs Scratch","text":"Feature Vault ARC Projects ARC Home Scratch Persistence \u2705 Permanent \u2705 Permanent \u2705 Permanent \u274c Session only Backup \u2705 Geo-redundant \u26a0\ufe0f Basic \u26a0\ufe0f Basic \u274c None Sharing \u2705 Flexible permissions \u26a0\ufe0f Group-based \u26a0\ufe0f User-based \u274c Session only Public access \u2705 Public URLs \u274c Private \u274c Private \u274c Session only Metadata \u2705 Rich metadata \u26a0\ufe0f Basic \u26a0\ufe0f Basic \u274c None API access \u2705 VOSpace API \u2705 VOSpace API \u2705 VOSpace API \u274c None Speed Slow (network) Medium (network) Medium (network) Fast (SSD)"},{"location":"platform/storage/vospace/#web-interface","title":"\ud83c\udf0d Web Interface","text":""},{"location":"platform/storage/vospace/#accessing-vospace","title":"Accessing VOSpace","text":"<ol> <li>Navigate to:<ul> <li>Vault VOSpace File Manager</li> <li>ARC VOSpace File Manager</li> </ul> </li> <li>Login: Use your CADC credentials</li> <li>Browse: Navigate through your space and shared spaces</li> </ol>"},{"location":"platform/storage/vospace/#web-interface-features","title":"Web Interface Features","text":""},{"location":"platform/storage/vospace/#file-operations","title":"File Operations","text":"<ul> <li>Upload: Drag and drop or click \"Add\" \u2192 \"Upload Files\"</li> <li>Download: Select files \u2192 \"Download\" (ZIP, URL list, or HTML list)</li> <li>Create folders: \"Add\" \u2192 \"Create Folder\"</li> <li>Delete: Select items \u2192 \"Delete\"</li> <li>Move/Copy: Drag and drop or cut/paste</li> </ul>"},{"location":"platform/storage/vospace/#sharing-and-permissions","title":"Sharing and Permissions","text":"<pre><code>Right-click file/folder \u2192 Properties \u2192 Permissions\n\nPermission Types:\n- Read (r): View and download\n- Write (w): Modify and delete  \n- Execute (x): Navigate directories\n\nTarget Groups:\n- Owner: You (full control)\n- Group: Project members\n- Other: Public access\n</code></pre>"},{"location":"platform/storage/vospace/#command-line-interface","title":"\ud83d\udcbb Command Line Interface","text":""},{"location":"platform/storage/vospace/#installation","title":"Installation","text":"<p>VOSpace tools are pre-installed in CANFAR sessions in CANFAR-maintained containers such as <code>astroml</code>.  For local or custom installation, use <code>pip</code>:</p> <pre><code># Install VOS python module with vcp/vsync/vls/vchmod/vmkdir commands\npip install vos\n\n# Verify installation\nvls --help\nvcp --help\n</code></pre>"},{"location":"platform/storage/vospace/#authentication","title":"Authentication","text":"<pre><code># Get security certificate (valid 24 hours)\ncadc-get-cert -u [user]\n\n# Verify authentication\nvls vos:[user]\n</code></pre>"},{"location":"platform/storage/vospace/#basic-operations","title":"Basic Operations","text":""},{"location":"platform/storage/vospace/#directory-operations","title":"Directory Operations","text":"<pre><code># List directories and files\nvls vos:[user]/                    # Your root directory\nvls vos:[user]/projects/           # Subdirectory\nvls -l vos:[user]/data/            # Detailed listing\n\n# Create directories\nvmkdir vos:[user]/new_project/\nvmkdir vos:[user]/data/2024/\n\n# Navigate hierarchically\nvls vos:[user]/projects/survey/data/\n</code></pre>"},{"location":"platform/storage/vospace/#file-operations_1","title":"File Operations","text":"<pre><code># Upload files\nvcp mydata.fits vos:[user]/data/\nvcp *.fits vos:[user]/observations/\n# vcp is recursive\nvcp ./analysis_scripts/ vos:[user]/code/ \n\n# Download files  \nvcp vos:[user]/data/results.fits ./\nvcp \"vos:[user]/observations/*.fits\" ./data/\nvcp vos:[user]/code/ ./local_scripts/\n\n# Copy between VOSpace locations\nvcp vos:[user]/data/obs1.fits vos:[user]/backup/\n</code></pre>"},{"location":"platform/storage/vospace/#file-management","title":"File Management","text":"<pre><code># Move/rename files\nvmv vos:[user]/old_name.fits vos:[user]/new_name.fits\nvmv vos:[user]/temp/ vos:[user]/archive/\n\n# Delete files and directories\nvrm vos:[user]/old_file.fits\nvrm vos:[user]/old_directory/\n\n# View file contents (for text files)\nvcat vos:[user]/catalog.csv\n</code></pre>"},{"location":"platform/storage/vospace/#advanced-operations","title":"Advanced Operations","text":""},{"location":"platform/storage/vospace/#bulk-operations","title":"Bulk Operations","text":"<pre><code># Synchronise directories\nvsync ./local_data/ vos:[user]/backup/\nvsync vos:[user]/analysis/ ./local_analysis/\n\n# Parallel transfers for speed\nvsync --nstreams=4 huge_dataset.tar vos:[user]/archives/\n</code></pre>"},{"location":"platform/storage/vospace/#permission-management","title":"Permission Management","text":"<pre><code># Make file publicly readable\nvchmod o+r vos:[user]/public_catalog.fits\n\n# Grant group access\nvchmod g+rw vos:[user]/shared_data.fits\n\n# Set permissions for specific groups\nvchmod g+r:external-collaborators vos:[user]/collaboration_data/\n\n# View current permissions\nvls -l vos:[user]/myfile.fits\n</code></pre>"},{"location":"platform/storage/vospace/#data-cutouts-and-processing","title":"Data Cutouts and Processing","text":"<pre><code># FITS cutouts (pixel coordinates)\nvcp \"vos:[user]/image.fits[100:200,100:200]\" ./cutout.fits\n\n# Header-only download\nvcp --head vos:[user]/large_image.fits ./headers.txt\n\n# Inspect headers without downloading\nvcat --head vos:[user]/observation.fits\n</code></pre>"},{"location":"platform/storage/vospace/#python-api","title":"\ud83d\udc0d Python API","text":""},{"location":"platform/storage/vospace/#basic-setup","title":"Basic Setup","text":"<pre><code>import vos\nfrom vos import Client\n\n# Initialize client (uses existing authentication)\nclient = Client()\n\n# Alternative: specify authentication\nclient = Client(username='[user]', password='[password]')\n</code></pre>"},{"location":"platform/storage/vospace/#file-operations_2","title":"File Operations","text":"<pre><code># List directory contents\nfiles = client.listdir('vos:[user]/')\nprint(f\"Found {len(files)} files\")\n\n# Check if file exists\nexists = client.isfile('vos:[user]/data.fits')\nif not exists:\n    print(\"File not found\")\n\n# Get file information\ninfo = client.get_info('vos:[user]/data.fits')\nprint(f\"Size: {info['size']} bytes\")\nprint(f\"Modified: {info['date']}\")\n\n# Copy files\nclient.copy('mydata.fits', 'vos:[user]/uploads/mydata.fits')\nclient.copy('vos:[user]/results.txt', './local_results.txt')\n\n# Create directories\nclient.mkdir('vos:[user]/new_project/')\n\n# Delete files\nclient.delete('vos:[user]/old_file.fits')\n</code></pre>"},{"location":"platform/storage/vospace/#advanced-python-usage","title":"Advanced Python Usage","text":""},{"location":"platform/storage/vospace/#batch-processing","title":"Batch Processing","text":"<pre><code>import os\nfrom pathlib import Path\n\ndef process_vospace_directory(vospace_path, local_temp_dir):\n    \"\"\"Download, process, and re-upload files from VOSpace\"\"\"\n\n    # Create local working directory\n    Path(local_temp_dir).mkdir(exist_ok=True)\n\n    # List files in VOSpace\n    files = client.listdir(vospace_path)\n    fits_files = [f for f in files if f.endswith('.fits')]\n\n    for fits_file in fits_files:\n        vospace_file = f\"{vospace_path}/{fits_file}\"\n        local_file = f\"{local_temp_dir}/{fits_file}\"\n        processed_file = f\"{local_temp_dir}/processed_{fits_file}\"\n\n        # Download\n        print(f\"Downloading {fits_file}\")\n        client.copy(vospace_file, local_file)\n\n        # Process (example: your analysis here)\n        process_fits_file(local_file, processed_file)\n\n        # Upload processed version\n        processed_vospace = f\"{vospace_path}/processed_{fits_file}\"\n        client.copy(processed_file, processed_vospace)\n\n        # Cleanup local files\n        os.remove(local_file)\n        os.remove(processed_file)\n\n# Usage\nprocess_vospace_directory('vos:[user]/raw_data', './temp_processing')\n</code></pre>"},{"location":"platform/storage/vospace/#metadata-management","title":"Metadata Management","text":"<pre><code># Get file node (for metadata operations)\nnode = client.get_node('vos:[user]/observation.fits')\n\n# Set metadata\nnode.props['TELESCOPE'] = 'ALMA'\nnode.props['OBJECT'] = 'NGC1365'\nnode.props['DATE-OBS'] = '2024-03-15T10:30:00'\n\n# Update node with new metadata\nclient.update(node)\n\n# Read metadata\nprops = node.props\ntelescope = props.get('TELESCOPE', 'Unknown')\nobject_name = props.get('OBJECT', 'Unknown')\n\nprint(f\"Observation of {object_name} with {telescope}\")\n</code></pre>"},{"location":"platform/storage/vospace/#progress-monitoring","title":"Progress Monitoring","text":"<pre><code>def upload_with_progress(local_file, vospace_path):\n    \"\"\"Upload file with progress monitoring\"\"\"\n\n    file_size = os.path.getsize(local_file)\n\n    def progress_callback(bytes_transferred):\n        percent = (bytes_transferred / file_size) * 100\n        print(f\"\\rProgress: {percent:.1f}% ({bytes_transferred}/{file_size} bytes)\", end='')\n\n    try:\n        client.copy(local_file, vospace_path, callback=progress_callback)\n        print(\"\\nUpload completed successfully!\")\n    except Exception as e:\n        print(f\"\\nUpload failed: {e}\")\n\n# Usage\nupload_with_progress('large_dataset.fits', 'vos:[user]/archives/dataset.fits')\n</code></pre>"},{"location":"platform/storage/vospace/#sharing-and-collaboration","title":"\ud83d\udd12 Sharing and Collaboration","text":""},{"location":"platform/storage/vospace/#permission-levels","title":"Permission Levels","text":""},{"location":"platform/storage/vospace/#owner-permissions","title":"Owner Permissions","text":"<ul> <li>Full control: Read, write, delete, change permissions</li> <li>Default: Only owner has access to new files</li> </ul>"},{"location":"platform/storage/vospace/#group-permissions","title":"Group Permissions","text":"<ul> <li>Read: Group members can view and download</li> <li>Write: Group members can modify and upload</li> <li>Execute: Group members can navigate directories</li> </ul>"},{"location":"platform/storage/vospace/#public-permissions","title":"Public Permissions","text":"<ul> <li>Read: Anyone with the URL can download</li> <li>Useful for: Publishing datasets, sharing with external collaborators</li> </ul>"},{"location":"platform/storage/vospace/#setting-up-sharing","title":"Setting Up Sharing","text":""},{"location":"platform/storage/vospace/#command-line-sharing","title":"Command Line Sharing","text":"<pre><code># Make dataset publicly available\nvchmod o+r vos:[user]/public_datasets/gaia_subset.fits\n\n# Share with research group\nvchmod g+rw:my_research_group vos:[user]/shared_analysis/\n\n# Create public directory\nvmkdir vos:[user]/public/\nvchmod o+r vos:[user]/public/\n\n# Share specific project data\nvchmod g+r:external_collaborators vos:[user]/collaboration/survey_data/\n</code></pre>"},{"location":"platform/storage/vospace/#public-urls","title":"Public URLs","text":"<pre><code># Files with public read permissions get accessible URLs:\n# https://ws-cadc.canfar.net/vault/nodes/[user]/public_file.fits\n\n# Direct download links for shared data:\ncurl -O https://ws-cadc.canfar.net/vault/nodes/[user]/public/catalog.csv\n</code></pre>"},{"location":"platform/storage/vospace/#collaboration-workflows","title":"Collaboration Workflows","text":""},{"location":"platform/storage/vospace/#multi-institutional-project","title":"Multi-Institutional Project","text":"<pre><code># Project coordinator sets up shared space\nvmkdir vos:[project]/data\nvmkdir vos:[project]/public\nvmkdir vos:[project]/results\nvchmod g+rw:all_collaborators vos:[project]/data/\n\n# Collaborators contribute data\nvcp local_observations.fits vos:[project]/data/institution_a/\nvcp analysis_results.csv vos:[project]/results/\n\n# Public data release\nvcp vos:[project]/data/final_catalogue.fits vos:[project]/public/\nvchmod o+r vos:[project]/public/final_catalogue.fits\n</code></pre>"},{"location":"platform/storage/vospace/#data-publication","title":"Data Publication","text":"<pre><code>import vos\n\ndef publish_dataset(local_files, publication_space):\n    \"\"\"Publish dataset with proper metadata\"\"\"\n\n    client = vos.Client()\n\n    # Create publication directory\n    client.mkdir(publication_space)\n\n    for local_file in local_files:\n        filename = os.path.basename(local_file)\n        vospace_path = f\"{publication_space}/{filename}\"\n\n        # Upload file\n        client.copy(local_file, vospace_path)\n\n        # Set metadata\n        node = client.get_node(vospace_path)\n        node.props['AUTHOR'] = 'Dr. Astronomer'\n        node.props['PUBLICATION'] = 'ApJ 2024, 123, 456'\n        node.props['DOI'] = '10.1088/example'\n        client.update(node)\n\n        # Make publicly accessible\n        client.set_permissions(vospace_path, public_read=True)\n\n        print(f\"Published: {vospace_path}\")\n\n# Usage\nfiles_to_publish = ['final_catalog.fits', 'processed_images.tar.gz']\npublish_dataset(files_to_publish, 'vos:[user]/publications/survey2024')\n</code></pre>"},{"location":"platform/storage/vospace/#integration-with-astronomical-tools","title":"\ud83d\udd27 Integration with Astronomical Tools","text":""},{"location":"platform/storage/vospace/#fits-file-handling","title":"FITS File Handling","text":"<pre><code>from astropy.io import fits\nimport tempfile\nimport os\n\ndef analyze_vospace_fits(vospace_path):\n    \"\"\"Analyze FITS file stored in VOSpace\"\"\"\n\n    # Download to temporary file\n    with tempfile.NamedTemporaryFile(suffix='.fits', delete=False) as tmp:\n        client.copy(vospace_path, tmp.name)\n\n        # Open with astropy\n        with fits.open(tmp.name) as hdul:\n            header = hdul[0].header\n            data = hdul[0].data\n\n            # Perform analysis\n            mean_value = data.mean()\n            max_value = data.max()\n\n            print(f\"Image stats: mean={mean_value:.2f}, max={max_value:.2f}\")\n\n            # Extract key information\n            telescope = header.get('TELESCOP', 'Unknown')\n            object_name = header.get('OBJECT', 'Unknown')\n\n        # Cleanup\n        os.unlink(tmp.name)\n\n    return {'mean': mean_value, 'max': max_value, 'telescope': telescope}\n\n# Usage\nstats = analyze_vospace_fits('vos:[user]/observations/ngc1365.fits')\n</code></pre>"},{"location":"platform/storage/vospace/#integration-with-archives","title":"Integration with Archives","text":"<pre><code>def mirror_archive_data(archive_url, vospace_destination):\n    \"\"\"Download from astronomical archive and store in VOSpace\"\"\"\n\n    import requests\n    import tempfile\n\n    # Download from archive\n    response = requests.get(archive_url)\n\n    # Save to temporary file\n    with tempfile.NamedTemporaryFile(delete=False) as tmp:\n        tmp.write(response.content)\n        tmp_path = tmp.name\n\n    try:\n        # Upload to VOSpace\n        client.copy(tmp_path, vospace_destination)\n\n        # Set metadata about source\n        node = client.get_node(vospace_destination)\n        node.props['ARCHIVE_URL'] = archive_url\n        node.props['DOWNLOAD_DATE'] = datetime.now().isoformat()\n        client.update(node)\n\n        print(f\"Mirrored {archive_url} to {vospace_destination}\")\n\n    finally:\n        os.unlink(tmp_path)\n\n# Example: Mirror HST data\nmirror_archive_data(\n    'https://archive.stsci.edu/missions/hubble/...',\n    'vos:[user]/hst_data/observation_123.fits'\n)\n</code></pre>"},{"location":"platform/storage/vospace/#performance-and-optimization","title":"\ud83d\udcca Performance and Optimization","text":""},{"location":"platform/storage/vospace/#transfer-performance","title":"Transfer Performance","text":""},{"location":"platform/storage/vospace/#caching-and-local-mirrors","title":"Caching and Local Mirrors","text":"<pre><code>import hashlib\nfrom pathlib import Path\n\nclass VOSpaceCache:\n    def __init__(self, cache_dir='./vospace_cache'):\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(exist_ok=True)\n        self.client = vos.Client()\n\n    def get_cached_file(self, vospace_path, force_refresh=False):\n        \"\"\"Get file from cache or download if needed\"\"\"\n\n        # Generate cache filename\n        cache_name = hashlib.md5(vospace_path.encode()).hexdigest()\n        cache_file = self.cache_dir / cache_name\n\n        # Check if cache is valid\n        if not force_refresh and cache_file.exists():\n            # Compare modification times\n            local_mtime = cache_file.stat().st_mtime\n            try:\n                remote_info = self.client.get_info(vospace_path)\n                remote_mtime = remote_info['date']\n\n                if local_mtime &gt;= remote_mtime:\n                    print(f\"Using cached version: {cache_file}\")\n                    return str(cache_file)\n            except:\n                pass\n\n        # Download fresh copy\n        print(f\"Downloading {vospace_path} to cache\")\n        self.client.copy(vospace_path, str(cache_file))\n        return str(cache_file)\n\n# Usage\ncache = VOSpaceCache()\nlocal_file = cache.get_cached_file('vos:[user]/large_catalog.fits')\n</code></pre>"},{"location":"platform/storage/vospace/#monitoring-and-logging","title":"Monitoring and Logging","text":"<pre><code>import logging\nimport time\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef monitored_transfer(source, destination):\n    \"\"\"Transfer with monitoring and timing\"\"\"\n\n    start_time = time.time()\n    logger.info(f\"Starting transfer: {source} \u2192 {destination}\")\n\n    try:\n        client.copy(source, destination)\n\n        end_time = time.time()\n        duration = end_time - start_time\n\n        # Get file size for speed calculation\n        if source.startswith('vos:'):\n            info = client.get_info(source)\n            size_mb = info['size'] / (1024 * 1024)\n        else:\n            size_mb = os.path.getsize(source) / (1024 * 1024)\n\n        speed = size_mb / duration if duration &gt; 0 else 0\n\n        logger.info(f\"Transfer completed: {size_mb:.1f} MB in {duration:.1f}s ({speed:.1f} MB/s)\")\n        return True\n\n    except Exception as e:\n        logger.error(f\"Transfer failed: {e}\")\n        return False\n\n# Usage\nsuccess = monitored_transfer('large_file.fits', 'vos:[user]/archives/')\n</code></pre>"},{"location":"platform/storage/vospace/#troubleshooting","title":"\ud83d\udee0\ufe0f Troubleshooting","text":""},{"location":"platform/storage/vospace/#common-issues","title":"Common Issues","text":""},{"location":"platform/storage/vospace/#authentication-problems","title":"Authentication Problems","text":"<pre><code># Certificate expired\ncadc-get-cert -u [user]\n\n# Check certificate validity\ncadc-get-cert --days-valid\n\n# Clear certificate cache\nrm ~/.ssl/cadcproxy.pem\ncadc-get-cert -u [user]\n</code></pre>"},{"location":"platform/storage/vospace/#permission-errors","title":"Permission Errors","text":"<pre><code># Check file permissions\nvls -l vos:[user]/file.fits\n\n# Verify directory permissions\nvls -l vos:[user]/\n\n# Check group membership\n# (Contact CANFAR support if needed)\n</code></pre>"},{"location":"platform/storage/vospace/#network-and-transfer-issues","title":"Network and Transfer Issues","text":"<pre><code># Test connectivity\nping ws-cadc.canfar.net\n\n# Check VOSpace service status\nvls vos:\n\n# Retry with different parameters\nvcp --timeout=3600 large_file.fits vos:[user]/  # Increase timeout\nvcp --nstreams=1 problematic_file.fits vos:[user]/  # Reduce streams\n</code></pre>"},{"location":"platform/storage/vospace/#debugging-and-diagnostics","title":"Debugging and Diagnostics","text":"<pre><code>import vos\nimport logging\n\n# Enable debug logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Get detailed client information\nclient = vos.Client()\nprint(f\"VOSpace endpoint: {client.vospace_url}\")\nprint(f\"Authentication: {client.get_auth()}\")\n\n# Test basic operations\ntry:\n    files = client.listdir('vos:')\n    print(f\"Root access successful, found {len(files)} items\")\nexcept Exception as e:\n    print(f\"Root access failed: {e}\")\n\n# Check specific paths\ntest_paths = ['vos:[user]/', 'vos:[user]/data/']\nfor path in test_paths:\n    try:\n        contents = client.listdir(path)\n        print(f\"\u2713 {path}: {len(contents)} items\")\n    except Exception as e:\n        print(f\"\u2717 {path}: {e}\")\n</code></pre>"},{"location":"platform/storage/vospace/#next-steps","title":"\ud83d\udd17 Next Steps","text":"<ul> <li>Data Transfers \u2192 - Moving data between storage systems</li> <li>Filesystem Access \u2192 - ARC storage and SSHFS mounting</li> <li>Storage Overview \u2192 - Understanding all CANFAR storage types</li> <li>Interactive Sessions \u2192 - Using VOSpace within CANFAR sessions <pre><code>#### ARC (Inside CANFAR session)\n\n```bash\n# List files and directories\nls /arc/projects/[project]/\n\n# Copy files\ncp mydata.fits /arc/projects/[project]/data/\n\n# Create directories\nmkdir /arc/projects/[project]/survey_analysis/\n\n# Move/rename files\nmv /arc/projects/[project]/old.fits /arc/projects/[project]/new.fits\n\n# Remove files\nrm /arc/projects/[project]/temp/old_data.fits\n</code></pre></li> </ul>"},{"location":"platform/storage/vospace/#bulk-operations_1","title":"Bulk Operations","text":"<p>Note: <code>vsync</code> and <code>vcp</code> are always recursive; no <code>--recursive</code> flag is needed.</p>"},{"location":"platform/storage/vospace/#vault-vospace-api","title":"Vault (VOSpace API)","text":"<pre><code># Sync entire directories to Vault\nvsync ./local_data/ vos:[user]/backup/\n\n# Download project data from Vault\nvsync vos:[project]/survey_data/ ./project_data/\n\n# Upload analysis results to Vault\nvsync ./results/ vos:[user]/analysis_outputs/\n</code></pre>"},{"location":"platform/storage/vospace/#arc-vospace-api-outside-canfar","title":"ARC (VOSpace API, outside CANFAR)","text":"<pre><code># Sync entire directories to ARC\nvsync ./local_data/ arc:projects/[project]/backup/\n\n# Download project data from ARC\nvsync arc:projects/[project]/survey_data/ ./project_data/\n\n# Upload analysis results to ARC\nvsync ./results/ arc:projects/[project]/analysis_outputs/\n</code></pre>"},{"location":"platform/storage/vospace/#python-api_1","title":"Python API","text":""},{"location":"platform/storage/vospace/#basic-usage","title":"Basic Usage","text":"<pre><code>import vos\n\n# Initialize client\nclient = vos.Client()\n\n\n# List directory contents in Vault\nfiles_vault = client.listdir(\"vos:[user]/\")\nprint(files_vault)\n\n# List directory contents in ARC\nfiles_arc = client.listdir(\"arc:projects/[project]/\")\nprint(files_arc)\n\n# Check if file exists in Vault\nexists_vault = client.isfile(\"vos:[user]/data.fits\")\n\n# Check if file exists in ARC\nexists_arc = client.isfile(\"arc:projects/[project]/data.fits\")\n\n# Get file info from Vault\ninfo_vault = client.get_info(\"vos:[user]/data.fits\")\nprint(f\"Size: {info_vault['size']} bytes\")\nprint(f\"Modified: {info_vault['date']}\")\n\n# Get file info from ARC\ninfo_arc = client.get_info(\"arc:projects/[project]/data.fits\")\nprint(f\"Size: {info_arc['size']} bytes\")\nprint(f\"Modified: {info_arc['date']}\")\n</code></pre>"},{"location":"platform/storage/vospace/#file-operations_3","title":"File Operations","text":"<pre><code># Copy file to Vault\nclient.copy(\"mydata.fits\", \"vos:[user]/data/mydata.fits\")\n\n# Copy file to ARC\nclient.copy(\"mydata.fits\", \"arc:projects/[project]/data/mydata.fits\")\n\n# Copy file from Vault\nclient.copy(\"vos:[user]/data/results.txt\", \"./results.txt\")\n\n# Copy file from ARC\nclient.copy(\"arc:projects/[project]/data/results.txt\", \"./results.txt\")\n\n# Create directory in Vault\nclient.mkdir(\"vos:[user]/new_project/\")\n\n# Create directory in ARC\nclient.mkdir(\"arc:projects/[project]/new_project/\")\n\n# Delete file in Vault\nclient.delete(\"vos:[user]/temp/old_file.txt\")\n\n# Delete file in ARC\nclient.delete(\"arc:projects/[project]/temp/old_file.txt\")\n</code></pre>"},{"location":"platform/storage/vospace/#advanced-operations_1","title":"Advanced Operations","text":"<pre><code>import os\nfrom astropy.io import fits\n\ndef process_fits_files(vospace_dir, output_dir):\n    \"\"\"Process all FITS files in a Vault or ARC directory\"\"\"\n\n    # List all FITS files\n    files = client.listdir(vospace_dir)\n    fits_files = [f for f in files if f.endswith(\".fits\")]\n\n    for fits_file in fits_files:\n        vospace_path = f\"{vospace_dir}/{fits_file}\"\n        local_path = f\"./temp_{fits_file}\"\n\n        # Download file\n        client.copy(vospace_path, local_path)\n\n        # Process with astropy\n        with fits.open(local_path) as hdul:\n            # Your processing here\n            processed_data = hdul[0].data * 2  # Example processing\n\n            # Save processed file\n            output_path = f\"{output_dir}/processed_{fits_file}\"\n            fits.writeto(output_path, processed_data, overwrite=True)\n\n\n            # Upload to Vault or ARC\n            if vospace_dir.startswith(\"vos:\"):\n                client.copy(output_path, f\"vos:[user]/processed/{fits_file}\")\n            else:\n                client.copy(output_path, f\"arc:projects/[project]/processed/{fits_file}\")\n\n        # Clean up temporary file\n        os.remove(local_path)\n\n# Usage\n\nprocess_fits_files(\"vos:[user]/raw_data\", \"./processed/\")\nprocess_fits_files(\"arc:projects/[project]/raw_data\", \"./processed/\")\n</code></pre>"},{"location":"platform/storage/vospace/#automation-workflows","title":"Automation Workflows","text":""},{"location":"platform/storage/vospace/#batch-processing-script","title":"Batch Processing Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nAutomated data processing pipeline using Vault (VOSpace API) and ARC\n\"\"\"\nimport vos\nimport sys\nimport logging\nfrom pathlib import Path\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef setup_vospace():\n    \"\"\"Initialize VOSpace client with authentication\"\"\"\n    try:\n        client = vos.Client()\n        # Test connection\n        client.listdir(\"vos:[project]/\")\n        return client\n    except Exception as e:\n        logger.error(f\"VOSpace authentication failed: {e}\")\n        sys.exit(1)\n\ndef sync_input_data(client, remote_dir, local_dir):\n    \"\"\"Download input data from Vault or ARC\"\"\"\n    logger.info(f\"Syncing {remote_dir} to {local_dir}\")\n\n    Path(local_dir).mkdir(parents=True, exist_ok=True)\n\n    # Get list of files\n    files = client.listdir(remote_dir)\n\n    for file in files:\n        if file.endswith((\".fits\", \".txt\", \".csv\")):\n            remote_path = f\"{remote_dir}/{file}\"\n            local_path = f\"{local_dir}/{file}\"\n\n            if not Path(local_path).exists():\n                logger.info(f\"Downloading {file}\")\n                client.copy(remote_path, local_path)\n\ndef upload_results(client, local_dir, remote_dir):\n    \"\"\"Upload processing results to Vault or ARC\"\"\"\n    logger.info(f\"Uploading results from {local_dir} to {remote_dir}\")\n\n    # Ensure remote directory exists\n    try:\n        client.mkdir(remote_dir)\n    except:\n        pass  # Directory might already exist\n\n    for file_path in Path(local_dir).glob(\"*\"):\n        if file_path.is_file():\n            remote_path = f\"{remote_dir}/{file_path.name}\"\n            logger.info(f\"Uploading {file_path.name}\")\n            client.copy(str(file_path), remote_path)\n\ndef main():\n    \"\"\"Main processing pipeline\"\"\"\n    client = setup_vospace()\n\n    # Configuration\n    input_remote_vault = \"vos:[project]/raw_data\"\n    input_remote_arc = \"arc:projects/[project]/raw_data\"\n    output_remote_vault = \"vos:[user]/processed_results\"\n    output_remote_arc = \"arc:projects/[project]/processed_results\"\n    local_input = \"./input_data\"\n    local_output = \"./output_data\"\n\n    # Download input data from Vault\n    sync_input_data(client, input_remote_vault, local_input)\n    # Download input data from ARC\n    sync_input_data(client, input_remote_arc, local_input)\n\n    # Your processing code here\n    logger.info(\"Processing data...\")\n    # ... processing logic ...\n\n    # Upload results to Vault\n    upload_results(client, local_output, output_remote_vault)\n    # Upload results to ARC\n    upload_results(client, local_output, output_remote_arc)\n\n    logger.info(\"Pipeline completed successfully\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"platform/storage/vospace/#monitoring-and-logging_1","title":"Monitoring and Logging","text":""},{"location":"platform/storage/vospace/#transfer-progress","title":"Transfer Progress","text":"<pre><code>def copy_with_progress(client, source, destination):\n    \"\"\"Copy file with progress monitoring\"\"\"\n    import time\n\n    # Start transfer\n    start_time = time.time()\n    client.copy(source, destination)\n    end_time = time.time()\n\n    # Get file size for speed calculation\n    if source.startswith(\"vos:\"):\n        info = client.get_info(source)\n        size_mb = info[\"size\"] / (1024 * 1024)\n    else:\n        size_mb = os.path.getsize(source) / (1024 * 1024)\n\n    duration = end_time - start_time\n    speed = size_mb / duration if duration &gt; 0 else 0\n\n    print(f\"Transfer completed: {size_mb:.1f} MB in {duration:.1f}s ({speed:.1f} MB/s)\")\n</code></pre>"},{"location":"platform/storage/vospace/#error-handling","title":"Error Handling","text":"<pre><code>def robust_copy(client, source, destination, max_retries=3):\n    \"\"\"Copy with retry logic\"\"\"\n    import time\n\n    for attempt in range(max_retries):\n        try:\n            client.copy(source, destination)\n            return True\n        except Exception as e:\n            logger.warning(f\"Copy attempt {attempt + 1} failed: {e}\")\n            if attempt &lt; max_retries - 1:\n                time.sleep(2**attempt)  # Exponential backoff\n            else:\n                logger.error(f\"Copy failed after {max_retries} attempts\")\n                return False\n</code></pre>"},{"location":"platform/storage/vospace/#performance-optimization","title":"Performance Optimization","text":""},{"location":"platform/storage/vospace/#parallel-transfers","title":"Parallel Transfers","text":"<pre><code>import concurrent.futures\nimport threading\n\n\ndef parallel_upload(client, file_list, remote_dir, max_workers=4):\n    \"\"\"Upload multiple files in parallel\"\"\"\n\n    def upload_file(file_path):\n        remote_path = f\"{remote_dir}/{file_path.name}\"\n        try:\n            client.copy(str(file_path), remote_path)\n            return f\"\u2713 {file_path.name}\"\n        except Exception as e:\n            return f\"\u2717 {file_path.name}: {e}\"\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = [executor.submit(upload_file, f) for f in file_list]\n\n        for future in concurrent.futures.as_completed(futures):\n            result = future.result()\n            print(result)\n</code></pre>"},{"location":"platform/storage/vospace/#caching-strategy","title":"Caching Strategy","text":"<pre><code>import hashlib\nfrom pathlib import Path\n\n\ndef cached_download(client, vospace_path, local_path, force_refresh=False):\n    \"\"\"Download file only if it has changed\"\"\"\n\n    local_file = Path(local_path)\n    cache_file = Path(f\"{local_path}.cache_info\")\n\n    # Get remote file info\n    remote_info = client.get_info(vospace_path)\n    remote_hash = remote_info.get(\"MD5\", \"\")\n\n    # Check if we have cached info\n    if not force_refresh and local_file.exists() and cache_file.exists():\n        cached_hash = cache_file.read_text().strip()\n        if cached_hash == remote_hash:\n            print(f\"Using cached version of {local_file.name}\")\n            return local_path\n\n    # Download file\n    print(f\"Downloading {local_file.name}\")\n    client.copy(vospace_path, local_path)\n\n    # Save cache info\n    cache_file.write_text(remote_hash)\n\n    return local_path\n</code></pre>"},{"location":"platform/storage/vospace/#integration-examples","title":"Integration Examples","text":""},{"location":"platform/storage/vospace/#with-astropy","title":"With Astropy","text":"<pre><code>from astropy.io import fits\nfrom astropy.table import Table\n\n\ndef analyze_vospace_catalog(client, catalog_path):\n    \"\"\"Analyze a catalog stored in VOSpace\"\"\"\n\n    # Download catalog\n    local_path = \"./temp_catalog.fits\"\n    client.copy(catalog_path, local_path)\n\n    # Load and analyze\n    table = Table.read(local_path)\n\n    # Example analysis\n    bright_sources = table[table[\"magnitude\"] &lt; 15]\n    print(f\"Found {len(bright_sources)} bright sources\")\n\n    # Save filtered results\n    result_path = \"./bright_sources.fits\"\n    bright_sources.write(result_path, overwrite=True)\n\n    # Upload results\n    result_vospace = catalog_path.replace(\".fits\", \"_bright.fits\")\n    client.copy(result_path, result_vospace)\n\n    # Cleanup\n    os.remove(local_path)\n    os.remove(result_path)\n</code></pre>"},{"location":"platform/storage/vospace/#with-batch-jobs","title":"With Batch Jobs","text":"<pre><code>#!/bin/bash\n# Batch job script using Vault and ARC via VOSpace API\n\n# Authenticate\ncadc-get-cert --cert ~/.ssl/cadcproxy.pem\n\n\n# Download input data from Vault\nvcp vos:[project]/input/data.fits ./input.fits\n# Download input data from ARC\nvcp arc:projects/[project]/input/data.fits ./input_arc.fits\n\n# Process data\npython analysis_script.py input.fits output.fits\n\n# Upload results to Vault\nvcp output.fits vos:[project]/results/processed_$(date +%Y%m%d).fits\n# Upload results to ARC\nvcp output.fits arc:projects/[project]/results/processed_$(date +%Y%m%d).fits\n\n# Cleanup\nrm input.fits input_arc.fits output.fits\n</code></pre>"},{"location":"platform/storage/vospace/#troubleshooting_1","title":"Troubleshooting","text":""},{"location":"platform/storage/vospace/#common-issues_1","title":"Common Issues","text":"<p>Authentication Problems: <pre><code># Refresh certificate\ncadc-get-cert --cert ~/.ssl/cadcproxy.pem\n\n# Check certificate validity\ncadc-get-cert --cert ~/.ssl/cadcproxy.pem --days-valid\n</code></pre></p> <p>Network Timeouts: <pre><code># Increase timeout for large files\nimport vos\n\nclient = vos.Client()\nclient.timeout = 300  # 5 minutes\n</code></pre></p> <p>Permission Errors: <pre><code># Check file permissions in Vault\nvls -l vos:[user]/file.fits\n# Check file permissions in ARC\nvls -l arc:home/[user]/script.py\n\n# Check directory access in Vault\nvls vos:[project]/\n# Check directory access in ARC\nvls arc:projects/[project]/\n</code></pre></p>"},{"location":"platform/support/","title":"Getting Help and Support","text":"<p>\ud83c\udfaf Support Resources Overview</p> <p>Find the help you need:</p> <ul> <li>Self-service: Documentation, troubleshooting guides, and FAQs</li> <li>Community: User discussions, office hours, and peer assistance</li> <li>Direct support: Help from CANFAR platform specialists</li> <li>Emergency: Rapid response for critical incidents</li> </ul> <p>The CANFAR Science Platform offers several ways to get assistance. Start with self-service resources, then move to community channels or direct support as needed.</p>"},{"location":"platform/support/#quick-start-for-support","title":"\ud83d\ude80 Quick Start for Support","text":""},{"location":"platform/support/#new-to-canfar","title":"New to CANFAR?","text":"<ul> <li>Get Started Guide: 10-minute overview</li> <li>First Login: Account activation and access</li> <li>Choose Your Interface: Pick the right session type</li> </ul>"},{"location":"platform/support/#having-problems","title":"Having Problems?","text":"<ul> <li>FAQ: Quick answers to common questions</li> <li>Troubleshooting: Diagnostic steps for common issues</li> <li>Contact Support: Reach the CANFAR team for help</li> </ul>"},{"location":"platform/support/#self-help-resources","title":"\ud83d\udcda Self-Help Resources","text":"<ul> <li>Documentation search: Use the search box or browse by topic</li> <li>Concepts: Platform architecture and terminology</li> <li>Storage: Managing data effectively</li> <li>Containers: Using and building software environments</li> <li>Interactive Sessions: Jupyter, Desktop, CARTA, Firefly</li> <li>Batch Jobs: Automated and large-scale processing</li> </ul>"},{"location":"platform/support/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"platform/support/#quick-checks","title":"Quick Checks","text":"<ol> <li>Confirm there are no current maintenance announcements</li> <li>Try Chrome or Firefox and clear the browser cache</li> <li>Use a private/incognito window to rule out extensions</li> <li>Verify your network connection is stable</li> </ol>"},{"location":"platform/support/#frequent-issues","title":"Frequent Issues","text":""},{"location":"platform/support/#session-wont-start","title":"Session won't start","text":"<ul> <li>Lower memory or CPU requests and retry</li> <li>Try a different container image or launch time</li> <li>Ensure your account has the required group memberships</li> </ul>"},{"location":"platform/support/#cannot-access-files","title":"Cannot access files","text":"<pre><code># Check locations and permissions\nls /arc/home/[user]/\nls /arc/projects/[project]/\nls -la /arc/projects/[project]/\ngetfacl /arc/projects/[project]/\n</code></pre> <ul> <li>Confirm the path and project name</li> <li>Verify you belong to the correct project group</li> <li>Contact the project administrator if permissions are missing</li> </ul>"},{"location":"platform/support/#performance-feels-slow","title":"Performance feels slow","text":"<ul> <li>Monitor resource usage with <code>htop</code></li> <li>Close unused applications and tabs</li> <li>Use <code>/scratch/</code> for temporary, high-I/O workloads</li> <li>Submit a support request if performance remains degraded</li> </ul>"},{"location":"platform/support/#browser-quirks","title":"Browser quirks","text":"<ul> <li>Stick to Chrome or Firefox and keep them updated</li> <li>Enable JavaScript and cookies for <code>canfar.net</code></li> <li>Disable ad blockers or privacy extensions for the site</li> </ul>"},{"location":"platform/support/#gather-information-before-asking-for-help","title":"Gather Information Before Asking for Help","text":"<p>Run these commands to capture context for a support request:</p> <pre><code># Platform status\ncanfar info [session-id]\ncanfar stats\n\n# Session details\necho $USER\ngroups\nenv | grep -E \"(CANFAR|SKAHA)\"\n</code></pre>"},{"location":"platform/support/#contact-support","title":"\ud83d\udce7 Contact Support","text":""},{"location":"platform/support/#when-to-reach-out","title":"When to Reach Out","text":"<p>Email support@canfar.net when you encounter:</p> <ul> <li>Account issues: Login failures, certificate problems, group membership</li> <li>Technical problems: Persistent errors, failed sessions, system outages</li> <li>Data concerns: Missing files, data corruption, recovery requests</li> <li>Resource changes: Requests for additional storage, CPU, or RAM</li> <li>Software help: Complex installations or container customization</li> </ul>"},{"location":"platform/support/#what-to-include","title":"What to Include","text":"<p>Provide clear, specific details to speed up triage:</p> <ul> <li>Subject: Short summary of the problem</li> <li>Contact: CANFAR username and email</li> <li>Timeline: Date and time (with timezone) when the issue occurred</li> <li>Environment: Session type, container, operating system, browser</li> <li>Steps to reproduce: Numbered list of actions leading to the issue</li> <li>Observed vs expected: What happened and what you expected</li> <li>Error output: Copy exact error text and attach screenshots when available</li> <li>What you tried: Mention any workarounds attempted</li> </ul>"},{"location":"platform/support/#expected-response-times","title":"Expected Response Times","text":"Priority Response Time Examples Critical Same day System outages, data loss, security issues High 1\u20132 business days Session failures, access problems Normal 2\u20133 business days General questions, documentation requests Low 3\u20135 business days Feature requests, enhancement suggestions"},{"location":"platform/support/#escalation","title":"Escalation","text":"<p>If a ticket is not progressing within the expected timeframe:</p> <ol> <li>Reply to the original email and add \"URGENT\" to the subject</li> <li>Share any new details or screenshots gathered since the initial report</li> <li>For emergencies, follow the contacts listed in \ud83d\udea8 Emergency Contacts</li> </ol>"},{"location":"platform/support/#community-support","title":"\ud83d\udc65 Community Support","text":""},{"location":"platform/support/#discord","title":"Discord","text":"<p>Join the CANFAR Discord for real-time conversations with other users and staff.</p> <ul> <li>Search existing threads before posting</li> <li>Use the channel that matches your topic</li> <li>Share concise questions and relevant context</li> <li>Never publish sensitive data or credentials</li> </ul>"},{"location":"platform/support/#github","title":"GitHub","text":"<p>Use GitHub Issues to track bugs, suggest enhancements, or contribute documentation updates.</p> <ul> <li>Reference related documentation pages or example workflows</li> <li>Tag issues appropriately (e.g., <code>bug</code>, <code>documentation</code>, <code>feature-request</code>)</li> <li>Follow up on discussions to confirm fixes or add clarifications</li> </ul>"},{"location":"platform/support/#helpful-bug-reports","title":"\ud83d\udc1b Helpful Bug Reports","text":""},{"location":"platform/support/#before-filing","title":"Before Filing","text":"<ol> <li>Search the documentation and FAQ for related answers</li> <li>Look for existing issues on GitHub to avoid duplicates</li> <li>Ask quick questions on Discord if you are unsure whether something is a bug</li> </ol>"},{"location":"platform/support/#what-maintainers-need","title":"What Maintainers Need","text":"<ul> <li>Clear, descriptive title</li> <li>Environment details (OS, browser, session type, container)</li> <li>Steps to reproduce, numbered and complete</li> <li>Expected result versus what actually happened</li> <li>Complete error output and supporting screenshots or logs</li> <li>Notes on any temporary workarounds you discovered</li> </ul> <p>This template can help structure a report:</p> <pre><code>## Bug Description\n[Short summary]\n\n## Environment\n- OS: [...]\n- Browser: [...]\n- Session Type: [...]\n- Container: [...]\n\n## Steps to Reproduce\n1. [...]\n2. [...]\n\n## Expected Behavior\n[...]\n\n## Actual Behavior\n[...]\n\n## Error Messages\n```text\n[Paste exact text]\n</code></pre>"},{"location":"platform/support/#screenshots","title":"Screenshots","text":"<p>If applicable, add screenshots to help explain the problem.</p>"},{"location":"platform/support/#additional-context","title":"Additional Context","text":"<p>[Anything else that helps] ```</p> <p>After submitting, monitor the issue for follow-up questions, provide additional details promptly, and test proposed fixes when available.</p>"},{"location":"platform/support/#emergency-contacts","title":"\ud83d\udea8 Emergency Contacts","text":""},{"location":"platform/support/#system-outages","title":"System Outages","text":"<ul> <li>Planned maintenance notices go out at least 48 hours in advance via email and Discord</li> <li>For unexpected outages, email support@canfar.net and request a status update</li> </ul>"},{"location":"platform/support/#critical-data-issues","title":"Critical Data Issues","text":"<ol> <li>Stop affected jobs or sessions immediately</li> <li>Document what happened and when</li> <li>Email support@canfar.net with URGENT in the subject</li> <li>Preserve files and logs so recovery is possible</li> </ol> <p>Daily snapshots of <code>/arc/</code> storage are retained for 30 days; support can coordinate point-in-time recovery when necessary.</p>"},{"location":"platform/support/#security-incidents","title":"Security Incidents","text":"<ol> <li>Revoke and reissue credentials right away</li> <li>Report the incident to support@canfar.net</li> <li>Describe what you observed, including timestamps and IP addresses if known</li> <li>Follow instructions from the security team before resuming activity</li> </ol>"},{"location":"platform/support/#contributing","title":"\ud83d\udcdd Contributing","text":"<p>Documentation is community-driven. If you spot something to improve:</p> <ol> <li>Browse the source on GitHub</li> <li>Follow the contribution guidelines in <code>CONTRIBUTING.md</code></li> <li>Submit a pull request or open an issue describing the change</li> </ol> <p>For help getting started, ask in Discord or email support@canfar.net.</p>"},{"location":"platform/support/faq/","title":"Frequently Asked Questions","text":"<p>\ud83c\udfaf Quick Navigation</p> <p>Find answers by topic:</p> <ul> <li>Platform Questions: Getting started, sessions, storage, performance</li> <li>Session Resources: Resource management and optimisation</li> <li>Client Questions: Python client automation and REST API usage</li> <li>CLI Questions: Command-line interface and authentication</li> <li>Troubleshooting: Common problems and solutions</li> </ul> <p>This unified FAQ covers the CANFAR Science Platform across all areas: Platform, Client, and CLI usage.</p>"},{"location":"platform/support/faq/#platform","title":"Platform","text":""},{"location":"platform/support/faq/#what-is-the-canfar-science-platform","title":"What is the CANFAR Science Platform?","text":"<p>The CANFAR Science Platform is a national cloud computing environment tailored for astronomy. It provides interactive notebooks and desktops, browser-native visualization (e.g., CARTA, Firefly), user-contributed web applications, batch jobs, and direct access to CADC data holdings.</p>"},{"location":"platform/support/faq/#who-can-use-it-and-what-does-it-cost","title":"Who can use it and what does it cost?","text":"<p>CANFAR is free for astronomical research. Canadian astronomers and their collaborators can use it subject to fair\u2011use and allocation limits. For larger needs, request additional resources via the Digital Research Alliance of Canada (DRAC) Resource Allocation Competition.</p>"},{"location":"platform/support/faq/#how-do-i-get-access","title":"How do I get access?","text":"<ol> <li>To start, you must have a CADC account. If you don't have a CADC account, you can request one at: https://www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/en/auth/request.html</li> <li>To get CANFAR access, send a short e-mail to support@canfar.net with a short note about who you are, your research and what you plan to do with CANFAR. Include your CADC username. Turnaround is typically 1-2 business days. This can be done in parallel with requesting a CADC account.</li> <li>Alternatively, if you already have a CADC account and you are part of a research group that is already using CANFAR, you can ask your PI to add you to the appropriate project groups.</li> </ol>"},{"location":"platform/support/faq/#what-session-types-are-available-and-when-should-i-use-them","title":"What session types are available and when should I use them?","text":"<ul> <li>Notebook: Jupyter Lab for interactive analysis and prototyping.</li> <li>Desktop: Full Linux desktop for GUI workflows and multi\u2011app sessions.</li> <li>Firefly: Interactive database access and visualization.</li> <li>CARTA: Specialised image/cube visualisation.</li> <li>Headless: Non\u2011GUI batch processing and automation.</li> </ul>"},{"location":"platform/support/faq/#how-long-can-sessions-run","title":"How long can sessions run?","text":"<ul> <li>Interactive sessions: up to 7 days of continuous runtime, with auto\u2011shutdown after prolonged inactivity; resumable if not deleted.</li> <li>Batch jobs: no strict time limit; queue priority depends on resource usage.</li> </ul>"},{"location":"platform/support/faq/#can-i-run-gpuaccelerated-workloads","title":"Can I run GPU\u2011accelerated workloads?","text":"<p>Yes. Request NVIDIA GPUs in the session configuration from the command line or the API. Ensure your chosen container supports CUDA libraries (i.e. <code>astroml-cuda</code>)</p>"},{"location":"platform/support/faq/#how-much-storage-do-i-get-and-where-should-i-put-data","title":"How much storage do I get and where should I put data?","text":"<ul> <li>Personal: <code>/arc/home/[user]/</code> (typically 10 GB).</li> <li>Project/group: <code>/arc/projects/[project]/</code> (hundreds of GB to TBs, varies by project). If you do not have a project space, request one by emailing support@canfar.net.</li> <li>Temporary: <code>/scratch/</code> inside sessions (cleared when the session ends).</li> </ul> <p>Suggested layout:</p> <ul> <li>Raw data: <code>/arc/projects/[project]/raw/</code></li> <li>Working data: <code>/arc/projects/[project]/working/</code></li> <li>Data: <code>/arc/projects/[project]/data/</code></li> <li>Results: <code>/arc/projects/[project]/results/</code></li> <li>Scripts: <code>/arc/projects/[project]/scripts/</code></li> </ul>"},{"location":"platform/support/faq/#how-do-i-transfer-large-datasets","title":"How do I transfer large datasets?","text":"<ul> <li>For files &lt;1 GB, the Science Portal file manager is convenient.</li> </ul> <p>or the VOSpace client <code>vcp</code> from the <code>vos</code> python package:</p> <ul> <li> <p>For larger transfers, you can use <code>sshfs</code>:</p> <pre><code>sshfs -o reconnect,ServerAliveInterval=15,ServerAliveCountMax=10,defer_permissions -p 64022 [user]@ws-uv.canfar.net:/ $HOME/arc\ncp largedata.fits $HOME/arc/projects/[project]/data/\n</code></pre> </li> <li> <p>Or use the VOSpace client <code>vcp</code> from the <code>vos</code> python package:</p> <pre><code># to vault VOSpace for long-term access\nvcp largefile.fits vos:[project]/data/\n# to arc file system (and VOSpace) for short-term access\nvcp largefile.fits arc:[project]/data/\n</code></pre> </li> </ul>"},{"location":"platform/support/faq/#what-softwarecontainers-are-available","title":"What software/containers are available?","text":"<p>Containers include general astronomy stacks (AstroPy ecosystem), Jupyter, data science tools, machine learning libraries, full Linux desktops, and specialised astronomy tools (CASA, CARTA, DS9, TOPCAT). You can also build and use custom containers. See the Container Guide at <code>platform/containers/index.md</code>.</p>"},{"location":"platform/support/faq/#can-i-install-additional-software","title":"Can I install additional software?","text":"<ul> <li><code>pip install --user ...</code> (the <code>--user</code> option may not be necessary depending on container) or within environments. Software will be persisted on <code>/arc</code></li> <li>Permanent: build a custom container with your required stack (see <code>platform/containers/index.md</code>). Software will be persisted on the container.</li> </ul>"},{"location":"platform/support/faq/#collaboration-and-sharing","title":"Collaboration and sharing","text":"<ul> <li>Share data via project groups and <code>/arc/projects/[project]/</code> or in vault VOSPace with appropriate permissions.</li> <li>Share container images via the Harbor registry on images.canfar.net</li> <li>Share code via Git and group storage; document workflows.</li> </ul>"},{"location":"platform/support/faq/#troubleshooting-slow-or-failing-sessions","title":"Troubleshooting slow or failing sessions","text":"<ul> <li>Resource constraints: Try flexible mode (default) for faster scheduling, or use fixed mode with fewer cores/less RAM if needed. Consider different times of day when cluster load varies.</li> <li>Variable performance in flexible mode: This is normal - performance adapts to cluster load. For consistent performance, use fixed mode with specific resource values.</li> <li>Container issues: Verify name/version; try a maintained baseline image.</li> <li>Account/group issues: Confirm group membership and active account status.</li> <li>Performance optimization: Process data in fast scratch (e.g., <code>/scratch/</code>), parallelize where appropriate, monitor with <code>htop</code>, <code>df -h</code>, <code>iotop</code>.</li> </ul>"},{"location":"platform/support/faq/#getting-help-and-community","title":"Getting help and community","text":"<ul> <li>Documentation: start at <code>platform/index.md</code>.</li> <li>Help &amp; Support: <code>platform/support/index.md</code> (how to contact support and what to include).</li> <li>Community: Discord for Q&amp;A and announcements; workshops and office hours are announced there.</li> </ul>"},{"location":"platform/support/faq/#session-resources","title":"Session Resources","text":""},{"location":"platform/support/faq/#why-is-my-session-performance-variable","title":"Why is my session performance variable?","text":"<p>If you're using flexible mode (the default), performance variation is normal and expected. Your session can use more resources when the cluster has capacity available, but may use fewer resources during peak usage times. This adaptive behavior allows for better overall cluster utilization.</p> <p>For consistent performance, use fixed mode by specifying exact <code>--cpu</code> and <code>--memory</code> values (CLI) or <code>cores</code> and <code>ram</code> parameters (Python API).</p>"},{"location":"platform/support/faq/#client","title":"Client","text":""},{"location":"platform/support/faq/#can-i-automate-session-management-with-the-python-client","title":"Can I automate session management with the Python client?","text":"<p>Yes. The Python client supports creating, monitoring, and cleaning up sessions programmatically.</p> <p>Example: <pre><code>import time\nfrom canfar import Session\n\nsession = Session()\nsid = session.launch(name=\"automated\", kind=\"headless\", cmd=\"python\", args=[\"script.py\"])\n\nwhile session.info(sid)[0][\"status\"] != \"Completed\":\n    time.sleep(60)\n\nsession.logs([sid])\nsession.destroy([sid])\n</code></pre></p>"},{"location":"platform/support/faq/#how-do-i-call-the-rest-api-directly","title":"How do I call the REST API directly?","text":"<p>You can use REST endpoints for jobs and sessions if you prefer low\u2011level control.</p> <p>Example: <pre><code>from canfar.sessions import Session\n\nsession = Session()\njob_ids = session.create(\n    name=\"automated-analysis\",\n    image=\"images.canfar.net/skaha/astroml:latest\",\n    cores=4,\n    ram=16,\n    kind=\"headless\",\n    cmd=\"python /arc/projects/[project]/scripts/analyze.py\",\n)\n</code></pre></p>"},{"location":"platform/support/faq/#authentication-options-for-programs","title":"Authentication options for programs","text":"<ul> <li>X.509 certificates (typical for many users).</li> <li>OIDC tokens via SRCNet for advanced and cross\u2011site workflows. See <code>cli/authentication-contexts.md</code> for options and flows.</li> </ul>"},{"location":"platform/support/faq/#cli","title":"CLI","text":""},{"location":"platform/support/faq/#how-do-i-authenticate","title":"How do I authenticate?","text":"<ul> <li> <p>Certificates:</p> <pre><code>cadc-get-cert -u [user]\n# enter CADC password when prompted\n</code></pre> </li> <li> <p>OIDC (SRCNet\u2011aware):</p> <pre><code>canfar auth login\n</code></pre> </li> </ul> <p>Certificates typically last ~10 days; renew as needed.</p>"},{"location":"platform/support/faq/#how-do-i-check-platform-status-and-quotas-from-the-cli","title":"How do I check platform status and quotas from the CLI?","text":"<pre><code>canfar stats\n</code></pre>"},{"location":"platform/support/faq/#why-is-my-session-stuck-in-pending","title":"Why is my session stuck in \"Pending\"?","text":"<p>Possible reasons: insufficient resources, image issues, quota limits, or maintenance windows. Inspect events:</p> <pre><code>canfar events &lt;session-id&gt;\n</code></pre>"},{"location":"platform/support/faq/#i-cant-connect-to-my-session-url","title":"I can\u2019t connect to my session URL","text":"<ol> <li>Ensure the session is Running (<code>canfar ps</code>).</li> <li>Check for VPN/firewall interference.</li> <li>Try another browser or clear cache/private mode.</li> </ol>"},{"location":"platform/support/faq/#can-i-run-multiple-sessions-at-once","title":"Can I run multiple sessions at once?","text":"<p>Yes. You can run multiple sessions concurrently subject to fair\u2011use and any configured limits per session type. Prefer batch/headless for automation.</p>"},{"location":"platform/support/faq/#where-can-i-find-more-cli-help","title":"Where can I find more CLI help?","text":"<ul> <li>Quick start: <code>cli/quick-start.md</code></li> <li>Auth contexts: <code>cli/authentication-contexts.md</code></li> <li>Command reference: <code>cli/cli-help.md</code></li> </ul>"},{"location":"platform/support/faq/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"platform/support/faq/#common-platform-issues","title":"Common Platform Issues","text":""},{"location":"platform/support/faq/#sessions-wont-start-or-take-too-long-to-launch","title":"Sessions won't start or take too long to launch","text":"<p>Possible causes:</p> <ul> <li>High cluster usage during peak hours</li> <li>Resource requirements too high</li> <li>Container image issues</li> <li>Insufficient group permissions</li> </ul> <p>Solutions:</p> <ul> <li>Try flexible mode for faster scheduling</li> <li>Reduce CPU/memory requirements</li> <li>Use off-peak hours (evenings, weekends)</li> <li>Verify container image name and availability</li> <li>Check group membership for required projects</li> </ul>"},{"location":"platform/support/faq/#cant-access-files-or-storage","title":"Can't access files or storage","text":"<p>Possible causes:</p> <ul> <li>Incorrect file paths</li> <li>Missing group permissions</li> <li>Storage quota exceeded</li> <li>Network connectivity issues</li> </ul> <p>Solutions:</p> <ul> <li>Verify file paths: <code>/arc/home/[user]/</code> vs <code>/arc/projects/[project]/</code></li> <li>Check group membership with project administrators</li> <li>Clean up old files to free space</li> <li>Use <code>ls -la</code> and <code>getfacl</code> to check permissions</li> </ul>"},{"location":"platform/support/faq/#performance-is-slow-or-variable","title":"Performance is slow or variable","text":"<p>In flexible mode (default):</p> <ul> <li>Performance varies with cluster load (this is normal)</li> <li>More resources available during off-peak hours</li> <li>Better overall cluster utilisation</li> </ul> <p>For consistent performance:</p> <ul> <li>Use fixed mode with specific CPU/memory values</li> <li>Consider batch jobs for large processing tasks</li> <li>Use <code>/scratch/</code> storage for temporary files</li> </ul>"},{"location":"platform/support/faq/#browser-or-interface-problems","title":"Browser or interface problems","text":"<p>Symptoms:</p> <ul> <li>Interface won't load</li> <li>Features don't work properly</li> <li>Connection timeouts</li> </ul> <p>Solutions:</p> <ul> <li>Use Chrome or Firefox (recommended browsers)</li> <li>Clear browser cache and cookies</li> <li>Try incognito/private mode</li> <li>Disable ad blockers for canfar.net</li> <li>Check network connection stability</li> </ul>"},{"location":"platform/support/faq/#authentication-and-access-issues","title":"Authentication and Access Issues","text":""},{"location":"platform/support/faq/#certificate-problems","title":"Certificate problems","text":"<p>Symptoms:</p> <ul> <li>Can't login or authenticate</li> <li>\"Certificate expired\" errors</li> <li>CLI commands fail with auth errors</li> </ul> <p>Solutions:</p> <pre><code># Renew certificate\ncadc-get-cert -u [username]\n\n# Check certificate status\ncadc-get-cert --days-valid\n\n# For OIDC authentication\ncanfar auth login\n</code></pre>"},{"location":"platform/support/faq/#permission-denied-errors","title":"Permission denied errors","text":"<p>Symptoms:</p> <ul> <li>Can't access project directories</li> <li>File operation failures</li> <li>Session creation blocked</li> </ul> <p>Solutions:</p> <ul> <li>Verify group membership with project PI</li> <li>Check project access through Science Portal</li> <li>Ensure account is active and in good standing</li> <li>Contact support if permissions seem incorrect</li> </ul>"},{"location":"platform/support/faq/#getting-help","title":"\ud83c\udd98 Getting Help","text":""},{"location":"platform/support/faq/#when-to-use-each-support-channel","title":"When to Use Each Support Channel","text":""},{"location":"platform/support/faq/#use-discord-for","title":"Use Discord for","text":"<ul> <li>Quick questions with fast community response</li> <li>Sharing tips and tricks with other users</li> <li>General platform discussions</li> <li>Finding collaborators</li> </ul>"},{"location":"platform/support/faq/#use-github-issues-for","title":"Use GitHub Issues for","text":"<ul> <li>Bug reports with reproducible problems</li> <li>Feature requests and suggestions</li> <li>Documentation improvements</li> <li>Technical discussions</li> </ul>"},{"location":"platform/support/faq/#email-support-for","title":"Email Support for","text":"<ul> <li>Account access problems</li> <li>Resource allocation requests</li> <li>Data recovery needs</li> <li>Complex technical issues</li> <li>Security concerns</li> </ul>"},{"location":"platform/support/faq/#before-contacting-support","title":"Before Contacting Support","text":"<ol> <li>Check this FAQ for common solutions</li> <li>Search existing issues on GitHub</li> <li>Try basic troubleshooting steps (restart browser, clear cache)</li> <li>Gather diagnostic information using commands in index.md</li> </ol>"},{"location":"platform/support/faq/#what-to-include-in-support-requests","title":"What to Include in Support Requests","text":"<p>Essential information:</p> <ul> <li>CANFAR username</li> <li>Date/time of issue</li> <li>Session type and container used</li> <li>Browser and version</li> <li>Complete error messages</li> <li>Steps to reproduce the problem</li> </ul> <p>Helpful additional details:</p> <ul> <li>Screenshots of error screens</li> <li>Session IDs for failed jobs</li> <li>File paths for access issues</li> <li>What you've already tried</li> </ul>"},{"location":"platform/support/faq/#response-time-expectations","title":"Response Time Expectations","text":"Issue Type Response Time Examples Critical Same day System outages, data loss, security High 1-2 business days Session failures, access problems Normal 2-3 business days General questions, how-to requests Low 3-5 business days Feature requests, documentation"},{"location":"platform/support/faq/#community-resources","title":"Community Resources","text":"<ul> <li>Discord Community: Active community chat</li> </ul> <p>Remember: The CANFAR community is here to help! Don't hesitate to ask questions, share your experiences, or contribute solutions that might help other users.</p>"},{"location":"releases/2025-1/","title":"CANFAR Science Platform Release 2025.1 - Sept 9, 2025","text":"<p>CanfarSP 2025.1 - Sept 9, 2025</p> <p>Dear CANFAR Community,</p> <p>We are pleased to announce a major milestone for the CANFAR Science Platform: On September 9, 2025, we completed a transition from a beta system, initially released in 2021, to our first production release, CANFAR Science Platoform 2025.1, marking the beginning of an official production release cycle.</p> <p>This latest version is ready for use on www.canfar.net, and is also available for deployments to pick up across SRCNet.</p> <p>If you use scripts to launch sessions on the science platform via the now deprecated skaha python package or with curl, please switch to the new CANFAR Python Client or CLI. If you access the API directly, please switch the reference to <code>skaha/v0</code> to <code>skaha/v1</code> as soon as possible.</p>"},{"location":"releases/2025-1/#highlights","title":"\u2728 Highlights","text":"<ul> <li>New &amp; Improved User Documentation Hub</li> <li>Official Release of the CANFAR Python Client &amp; CLI \u2014 see clients docs</li> <li>Smart Session Launching \u2014 choose between flexible (auto-scaling) and fixed modes</li> <li>Science Portal UI Improvements \u2014 added display for home directory &amp; storage quota usage</li> <li>CARTA 5.0: latest radio astronomy visualization tool (August 2025 Release)</li> <li>Firefly: IVOA-compliant catalog browsing and visualization platform</li> </ul>"},{"location":"releases/2025-1/#changes-deprecations","title":"\ud83d\udcdd Changes &amp; Deprecations","text":"<ul> <li>Breaking Changes:<ul> <li>For API users, <code>headless</code> sessions no longer require the <code>type</code> parameter</li> <li>For Python Client &amp; CLI users, <code>headless</code> sessions no longer require the <code>kind</code> parameter and the <code>headless</code> session <code>kind</code> will be deprecated in a future release.</li> <li><code>Succeeded</code> status is now <code>Completed</code> for all session types, e.g. when performing a <code>session.info()</code> query.</li> </ul> </li> <li>Skaha API <code>v1</code> Released \u2014 <code>v0</code> API will be sunset with the next major release. Portal users are unaffected; API users should plan to migrate to <code>v1</code> as soon as possible.</li> <li>Container Image Labels are no longer required in the Harbor Image Registry. They are only used to populate dropdown menu options in the Science Portal UI.</li> <li>Session Types \u2014 launching via API, omit the <code>type</code> parameter for headless mode; interactive sessions require the <code>type</code> parameter.</li> <li>Status Changes \u2014 Job status <code>Succeeded</code> is now <code>Completed</code> for all session types.</li> </ul>"},{"location":"releases/2025-1/#fixes","title":"\ud83d\udc1b Fixes","text":"<ul> <li>Resource Monitoring \u2014 RAM and CPU usage for sessions now display correctly in the Science Portal UI.</li> </ul>"},{"location":"releases/2025-1/#technical-changes","title":"\u2699\ufe0f Technical Changes","text":"<ul> <li>CANFAR deployment requires Kubernetes v1.29 or later</li> <li>Kueue Scheduling \u2014 optional advanced job scheduling system that can be enabled per namespace to reduce cluster pressure and provide queue management.</li> <li>Monitoring Fixes \u2014 Skaha API now uses the the Job API instead of the Pod API internally to provide more accurate resource usage information.</li> <li>Flexible sessions use the <code>Burstable</code> Kubernetes Quality of Service (QoS) class instead of <code>Guaranteed</code>, which provides better resource efficiency on the cluster. Currently, flexible sessions can grow up to 8 cores and 32GB of RAM.</li> <li>Internal API's have been updated to use the <code>Job</code> API instead of the <code>Pod</code> API. This provides better resource monitoring and usage information.</li> </ul>"},{"location":"releases/2025-1/#deployment-notes","title":"\ud83d\udce6 Deployment Notes","text":"<ul> <li>Use the offically supported helm charts in the opencadc/deployments for CANFAR 2025.1 deployments.</li> <li>To test, profile and setup the Kueue scheduling system, see the deployment guide for detailed instructions.</li> </ul>"},{"location":"releases/2025-1/#python-client-cli","title":"Python Client &amp; CLI","text":"Component Version canfar v1.0.2"},{"location":"releases/2025-1/#helm-charts-container-images","title":"Helm Charts &amp; Container Images","text":"Component Helm Chart Version Container Image base 0.4.0 N/A cavern 0.7.0 images.opencadc.org/platform/cavern:0.9.0 skaha 1.0.3 images.opencadc.org/platform/skaha:1.0.2 posix-mapper 0.4.4 images.opencadc.org/platform/posix-mapper:0.3.2 science-portal 1.0.0 images.opencadc.org/platform/science-portal:1.0.0 storage-ui 0.6.0 images.opencadc.org/client/storage-ui:1.3.0"},{"location":"releases/2025-1/#contact-support","title":"\ud83d\udcac Contact &amp; Support","text":"<p>For any questions about this release, or for information relating to CANFAR issues or deployment support, head over to the CANFAR Discord Server or please contact us at support@canfar.net.</p> <p>  Built with  at CADC </p>"},{"location":"releases/2025-2/","title":"CANFAR Science Platform Release 2025.2 - Nov 25, 2025","text":"<p>CanfarSP 2025.2 - Nov 25, 2025</p>"},{"location":"releases/2025-2/#features","title":"\u2728 Features","text":"<ul> <li>Modified resource selection controls - On the CANFAR Science Portal, when launching a session in Fixed-mode, there is more fine-tuned control of the resource selections. </li> <li>Cluster-aware resource selection on CANFAR Science Portal \u2014 Memory and Core options reflect characteristics of the underlying kubernetes cluster</li> <li>GPU selection \u2014 In CANFAR clusters where GPUs are available, a GPU selection option presented in the science portal</li> </ul>"},{"location":"releases/2025-2/#fixes","title":"\ud83d\udc1b Fixes","text":"<ul> <li>Addresses problem with the incorrect enforcement of the maximum number of sessions allowed</li> <li>Improved accuracy of Global session statistics on CANFAR portal</li> <li>canfar CLI fixes<ul> <li>Graceful Degradation: The CLI commands (canfar info, canfar ps) now continue to work even when the API returns incomplete session data, displaying partial information instead of crashing</li> <li>Better Error Reporting: Missing or invalid fields are tracked internally and can be viewed with the --debug flag for troubleshooting</li> <li>Enhanced Display: Resource usage metrics for flexible sessions is now reported with better readability</li> <li>Type Safety: Session type validation has been strengthened using Pydantic's built-in validators</li> </ul> </li> </ul>"},{"location":"releases/2025-2/#technical-changes","title":"\u2699\ufe0f Technical Changes","text":"<ul> <li>Cavern controlls user allocations - called by authorized clients such as skaha and prepareData</li> <li>All registry lookups benefit from registry mirrroring and failover</li> </ul>"},{"location":"releases/2025-2/#deployment-notes","title":"\ud83d\udce6 Deployment Notes","text":"<ul> <li>Deployers now specify limits of their cluster as LimitRange objects from helm charts [DONE]</li> <li>Deployers must specify properties about their cavern installation</li> <li>Ability to specify a default project for each of the harbor instances configured</li> <li>Ability to define multiple registries to support mirroring</li> <li>Use the offically supported helm charts in the opencadc/deployments for CANFAR 2025.2 deployments.</li> </ul>"},{"location":"releases/2025-2/#python-client-cli","title":"Python Client &amp; CLI","text":"Component Version canfar v1.1+"},{"location":"releases/2025-2/#helm-charts-container-images","title":"Helm Charts &amp; Container Images","text":"Component Helm Chart Version Container Image base 0.4.0 N/A cavern 0.9.0 images.opencadc.org/platform/cavern:0.9.2 skaha 1.3.2 images.opencadc.org/platform/skaha:1.1.7 posix-mapper 0.5.0 images.opencadc.org/platform/posix-mapper:0.3.2 science-portal 1.1.2 images.opencadc.org/platform/science-portal:1.2.5 storage-ui 0.8.0 images.opencadc.org/client/storage-ui:1.4.1"},{"location":"releases/2025-2/#contact-support","title":"\ud83d\udcac Contact &amp; Support","text":"<p>For any questions about this release, or for information relating to CANFAR issues or deployment support, head over to the CANFAR Discord Server or please contact us at support@canfar.net.</p> <p>  Built with  at CADC </p>"},{"location":"releases/releases/","title":"CANFAR Releases","text":"<p>CanfarSP Releases</p> <ul> <li>Nov 25, 2025 2025.2</li> <li>Sept 9, 2025 2025.1</li> </ul>"},{"location":"releases/releases/#canfar-releases_1","title":"\ud83c\udfaf CANFAR Releases","text":"<p>The CADC team is releasing the CANFAR Science Platform (CanfarSP) on a fixed, predictable schedule.  This is to allow deployers to build into their schedules the rollout of new versions of CanfarSP.</p> <p>CanfarSP releases are named using the year and number, in the form CanfarSP ..  For example, the first release of 2025 was named CanfarSP 2025.1. <p>We currently aiming for a CanfarSP release every 3 months.</p> <p>In the subpages of this section of documentation, detailed notes about each release can be found.</p> <p>CanfarSP releases include a time of dedicated integration and regression testing, and internal user testing.  Generally:</p> <ul> <li>New features can only be added to CanfarSP in a fixed cycle release</li> <li>Bug fixes are added to CanfarSP immediately and result in a patch release</li> <li>Collaborators can get access to new features prior to their target release, but only as experimental software because the features have not been thoroughly tested.</li> </ul>"},{"location":"releases/releases/#roadmap","title":"Roadmap","text":"<p>Currently, the CanfarSP Roadmap is managed interally by CADC, though we plan on making it public in the near future.  For questions or suggestions about the CanfarSP Roadmap, please reach out to the CADC team.</p>"},{"location":"releases/releases/#development-contributions","title":"Development Contributions","text":"<p>We welcome and encourage contributions to all areas of the CANFAR and OpenCADC code bases!  For information on how to start, please see the contributing guidelines in these OpenCADC repositories:</p> <ul> <li>CANFAR Client and Documentation</li> <li>Science Platform Infrastructure</li> <li>Deployment Configuration</li> </ul>"}]}